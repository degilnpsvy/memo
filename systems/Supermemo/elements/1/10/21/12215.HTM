<B><SPAN style="FONT-SIZE: 16.5pt; mso-bidi-font-size: 12.0pt" lang=EN-US><B><SPAN style="FONT-SIZE: 16.5pt; mso-bidi-font-size: 12.0pt" lang=en-us></SPAN></B>The memory subsystem</SPAN></B> 
<P></P>
<P><SPAN style="FONT-SIZE: 16.5pt; mso-bidi-font-size: 12.0pt" lang=EN-US>GPUs don&#8217;t have your regular memory subsystem &#8211; it&#8217;s different from what you see in general-purpose CPUs or other hardware, because it&#8217;s designed for very different usage patterns. There&#8217;s two fundamental ways in which a GPU&#8217;s memory subsystem differs from what you see in a regular machine:</SPAN></P>
<P><SPAN style="FONT-SIZE: 16.5pt; mso-bidi-font-size: 12.0pt" lang=EN-US><FONT class=extract>The first is that GPU memory subsystems are&nbsp;<I>fast</I>. Seriously fast. A Core i7 2600K will hit maybe 19 GB/s memory bandwidth &#8211; on a good day. With tail wind. Downhill. A GeForce GTX 480, on the other hand, has a total memory bandwidth of close to 180 GB/s &#8211; nearly an order of magnitude difference! Whoa.</FONT></SPAN></P>
<P><SPAN style="FONT-SIZE: 16.5pt; mso-bidi-font-size: 12.0pt" lang=EN-US><FONT class=extract>The second is that GPU memory subsystems are&nbsp;<I>slow</I>. Seriously slow. A cache miss to main memory on a Nehalem (first-generation Core i7) takes about 140 cycles if you divide the&nbsp;</FONT><A href="http://www.daili987.com/weibo.com.php?u=uWbqpkPTKj%2B7Uc0jcb2cwXVilmlYXHA6mL2HpGoFYT0%3D&amp;b=3"><B><FONT class=extract>memory latency as given by AnandTech</FONT></B></A><FONT class=extract>&nbsp;by the clock rate. The GeForce GTX 480 I mentioned previously has a&nbsp;</FONT><A href="http://www.daili987.com/weibo.com.php?u=uWbqpkPTKi2hUc0haqqbhz5ljCsTSmghwNv23BsYKmd6mBXPcylcOEQr4ncXk4gaSw2OIzBQ%2Bf3oBMTWDwjtOVtV&amp;b=3"><B><FONT class=extract>memory access latency of 400-800 clocks</FONT></B></A><FONT class=extract>. So let&#8217;s just say that, measured in cycles, the GeForce GTX 480 has a bit more than 4x the average memory latency of a Core i7. Except that Core i7 I just mentioned is clocked at 2.93GHz, whereas GTX 480 shader clock is 1.4 GHz &#8211; that&#8217;s it, another 2x right there. Woops &#8211; again, nearly an order of magnitude difference!</FONT> Wait, something funny is going on here. My common sense is tingling. This must be one of those trade-offs I keep hearing about in the news!</SPAN></P>
<P><SPAN style="FONT-SIZE: 16.5pt; mso-bidi-font-size: 12.0pt" lang=EN-US>Yep &#8211; GPUs get a massive increase in bandwidth, but they pay for it with a massive increase in latency (and, it turns out, a sizable hit in power draw too, but that&#8217;s beyond the scope of this article). This is part of a general pattern &#8211; GPUs are all about throughput over latency; don&#8217;t wait for results that aren&#8217;t there yet, do something else instead!</SPAN></P>
<P><SPAN style="FONT-SIZE: 16.5pt; mso-bidi-font-size: 12.0pt" lang=EN-US>That&#8217;s almost all you need to know about GPU memory, except for one general DRAM tidbit that will be important later on: DRAM chips are organized as a 2D grid &#8211; both logically and physically. There&#8217;s (horizontal) row lines and (vertical) column lines. At each intersection between such lines is a transistor and a capacitor; if at this point you want to know how to actually build memory from these ingredients,&nbsp;<A href="http://www.daili987.com/weibo.com.php?u=uWbqtFqKcze%2BWdMiYbGehzRznisARnM8wNbn0BM%3D&amp;b=3#Operation_principle"><B>Wikipedia is your friend</B></A>. Anyway, the salient point here is that the address of a location in DRAM is split into a row address and a column address, and DRAM reads/writes internally always end up accessing all columns in the given row at the same time. What this means is that it&#8217;s much cheaper to access a swath of memory that maps to exactly one DRAM row than it is to access the same amount of memory spread across multiple rows. Right now this may seem like just a random bit of DRAM trivia, but this will become important later on; in other words, pay attention: this will be on the exam. But to tie this up with the figures in the previous paragraphs, just let me note that you can&#8217;t reach those peak memory bandwidth figures above by just reading a few bytes all over memory; if you want to saturate memory bandwidth, you better do it one full DRAM row at a time.</SPAN>