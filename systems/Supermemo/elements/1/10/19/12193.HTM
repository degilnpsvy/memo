<B><SPAN style="FONT-SIZE: 16.5pt; mso-bidi-font-size: 12.0pt" lang=EN-US><FONT class=extract>The user-mode graphics driver (or UMD)</FONT></SPAN></B> 
<P></P>
<P><SPAN style="FONT-SIZE: 16.5pt; mso-bidi-font-size: 12.0pt" lang=EN-US><FONT class=extract><FONT class=extract>This is where most of the &#8220;magic&#8221; on the CPU side happens. If your app crashes because of some API call you did, it will usually be in here :).</FONT> <FONT class=extract>It&#8217;s called &#8220;nvd3dum.dll&#8221; (NVidia) or &#8220;atiumd*.dll&#8221; (AMD).</FONT> As the name suggests, this is user-mode code; it&#8217;s running in the same context and address space as your app (and the API runtime) and has no elevated privileges whatsoever. It implements a lower-level API (the DDI) that is called by D3D; this API is fairly similar to the one you&#8217;re seeing on the surface, but a bit more explicit about things like memory management and such.</FONT></SPAN></P>
<P><SPAN style="FONT-SIZE: 16.5pt; mso-bidi-font-size: 12.0pt" lang=EN-US><FONT class=extract>This module is where things like shader compilation happen. D3D passes a pre-validated shader token stream to the UMD &#8211; i.e. it&#8217;s already checked that the code is valid in the sense of being syntactically correct and obeying D3D constraints (using the right types, not using more textures/samplers than available, not exceeding the number of available constant buffers, stuff like that). This is compiled from HLSL code and usually has quite a number of high-level optimizations (various loop optimizations, dead-code elimination, constant propagation, predicating ifs etc.) applied to it &#8211; this is good news since it means the driver benefits from all these relatively costly optimizations that have been performed at compile time. However, it also has a bunch of lower-level optimizations (such as register allocation and loop unrolling) applied that drivers would rather do themselves; long story short, this usually just gets immediately turned into a intermediate representation (IR) and then compiled some more; shader hardware is close enough to D3D bytecode that compilation doesn&#8217;t need to work wonders to give good results (and the HLSL compiler having done some of the high-yield and high-cost optimizations already definitely helps), but there&#8217;s still lots of low-level details (such as HW resource limits and scheduling constraints) that D3D neither knows nor cares about, so this is not a trivial process.</FONT></SPAN></P>
<P><SPAN style="FONT-SIZE: 16.5pt; mso-bidi-font-size: 12.0pt" lang=EN-US><FONT class=extract>And of course, if your app is a well-known game, programmers at NV/AMD have probably looked at your shaders and wrote hand-optimized replacements for their hardware &#8211; though they better produce the same results lest there be a scandal :). These shaders get detected and substituted by the UMD too. You&#8217;re welcome.</FONT></SPAN></P>
<P><SPAN style="FONT-SIZE: 16.5pt; mso-bidi-font-size: 12.0pt" lang=EN-US><FONT class=extract>More fun: Some of the API state may actually end up being compiled into the shader &#8211; to give an example, relatively exotic (or at least infrequently used) features such as texture borders are probably not implemented in the texture sampler, but emulated with extra code in the shader (or just not supported at all). This means that there&#8217;s sometimes multiple versions of the same shader floating around, for different combinations of API states.</FONT></SPAN></P>
<P><SPAN style="FONT-SIZE: 16.5pt; mso-bidi-font-size: 12.0pt" lang=EN-US><FONT class=extract>Incidentally, this is also the reason why you&#8217;ll often see a delay the first time you use a new shader or resource; a lot of the creation/compilation work is deferred by the driver and only executed when it&#8217;s actually necessary (you wouldn&#8217;t believe how much unused crap some apps create!). Graphics programmers know the other side of the story &#8211; if you want to make sure something is actually created (as opposed to just having memory reserved), you need to issue a dummy draw call that uses it to &#8220;warm it up&#8221;. Ugly and annoying, but this has been the case since I first started using 3D hardware in 1999 &#8211; meaning, it&#8217;s pretty much a fact of life by this point, so get used to it. :)</FONT></SPAN></P>
<P><SPAN style="FONT-SIZE: 16.5pt; mso-bidi-font-size: 12.0pt" lang=EN-US>Anyway, moving on. The UMD also gets to deal with fun stuff like all the D3D9 &#8220;legacy&#8221; shader versions and the fixed function pipeline &#8211; yes, all of that will get faithfully passed through by D3D. The 3.0 shader profile ain&#8217;t that bad (it&#8217;s quite reasonable in fact), but 2.0 is crufty and the various 1.x shader versions are seriously whack &#8211; remember 1.3 pixel shaders? Or, for that matter, the fixed-function vertex pipeline with vertex lighting and such? Yeah, support for all that&#8217;s still there in D3D and the guts of every modern graphics driver, though of course they just translate it to newer shader versions by now (and have been doing so for quite some time).</SPAN></P>
<P><SPAN style="FONT-SIZE: 16.5pt; mso-bidi-font-size: 12.0pt" lang=EN-US><FONT class=extract>Then there&#8217;s things like memory management. The UMD will get things like texture creation commands and need to provide space for them. Actually, the UMD just suballocates some larger memory blocks it gets from the KMD (kernel-mode driver); actually mapping and unmapping pages (and managing which part of video memory the UMD can see, and conversely which parts of system memory the GPU may access) is a kernel-mode privilege and can&#8217;t be done by the UMD.</FONT></SPAN></P>
<P><SPAN style="FONT-SIZE: 16.5pt; mso-bidi-font-size: 12.0pt" lang=EN-US><FONT class=extract>But the UMD can do things like&nbsp;</FONT><A href="http://www.daili987.com/weibo.com.php?u=uWbqt1PNYS2wXo0waqqb2SlkindZTHc4wKCFoG8Yfjk22g2TYiFNIlAr9HUxn4odTAnLb2wE5b%2ByXYqBUVStLhA%3D&amp;b=3"><B><FONT class=extract>swizzling textures</FONT></B></A><FONT class=extract>&nbsp;(unless the GPU can do this in hardware, usually using 2D blitting units not the real 3D pipeline) and schedule transfers between system memory and (mapped) video memory and the like. Most importantly, it can also write command buffers (or &#8220;DMA buffers&#8221; &#8211; I&#8217;ll be using these two names interchangeably) once the KMD has allocated them and handed them over. A command buffer contains, well, commands :). All your state-changing and drawing operations will be converted by the UMD into commands that the hardware understands. As will a lot of things you don&#8217;t trigger manually &#8211; such as uploading textures and shaders to video memory.</FONT></SPAN></P>
<P><SPAN style="FONT-SIZE: 16.5pt; mso-bidi-font-size: 12.0pt" lang=EN-US><FONT class=extract>In general, drivers will try to put as much of the actual processing into the UMD as possible; the UMD is user-mode code, so anything that runs in it doesn&#8217;t need any costly kernel-mode transitions, it can freely allocate memory, farm work out to multiple threads, and so on &#8211; it&#8217;s just a regular DLL (even though it&#8217;s loaded by the API, not directly by your app). This has advantages for driver development too &#8211; if the UMD crashes, the app crashes with it, but not the whole system; it can just be replaced while the system is running (it&#8217;s just a DLL!); it can be debugged with a regular debugger; and so on. So it&#8217;s not only efficient, it&#8217;s also convenient.</FONT></SPAN></P>
<P><SPAN style="FONT-SIZE: 16.5pt; mso-bidi-font-size: 12.0pt" lang=EN-US>But there&#8217;s a big elephant in the room that I haven&#8217;t mentioned yet.</SPAN>