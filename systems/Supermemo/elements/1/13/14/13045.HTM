<DIV class=navheader style="WORD-SPACING: 0px; FONT: medium Simsun; TEXT-TRANSFORM: none; COLOR: rgb(0,0,0); TEXT-INDENT: 0px; WHITE-SPACE: normal; LETTER-SPACING: normal; orphans: auto; widows: auto; -webkit-text-stroke-width: 0px">
<TABLE width="100%" summary="Navigation header">
<TBODY>
<TR>
<TH align=middle colSpan=3>4.16.&nbsp;Floating Point</TH></TR>
<TR>
<TD align=left width="20%"><A accessKey=p href="http://www.cs.uwm.edu/classes/cs315/Bacon/Lecture/HTML/ch04s15.html">Prev</A>&nbsp;</TD>
<TH align=middle width="60%">Chapter&nbsp;4.&nbsp;Data Representation</TH>
<TD align=right width="20%">&nbsp;<A accessKey=n href="http://www.cs.uwm.edu/classes/cs315/Bacon/Lecture/HTML/ch04s17.html">Next</A></TD></TR></TBODY></TABLE>
<HR>
</DIV>
<DIV class=section title="4.16.&nbsp;Floating Point" style="WORD-SPACING: 0px; FONT: medium Simsun; TEXT-TRANSFORM: none; COLOR: rgb(0,0,0); TEXT-INDENT: 0px; WHITE-SPACE: normal; LETTER-SPACING: normal; orphans: auto; widows: auto; -webkit-text-stroke-width: 0px">
<DIV class=titlepage>
<DIV>
<DIV>
<H2 class=title style="CLEAR: both"><A name=floating_point></A>4.16.&nbsp;Floating Point</H2></DIV></DIV></DIV>
<DIV class=section title="4.16.1.&nbsp;The Basics">
<DIV class=titlepage>
<DIV>
<DIV>
<H3 class=title><A name=id286956541></A>4.16.1.&nbsp;The Basics</H3></DIV></DIV></DIV>
<P>Floating point gets around the limitations of fixed point by using a format similar to scientific notation.</P><PRE class=screen>	3.52 x 10<SUP>3</SUP> = 1520
	</PRE>
<P>A scientific notation number, as you probably know, consists of a mantissa (3.52 in the example above) a radix (always 10), and an exponent (3 in the example above). Hence, the general format of a scientific notation value is:</P>
<P>mantissa x radix<SUP>exponent</SUP></P>
<P>The<SPAN class=Apple-converted-space>&nbsp;</SPAN><EM class=glossterm>normalized</EM><SPAN class=Apple-converted-space>&nbsp;</SPAN>form always has a mantissa greater than or equal to 1.0, and less than 10.0. We can denormalize the value and express it in many other ways, such as 35.2 x 10<SUP>2</SUP>, or 0.00325 x 10<SUP>0</SUP>. For each position we shift the digits of the mantissa relative to the decimal point, we increase or decrease the value of the mantissa by a factor of 10. To compensate for this, we simply increase or decrease the exponent by 1. Denormalizing is necessary when adding scientific notation values:</P><PRE class=screen>	    3.52 x 10<SUP>3</SUP>
	+   1.97 x 10<SUP>5</SUP>
	-----------------------------------------
	
	      1
	    0.0352 x 10<SUP>5</SUP>
	+   1.97   x 10<SUP>5</SUP>
	-----------------------------------------
	    2.0052 x 10<SUP>5</SUP>
	</PRE>
<P>Adjusting the mantissa and exponent is also sometimes necessary to normalize results. For example, 9.9 x 10<SUP>2</SUP><SPAN class=Apple-converted-space>&nbsp;</SPAN>+ 9.9 x 10<SUP>2</SUP><SPAN class=Apple-converted-space>&nbsp;</SPAN>is 19.8 x 10<SUP>2</SUP>, which must be normalized to 1.98 x 10<SUP>3</SUP>.</P>
<P>A binary floating system stores a signed binary mantissa and a signed binary exponent, and usually uses a radix of 2. Using a radix of 2 (or any power of 2) allows us to normalize and denormalize by shifting the binary digits in the mantissa and adjusting the integer exponent on the radix of 2. ( Shifting binary digits in the mantissa n bits to the left or right multiplies or divides the mantissa by 2<SUP>n</SUP>. )</P>
<P>00010<SUB>2</SUB><SPAN class=Apple-converted-space>&nbsp;</SPAN>x 2<SUP>3</SUP><SPAN class=Apple-converted-space>&nbsp;</SPAN>= 01000<SUB>2</SUB><SPAN class=Apple-converted-space>&nbsp;</SPAN>x 2<SUP>1</SUP>.</P>
<P>The standard floating point formats are defined by the IEEE society. The IEEE formats are slightly more complex that necessary to understand floating point in general, so we will start with a simpler example here.</P></DIV>
<DIV class=section title="4.16.2.&nbsp;A Simple Floating Point Format">
<DIV class=titlepage>
<DIV>
<DIV>
<H3 class=title><A name=id286956629></A>4.16.2.&nbsp;A Simple Floating Point Format</H3></DIV></DIV></DIV>
<P>Suppose a 32-bit floating point format has a 24-bit two's complement mantissa, an 8-bit two's complement exponent, and a radix of 2. The general structure is:</P>
<P>mantissa x 2<SUP>exponent</SUP></P>
<P>Where mantissa is a 24-bit two's complement integer, and exponent is an 8-bit two's complement integer.</P>
<P>The binary format is as follows:</P>
<DIV class=table><A name=floating_point_format></A>
<P class=title><B>Table&nbsp;4.5.&nbsp;Floating Point Format</B></P>
<DIV class=table-contents>
<TABLE summary="Floating Point Format" border=1>
<COLGROUP>
<COL>
<COL></COLGROUP>
<THEAD>
<TR>
<TH align=left>Mantissa</TH>
<TH align=left>Exponent</TH></TR></THEAD>
<TBODY>
<TR>
<TD align=left>24 bits, two's compliment</TD>
<TD align=left>8 bits, two's compliment</TD></TR></TBODY></TABLE></DIV></DIV><BR class=table-break>
<DIV class=orderedlist>
<OL class=orderedlist type=1>
<LI class=listitem>
<P>What is the value of the following number?</P><PRE class=screen>	    000000000000000000010010 11111100
	    </PRE>
<P>The mantissa is 000000000000000000010010, or +(2 + 16) = +18.</P>
<P>The exponent is 11111100 = -(00000011 + 1) = -00000100 = -4.</P>
<P>The value is therefore +18 x 2<SUP>-4</SUP></P></LI>
<LI class=listitem>
<P>What is the largest positive value we can represent in this system?</P>
<P>The largest positive value will consist of the largest positive mantissa and the largest positive exponent.</P>
<P>The largest mantissa is 011111111111111111111111, which in two's complement is +2<SUP>23</SUP>-1 (+8388607). The largest exponent is 01111111, which in two's complement is +2<SUP>7</SUP>-1 (+127).</P>
<P>Hence, the largest positive value is +8388607 x 2<SUP>+127</SUP><SPAN class=Apple-converted-space>&nbsp;</SPAN>= 1.42 x 10<SUP>45</SUP>.</P></LI>
<LI class=listitem>What is the second largest positive value? What is the difference between the largest and second largest?</LI>
<LI class=listitem>
<P>What is the smallest positive value?</P>
<P>To find the smallest positive value in the form mantissa x radix<SUP>exponent</SUP>, we choose the smallest positive mantissa, and the smallest negative exponent (the negative exponent with the greatest magnitude).</P>
<P>Since the mantissa is an integer, the smallest positive value possible is 1.</P>
<P>Since the exponent is an 8-bit two's complement value, the smallest negative exponent is 10000000<SUB>2</SUB>, of -2<SUP>7</SUP><SPAN class=Apple-converted-space>&nbsp;</SPAN>= -128.</P>
<P>Hence the smallest positive value is 1 x 2<SUP>-128</SUP>, or 2.93873587706 x 10<SUP>-39</SUP>.</P></LI>
<LI class=listitem>What is the second smallest positive value? What is the difference between the smallest and second smallest?</LI>
<LI class=listitem>Represent -2.75 in this floating point system.
<DIV class=orderedlist>
<OL class=orderedlist type=a>
<LI class=listitem>
<P>Convert the number to fixed point binary using the methods described in previous sections:</P>
<P>-2.75<SUB>10</SUB><SPAN class=Apple-converted-space>&nbsp;</SPAN>= -(10.11<SUB>2</SUB>)</P></LI>
<LI class=listitem>
<P>Multiply by radix<SUP>exponent</SUP><SPAN class=Apple-converted-space>&nbsp;</SPAN>equal to 1:</P>
<P>-2.75<SUB>10</SUB><SPAN class=Apple-converted-space>&nbsp;</SPAN>= -(10.11<SUB>2</SUB>) x 2<SUP>0</SUP></P></LI>
<LI class=listitem>
<P>Shift the binary point to make the mantissa a whole number: -(1011<SUB>2</SUB>)</P>
<P>By moving the binary point two places to the right, we multiply the mantissa by 2<SUP>2</SUP>. We therefore must divide (radix<SUP>exponent</SUP>) by the same factor:</P>
<P>-2.75<SUB>10</SUB><SPAN class=Apple-converted-space>&nbsp;</SPAN>= -(1011<SUB>2</SUB>) x 2<SUP>-2</SUP></P></LI>
<LI class=listitem>
<P>Convert the mantissa and exponent into the specified formats (two's complement in this case):</P>
<P>Mantissa: -(000000000000000000001011) = 111111111111111111110101</P>
<P>Exponent: -2<SUB>10</SUB><SPAN class=Apple-converted-space>&nbsp;</SPAN>= 11111110</P>
<P>Binary representation = 11111111111111111111010111111110</P></LI></OL></DIV></LI>
<LI class=listitem>How many different values can this system represent?</LI></OL></DIV></DIV>
<DIV class=section title="4.16.3.&nbsp;Overflow and Underflow">
<DIV class=titlepage>
<DIV>
<DIV>
<H3 class=title><A name=id286956853></A>4.16.3.&nbsp;Overflow and Underflow</H3></DIV></DIV></DIV>
<P><EM class=glossterm>Overflow</EM><SPAN class=Apple-converted-space>&nbsp;</SPAN>occurs when the result of a floating point operation is larger than the largest positive value, or smaller than the smallest negative value. In other words, the magnitude is too large to represent.</P>
<P><EM class=glossterm>Underflow</EM><SPAN class=Apple-converted-space>&nbsp;</SPAN>occurs when the result of a floating point operation is smaller than the smallest positive value, or larger than the largest negative value. In other words, the magnitude is too small to represent.</P>
<P>The example 32-bit format above cannot represent values larger than about 10<SUP>45</SUP><SPAN class=Apple-converted-space>&nbsp;</SPAN>or smaller than about 10<SUP>-39</SUP>.</P>
<P>One technique to avoid overflow and underflow is to alternate operations that increase and decrease intermediate results. Rather than do all the multiplications first, which could cause overflow, or all the divisions first, which could cause underflow, we could alternate multiplications and divisions to moderate the results along the way. Techniques like these must often be used in complex scientific calculations.</P></DIV>
<DIV class=section title="4.16.4.&nbsp;Cost of Floating Point">
<DIV class=titlepage>
<DIV>
<DIV>
<H3 class=title><A name=id286956877></A>4.16.4.&nbsp;Cost of Floating Point</H3></DIV></DIV></DIV>
<P>Everything has a cost. The increased range and ability to represent non-whole numbers is no exception.</P>
<DIV class=section title=Precision>
<DIV class=titlepage>
<DIV>
<DIV>
<H4 class=title><A name=id286956883></A>Precision</H4></DIV></DIV></DIV>
<P>There are only 2<SUP>32</SUP><SPAN class=Apple-converted-space>&nbsp;</SPAN>patterns of 32 0's and 1's. Hence, there are only 2<SUP>32</SUP><SPAN class=Apple-converted-space>&nbsp;</SPAN>unique numbers that we can represent in 32 bits, regardless of the format.</P>
<P>So how is it we can represent numbers up to 10<SUP>45</SUP>?</P>
<P>Obviously, we must be sacrificing something in between. What floating point does for us is spread out the limited number of binary patterns we have available to cover a larger range of numbers. The larger the exponent, the larger the gap between consecutive numbers that we can accurately represent.</P>
<P>Close to 0, we can represent many numbers in a small range. Far from zero, there will be a whole range of whole numbers that cannot be represented.</P>
<P>The precision of a 32-bit floating point value is less than the precision of a 32-bit integer. By using 8 bits for the exponent, we sacrifice those 8 bits of precision. Hence, our example format has the same precision as a 24-bit signed integer system.</P></DIV>
<DIV class=section title=Performance>
<DIV class=titlepage>
<DIV>
<DIV>
<H4 class=title><A name=id286956907></A>Performance</H4></DIV></DIV></DIV>
<P>Arithmetic on floating point is several times slower than on integers. This is an inherent property of the format.</P>
<P>Consider the process of adding two scientific notation values.</P>
<DIV class=orderedlist>
<OL class=orderedlist type=1>
<LI class=listitem>Equalize the exponents</LI>
<LI class=listitem>Add the mantissas</LI>
<LI class=listitem>Normalize the result</LI></OL></DIV>
<P>Each of these operations take roughly the same amount of time in a computer as a single integer addition. Since floating point is stored like scientific notation, we can expect floating point addition to take about three times as long as integer addition. In reality, a typical PC takes about 2.5 times as long to execute a floating point arithmetic instruction as it does to do the same integer instruction.</P>
<P></P>
<P>Note that this applies only to operations that can be carried out using either a single integer instruction or a single floating point instruction. For example, suppose a program is running on a 32-bit computer, and there is no way to represent the data within the range of a 32-bit integer. In this case, multiple integer instructions will be necessary to process integer values of more than 32 bits, and the speed advantage of integers does not apply.</P>
<P>It is also possible in some systems that floating point and integer operations could occur at the same time, and hence utilizing the floating point hardware could result in better performance than performing additional integer operations while the floating point unit sits idle. This is the case with graphics rendering that occurs using floating point on the graphics processing unit (GPU) rather than the CPU. It would not make sense to move the rendering calculations to the CPU in order to use integers, as this would only increase the workload for the CPU and allow the power of the GPU to go to waste.</P>
<P>If hardware has floating point support built-in, then common operations like floating point addition, subtraction, etc. can each be handled by a single instruction. If hardware doesn't have a floating point unit (common in embedded processors), floating point operations must be handled by software routines. Hence, adding two floating point values will require dozens of instructions to complete instead of just one. These will be hundreds of times slower than integers, and will consume a big chunk of available program memory.</P>
<P>Most algorithms can be implemented using integers with a little thought. Use of floating point is often the result of sheer laziness. Don't use floating point just because it's intuitive.</P>
<P>More power consumption. CPUs achieve their maximum power consumption when doing intensive floating point calculations. This is not usually noticeable on a desktop PC, but can become a problem on large grids consisting of hundreds of PCs, since the power grid they are attached to may not be designed to provide for their maximum draw. It can also be a problem when running a laptop on battery while doing intensive computations. Battery life while doing intensive floating point computations could be a small fraction of what it is while reading email, browsing the web, or editing a document in OpenOffice.</P></DIV></DIV></DIV>
<DIV class=navfooter style="WORD-SPACING: 0px; FONT: medium Simsun; TEXT-TRANSFORM: none; COLOR: rgb(0,0,0); TEXT-INDENT: 0px; WHITE-SPACE: normal; LETTER-SPACING: normal; orphans: auto; widows: auto; -webkit-text-stroke-width: 0px">
<HR>

<TABLE width="100%" summary="Navigation footer">
<TBODY>
<TR>
<TD align=left width="40%"><A accessKey=p href="http://www.cs.uwm.edu/classes/cs315/Bacon/Lecture/HTML/ch04s15.html">Prev</A>&nbsp;</TD>
<TD align=middle width="20%"><A accessKey=u href="http://www.cs.uwm.edu/classes/cs315/Bacon/Lecture/HTML/ch04.html">Up</A></TD>
<TD align=right width="40%">&nbsp;<A accessKey=n href="http://www.cs.uwm.edu/classes/cs315/Bacon/Lecture/HTML/ch04s17.html">Next</A></TD></TR>
<TR>
<TD vAlign=top align=left width="40%">4.15.&nbsp;Hex and Octal with Signed Numbers&nbsp;</TD>
<TD align=middle width="20%"><A accessKey=h href="http://www.cs.uwm.edu/classes/cs315/Bacon/Lecture/HTML/index.html">Home</A></TD>
<TD vAlign=top align=right width="40%">&nbsp;4.17.&nbsp;IEEE Floating Point Formats</TD></TR></TBODY></TABLE></DIV>