#include/linux/interrupt.h
<P></P>
<P>/* interrupt.h */<BR>#ifndef _LINUX_INTERRUPT_H<BR>#define _LINUX_INTERRUPT_H</P>
<P></P>
<P>#include &lt;linux/kernel.h&gt;<BR>#include &lt;linux/linkage.h&gt;<BR>#include &lt;linux/bitops.h&gt;<BR>#include &lt;linux/preempt.h&gt;<BR>#include &lt;linux/cpumask.h&gt;<BR>#include &lt;linux/irqreturn.h&gt;<BR>#include &lt;linux/irqnr.h&gt;<BR>#include &lt;linux/hardirq.h&gt;<BR>#include &lt;linux/irqflags.h&gt;<BR>#include &lt;linux/hrtimer.h&gt;<BR>#include &lt;linux/kref.h&gt;<BR>#include &lt;linux/workqueue.h&gt;</P>
<P>#include &lt;linux/atomic.h&gt;<BR>#include &lt;asm/ptrace.h&gt;<BR>#include &lt;asm/irq.h&gt;</P>
<P>/*<BR>&nbsp;* These correspond to the IORESOURCE_IRQ_* defines in<BR>&nbsp;* linux/ioport.h to select the interrupt line behaviour.&nbsp; When<BR>&nbsp;* requesting an interrupt without specifying a IRQF_TRIGGER, the<BR>&nbsp;* setting should be assumed to be "as already configured", which<BR>&nbsp;* may be as per machine or firmware initialisation.<BR>&nbsp;*/<BR>#define IRQF_TRIGGER_NONE&nbsp;0x00000000<BR>#define IRQF_TRIGGER_RISING&nbsp;0x00000001<BR>#define IRQF_TRIGGER_FALLING&nbsp;0x00000002<BR>#define IRQF_TRIGGER_HIGH&nbsp;0x00000004<BR>#define IRQF_TRIGGER_LOW&nbsp;0x00000008<BR>#define IRQF_TRIGGER_MASK&nbsp;(IRQF_TRIGGER_HIGH | IRQF_TRIGGER_LOW | \<BR>&nbsp;&nbsp;&nbsp;&nbsp; IRQF_TRIGGER_RISING | IRQF_TRIGGER_FALLING)<BR>#define IRQF_TRIGGER_PROBE&nbsp;0x00000010</P>
<P>/*<BR>&nbsp;* These flags used only by the kernel as part of the<BR>&nbsp;* irq handling routines.<BR>&nbsp;*<BR>&nbsp;* IRQF_SHARED - allow sharing the irq among several devices<BR>&nbsp;* IRQF_PROBE_SHARED - set by callers when they expect sharing mismatches to occur<BR>&nbsp;* IRQF_TIMER - Flag to mark this interrupt as timer interrupt<BR>&nbsp;* IRQF_PERCPU - Interrupt is per cpu<BR>&nbsp;* IRQF_NOBALANCING - Flag to exclude this interrupt from irq balancing<BR>&nbsp;* IRQF_IRQPOLL - Interrupt is used for polling (only the interrupt that is<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; registered first in an shared interrupt is considered for<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; performance reasons)<BR>&nbsp;* IRQF_ONESHOT - Interrupt is not reenabled after the hardirq handler finished.<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Used by threaded interrupts which need to keep the<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; irq line disabled until the threaded handler has been run.<BR>&nbsp;* IRQF_NO_SUSPEND - Do not disable this IRQ during suspend.&nbsp; Does not guarantee<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; that this interrupt will wake the system from a suspended<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; state.&nbsp; See Documentation/power/suspend-and-interrupts.txt<BR>&nbsp;* IRQF_FORCE_RESUME - Force enable it on resume even if IRQF_NO_SUSPEND is set<BR>&nbsp;* IRQF_NO_THREAD - Interrupt cannot be threaded<BR>&nbsp;* IRQF_EARLY_RESUME - Resume IRQ early during syscore instead of at device<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; resume time.<BR>&nbsp;* IRQF_COND_SUSPEND - If the IRQ is shared with a NO_SUSPEND user, execute this<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; interrupt handler after suspending interrupts. For system<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; wakeup devices users need to implement wakeup detection in<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; their interrupt handlers.<BR>&nbsp;*/<BR>#define IRQF_SHARED&nbsp;&nbsp;0x00000080<BR>#define IRQF_PROBE_SHARED&nbsp;0x00000100<BR>#define __IRQF_TIMER&nbsp;&nbsp;0x00000200<BR>#define IRQF_PERCPU&nbsp;&nbsp;0x00000400<BR>#define IRQF_NOBALANCING&nbsp;0x00000800<BR>#define IRQF_IRQPOLL&nbsp;&nbsp;0x00001000<BR>#define IRQF_ONESHOT&nbsp;&nbsp;0x00002000<BR>#define IRQF_NO_SUSPEND&nbsp;&nbsp;0x00004000<BR>#define IRQF_FORCE_RESUME&nbsp;0x00008000<BR>#define IRQF_NO_THREAD&nbsp;&nbsp;0x00010000<BR>#define IRQF_EARLY_RESUME&nbsp;0x00020000<BR>#define IRQF_COND_SUSPEND&nbsp;0x00040000</P>
<P>#define IRQF_TIMER&nbsp;&nbsp;(__IRQF_TIMER | IRQF_NO_SUSPEND | IRQF_NO_THREAD)</P>
<P>/*<BR>&nbsp;* These values can be returned by request_any_context_irq() and<BR>&nbsp;* describe the context the interrupt will be run in.<BR>&nbsp;*<BR>&nbsp;* IRQC_IS_HARDIRQ - interrupt runs in hardirq context<BR>&nbsp;* IRQC_IS_NESTED - interrupt runs in a nested threaded context<BR>&nbsp;*/<BR>enum {<BR>&nbsp;IRQC_IS_HARDIRQ&nbsp;= 0,<BR>&nbsp;IRQC_IS_NESTED,<BR>};</P>
<P>typedef irqreturn_t (*irq_handler_t)(int, void *);</P>
<P>/**<BR>&nbsp;* struct irqaction - per interrupt action descriptor<BR>&nbsp;* @handler:&nbsp;interrupt handler function<BR>&nbsp;* @name:&nbsp;name of the device<BR>&nbsp;* @dev_id:&nbsp;cookie to identify the device<BR>&nbsp;* @percpu_dev_id:&nbsp;cookie to identify the device<BR>&nbsp;* @next:&nbsp;pointer to the next irqaction for shared interrupts<BR>&nbsp;* @irq:&nbsp;interrupt number<BR>&nbsp;* @flags:&nbsp;flags (see IRQF_* above)<BR>&nbsp;* @thread_fn:&nbsp;interrupt handler function for threaded interrupts<BR>&nbsp;* @thread:&nbsp;thread pointer for threaded interrupts<BR>&nbsp;* @thread_flags:&nbsp;flags related to @thread<BR>&nbsp;* @thread_mask:&nbsp;bitmask for keeping track of @thread activity<BR>&nbsp;* @dir:&nbsp;pointer to the proc/irq/NN/name entry<BR>&nbsp;*/<BR>struct irqaction {<BR>&nbsp;irq_handler_t&nbsp;&nbsp;handler;<BR>&nbsp;void&nbsp;&nbsp;&nbsp;*dev_id;<BR>&nbsp;void __percpu&nbsp;&nbsp;*percpu_dev_id;<BR>&nbsp;struct irqaction&nbsp;*next;<BR>&nbsp;irq_handler_t&nbsp;&nbsp;thread_fn;<BR>&nbsp;struct task_struct&nbsp;*thread;<BR>&nbsp;unsigned int&nbsp;&nbsp;irq;<BR>&nbsp;unsigned int&nbsp;&nbsp;flags;<BR>&nbsp;unsigned long&nbsp;&nbsp;thread_flags;<BR>&nbsp;unsigned long&nbsp;&nbsp;thread_mask;<BR>&nbsp;const char&nbsp;&nbsp;*name;<BR>&nbsp;struct proc_dir_entry&nbsp;*dir;<BR>} ____cacheline_internodealigned_in_smp;</P>
<P>extern irqreturn_t no_action(int cpl, void *dev_id);</P>
<P>extern int __must_check<BR>request_threaded_irq(unsigned int irq, irq_handler_t handler,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; irq_handler_t thread_fn,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; unsigned long flags, const char *name, void *dev);</P>
<P>static inline int __must_check<BR>request_irq(unsigned int irq, irq_handler_t handler, unsigned long flags,<BR>&nbsp;&nbsp;&nbsp;&nbsp; const char *name, void *dev)<BR>{<BR>&nbsp;return request_threaded_irq(irq, handler, NULL, flags, name, dev);<BR>}</P>
<P>extern int __must_check<BR>request_any_context_irq(unsigned int irq, irq_handler_t handler,<BR>&nbsp;&nbsp;&nbsp;unsigned long flags, const char *name, void *dev_id);</P>
<P>extern int __must_check<BR>request_percpu_irq(unsigned int irq, irq_handler_t handler,<BR>&nbsp;&nbsp;&nbsp;&nbsp; const char *devname, void __percpu *percpu_dev_id);</P>
<P>extern void free_irq(unsigned int, void *);<BR>extern void free_percpu_irq(unsigned int, void __percpu *);</P>
<P>struct device;</P>
<P>extern int __must_check<BR>devm_request_threaded_irq(struct device *dev, unsigned int irq,<BR>&nbsp;&nbsp;&nbsp;&nbsp; irq_handler_t handler, irq_handler_t thread_fn,<BR>&nbsp;&nbsp;&nbsp;&nbsp; unsigned long irqflags, const char *devname,<BR>&nbsp;&nbsp;&nbsp;&nbsp; void *dev_id);</P>
<P>static inline int __must_check<BR>devm_request_irq(struct device *dev, unsigned int irq, irq_handler_t handler,<BR>&nbsp;&nbsp; unsigned long irqflags, const char *devname, void *dev_id)<BR>{<BR>&nbsp;return devm_request_threaded_irq(dev, irq, handler, NULL, irqflags,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; devname, dev_id);<BR>}</P>
<P>extern int __must_check<BR>devm_request_any_context_irq(struct device *dev, unsigned int irq,<BR>&nbsp;&nbsp; irq_handler_t handler, unsigned long irqflags,<BR>&nbsp;&nbsp; const char *devname, void *dev_id);</P>
<P>extern void devm_free_irq(struct device *dev, unsigned int irq, void *dev_id);</P>
<P>/*<BR>&nbsp;* On lockdep we dont want to enable hardirqs in hardirq<BR>&nbsp;* context. Use local_irq_enable_in_hardirq() to annotate<BR>&nbsp;* kernel code that has to do this nevertheless (pretty much<BR>&nbsp;* the only valid case is for old/broken hardware that is<BR>&nbsp;* insanely slow).<BR>&nbsp;*<BR>&nbsp;* NOTE: in theory this might break fragile code that relies<BR>&nbsp;* on hardirq delivery - in practice we dont seem to have such<BR>&nbsp;* places left. So the only effect should be slightly increased<BR>&nbsp;* irqs-off latencies.<BR>&nbsp;*/<BR>#ifdef CONFIG_LOCKDEP<BR># define local_irq_enable_in_hardirq()&nbsp;do { } while (0)<BR>#else<BR># define local_irq_enable_in_hardirq()&nbsp;local_irq_enable()<BR>#endif</P>
<P>extern void disable_irq_nosync(unsigned int irq);<BR>extern bool disable_hardirq(unsigned int irq);<BR>extern void disable_irq(unsigned int irq);<BR>extern void disable_percpu_irq(unsigned int irq);<BR>extern void enable_irq(unsigned int irq);<BR>extern void enable_percpu_irq(unsigned int irq, unsigned int type);<BR>extern void irq_wake_thread(unsigned int irq, void *dev_id);</P>
<P>/* The following three functions are for the core kernel use only. */<BR>extern void suspend_device_irqs(void);<BR>extern void resume_device_irqs(void);</P>
<P>/**<BR>&nbsp;* struct irq_affinity_notify - context for notification of IRQ affinity changes<BR>&nbsp;* @irq:&nbsp;&nbsp;Interrupt to which notification applies<BR>&nbsp;* @kref:&nbsp;&nbsp;Reference count, for internal use<BR>&nbsp;* @work:&nbsp;&nbsp;Work item, for internal use<BR>&nbsp;* @notify:&nbsp;&nbsp;Function to be called on change.&nbsp; This will be<BR>&nbsp;*&nbsp;&nbsp;&nbsp;called in process context.<BR>&nbsp;* @release:&nbsp;&nbsp;Function to be called on release.&nbsp; This will be<BR>&nbsp;*&nbsp;&nbsp;&nbsp;called in process context.&nbsp; Once registered, the<BR>&nbsp;*&nbsp;&nbsp;&nbsp;structure must only be freed when this function is<BR>&nbsp;*&nbsp;&nbsp;&nbsp;called or later.<BR>&nbsp;*/<BR>struct irq_affinity_notify {<BR>&nbsp;unsigned int irq;<BR>&nbsp;struct kref kref;<BR>&nbsp;struct work_struct work;<BR>&nbsp;void (*notify)(struct irq_affinity_notify *, const cpumask_t *mask);<BR>&nbsp;void (*release)(struct kref *ref);<BR>};</P>
<P>#if defined(CONFIG_SMP)</P>
<P>extern cpumask_var_t irq_default_affinity;</P>
<P>/* Internal implementation. Use the helpers below */<BR>extern int __irq_set_affinity(unsigned int irq, const struct cpumask *cpumask,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; bool force);</P>
<P>/**<BR>&nbsp;* irq_set_affinity - Set the irq affinity of a given irq<BR>&nbsp;* @irq:&nbsp;Interrupt to set affinity<BR>&nbsp;* @cpumask:&nbsp;cpumask<BR>&nbsp;*<BR>&nbsp;* Fails if cpumask does not contain an online CPU<BR>&nbsp;*/<BR>static inline int<BR>irq_set_affinity(unsigned int irq, const struct cpumask *cpumask)<BR>{<BR>&nbsp;return __irq_set_affinity(irq, cpumask, false);<BR>}</P>
<P>/**<BR>&nbsp;* irq_force_affinity - Force the irq affinity of a given irq<BR>&nbsp;* @irq:&nbsp;Interrupt to set affinity<BR>&nbsp;* @cpumask:&nbsp;cpumask<BR>&nbsp;*<BR>&nbsp;* Same as irq_set_affinity, but without checking the mask against<BR>&nbsp;* online cpus.<BR>&nbsp;*<BR>&nbsp;* Solely for low level cpu hotplug code, where we need to make per<BR>&nbsp;* cpu interrupts affine before the cpu becomes online.<BR>&nbsp;*/<BR>static inline int<BR>irq_force_affinity(unsigned int irq, const struct cpumask *cpumask)<BR>{<BR>&nbsp;return __irq_set_affinity(irq, cpumask, true);<BR>}</P>
<P>extern int irq_can_set_affinity(unsigned int irq);<BR>extern int irq_select_affinity(unsigned int irq);</P>
<P>extern int irq_set_affinity_hint(unsigned int irq, const struct cpumask *m);</P>
<P>extern int<BR>irq_set_affinity_notifier(unsigned int irq, struct irq_affinity_notify *notify);</P>
<P>#else /* CONFIG_SMP */</P>
<P>static inline int irq_set_affinity(unsigned int irq, const struct cpumask *m)<BR>{<BR>&nbsp;return -EINVAL;<BR>}</P>
<P>static inline int irq_force_affinity(unsigned int irq, const struct cpumask *cpumask)<BR>{<BR>&nbsp;return 0;<BR>}</P>
<P>static inline int irq_can_set_affinity(unsigned int irq)<BR>{<BR>&nbsp;return 0;<BR>}</P>
<P>static inline int irq_select_affinity(unsigned int irq)&nbsp; { return 0; }</P>
<P>static inline int irq_set_affinity_hint(unsigned int irq,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;const struct cpumask *m)<BR>{<BR>&nbsp;return -EINVAL;<BR>}</P>
<P>static inline int<BR>irq_set_affinity_notifier(unsigned int irq, struct irq_affinity_notify *notify)<BR>{<BR>&nbsp;return 0;<BR>}<BR>#endif /* CONFIG_SMP */</P>
<P>/*<BR>&nbsp;* Special lockdep variants of irq disabling/enabling.<BR>&nbsp;* These should be used for locking constructs that<BR>&nbsp;* know that a particular irq context which is disabled,<BR>&nbsp;* and which is the only irq-context user of a lock,<BR>&nbsp;* that it's safe to take the lock in the irq-disabled<BR>&nbsp;* section without disabling hardirqs.<BR>&nbsp;*<BR>&nbsp;* On !CONFIG_LOCKDEP they are equivalent to the normal<BR>&nbsp;* irq disable/enable methods.<BR>&nbsp;*/<BR>static inline void disable_irq_nosync_lockdep(unsigned int irq)<BR>{<BR>&nbsp;disable_irq_nosync(irq);<BR>#ifdef CONFIG_LOCKDEP<BR>&nbsp;local_irq_disable();<BR>#endif<BR>}</P>
<P>static inline void disable_irq_nosync_lockdep_irqsave(unsigned int irq, unsigned long *flags)<BR>{<BR>&nbsp;disable_irq_nosync(irq);<BR>#ifdef CONFIG_LOCKDEP<BR>&nbsp;local_irq_save(*flags);<BR>#endif<BR>}</P>
<P>static inline void disable_irq_lockdep(unsigned int irq)<BR>{<BR>&nbsp;disable_irq(irq);<BR>#ifdef CONFIG_LOCKDEP<BR>&nbsp;local_irq_disable();<BR>#endif<BR>}</P>
<P>static inline void enable_irq_lockdep(unsigned int irq)<BR>{<BR>#ifdef CONFIG_LOCKDEP<BR>&nbsp;local_irq_enable();<BR>#endif<BR>&nbsp;enable_irq(irq);<BR>}</P>
<P>static inline void enable_irq_lockdep_irqrestore(unsigned int irq, unsigned long *flags)<BR>{<BR>#ifdef CONFIG_LOCKDEP<BR>&nbsp;local_irq_restore(*flags);<BR>#endif<BR>&nbsp;enable_irq(irq);<BR>}</P>
<P>/* IRQ wakeup (PM) control: */<BR>extern int irq_set_irq_wake(unsigned int irq, unsigned int on);</P>
<P>static inline int enable_irq_wake(unsigned int irq)<BR>{<BR>&nbsp;return irq_set_irq_wake(irq, 1);<BR>}</P>
<P>static inline int disable_irq_wake(unsigned int irq)<BR>{<BR>&nbsp;return irq_set_irq_wake(irq, 0);<BR>}</P>
<P>/*<BR>&nbsp;* irq_get_irqchip_state/irq_set_irqchip_state specific flags<BR>&nbsp;*/<BR>enum irqchip_irq_state {<BR>&nbsp;IRQCHIP_STATE_PENDING,&nbsp;&nbsp;/* Is interrupt pending? */<BR>&nbsp;IRQCHIP_STATE_ACTIVE,&nbsp;&nbsp;/* Is interrupt in progress? */<BR>&nbsp;IRQCHIP_STATE_MASKED,&nbsp;&nbsp;/* Is interrupt masked? */<BR>&nbsp;IRQCHIP_STATE_LINE_LEVEL,&nbsp;/* Is IRQ line high? */<BR>};</P>
<P>extern int irq_get_irqchip_state(unsigned int irq, enum irqchip_irq_state which,<BR>&nbsp;&nbsp;&nbsp;&nbsp; bool *state);<BR>extern int irq_set_irqchip_state(unsigned int irq, enum irqchip_irq_state which,<BR>&nbsp;&nbsp;&nbsp;&nbsp; bool state);</P>
<P>#ifdef CONFIG_IRQ_FORCED_THREADING<BR>extern bool force_irqthreads;<BR>#else<BR>#define force_irqthreads&nbsp;(0)<BR>#endif</P>
<P>#ifndef __ARCH_SET_SOFTIRQ_PENDING<BR>#define set_softirq_pending(x) (local_softirq_pending() = (x))<BR>#define or_softirq_pending(x)&nbsp; (local_softirq_pending() |= (x))<BR>#endif</P>
<P>/* Some architectures might implement lazy enabling/disabling of<BR>&nbsp;* interrupts. In some cases, such as stop_machine, we might want<BR>&nbsp;* to ensure that after a local_irq_disable(), interrupts have<BR>&nbsp;* really been disabled in hardware. Such architectures need to<BR>&nbsp;* implement the following hook.<BR>&nbsp;*/<BR>#ifndef hard_irq_disable<BR>#define hard_irq_disable()&nbsp;do { } while(0)<BR>#endif</P>
<P>/* PLEASE, avoid to allocate new softirqs, if you need not _really_ high<BR>&nbsp;&nbsp; frequency threaded job scheduling. For almost all the purposes<BR>&nbsp;&nbsp; tasklets are more than enough. F.e. all serial device BHs et<BR>&nbsp;&nbsp; al. should be converted to tasklets, not to softirqs.<BR>&nbsp;*/</P>
<P>enum<BR>{<BR>&nbsp;HI_SOFTIRQ=0,<BR>&nbsp;TIMER_SOFTIRQ,<BR>&nbsp;NET_TX_SOFTIRQ,<BR>&nbsp;NET_RX_SOFTIRQ,<BR>&nbsp;BLOCK_SOFTIRQ,<BR>&nbsp;BLOCK_IOPOLL_SOFTIRQ,<BR>&nbsp;TASKLET_SOFTIRQ,<BR>&nbsp;SCHED_SOFTIRQ,<BR>&nbsp;HRTIMER_SOFTIRQ, /* Unused, but kept as tools rely on the<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; numbering. Sigh! */<BR>&nbsp;RCU_SOFTIRQ,&nbsp;&nbsp;&nbsp; /* Preferable RCU should always be the last softirq */</P>
<P>&nbsp;NR_SOFTIRQS<BR>};</P>
<P>#define SOFTIRQ_STOP_IDLE_MASK (~(1 &lt;&lt; RCU_SOFTIRQ))</P>
<P>/* map softirq index to softirq name. update 'softirq_to_name' in<BR>&nbsp;* kernel/softirq.c when adding a new softirq.<BR>&nbsp;*/<BR>extern const char * const softirq_to_name[NR_SOFTIRQS];</P>
<P>/* softirq mask and active fields moved to irq_cpustat_t in<BR>&nbsp;* asm/hardirq.h to get better cache usage.&nbsp; KAO<BR>&nbsp;*/</P>
<P>struct softirq_action<BR>{<BR>&nbsp;void&nbsp;(*action)(struct softirq_action *);<BR>};</P>
<P>asmlinkage void do_softirq(void);<BR>asmlinkage void __do_softirq(void);</P>
<P>#ifdef __ARCH_HAS_DO_SOFTIRQ<BR>void do_softirq_own_stack(void);<BR>#else<BR>static inline void do_softirq_own_stack(void)<BR>{<BR>&nbsp;__do_softirq();<BR>}<BR>#endif</P>
<P>extern void open_softirq(int nr, void (*action)(struct softirq_action *));<BR>extern void softirq_init(void);<BR>extern void __raise_softirq_irqoff(unsigned int nr);</P>
<P>extern void raise_softirq_irqoff(unsigned int nr);<BR>extern void raise_softirq(unsigned int nr);</P>
<P>DECLARE_PER_CPU(struct task_struct *, ksoftirqd);</P>
<P>static inline struct task_struct *this_cpu_ksoftirqd(void)<BR>{<BR>&nbsp;return this_cpu_read(ksoftirqd);<BR>}</P>
<P>/* Tasklets --- multithreaded analogue of BHs.</P>
<P>&nbsp;&nbsp; Main feature differing them of generic softirqs: tasklet<BR>&nbsp;&nbsp; is running only on one CPU simultaneously.</P>
<P>&nbsp;&nbsp; Main feature differing them of BHs: different tasklets<BR>&nbsp;&nbsp; may be run simultaneously on different CPUs.</P>
<P>&nbsp;&nbsp; Properties:<BR>&nbsp;&nbsp; * If tasklet_schedule() is called, then tasklet is guaranteed<BR>&nbsp;&nbsp;&nbsp;&nbsp; to be executed on some cpu at least once after this.<BR>&nbsp;&nbsp; * If the tasklet is already scheduled, but its execution is still not<BR>&nbsp;&nbsp;&nbsp;&nbsp; started, it will be executed only once.<BR>&nbsp;&nbsp; * If this tasklet is already running on another CPU (or schedule is called<BR>&nbsp;&nbsp;&nbsp;&nbsp; from tasklet itself), it is rescheduled for later.<BR>&nbsp;&nbsp; * Tasklet is strictly serialized wrt itself, but not<BR>&nbsp;&nbsp;&nbsp;&nbsp; wrt another tasklets. If client needs some intertask synchronization,<BR>&nbsp;&nbsp;&nbsp;&nbsp; he makes it with spinlocks.<BR>&nbsp;*/</P>
<P>struct tasklet_struct<BR>{<BR>&nbsp;struct tasklet_struct *next;<BR>&nbsp;unsigned long state;<BR>&nbsp;atomic_t count;<BR>&nbsp;void (*func)(unsigned long);<BR>&nbsp;unsigned long data;<BR>};</P>
<P>#define DECLARE_TASKLET(name, func, data) \<BR>struct tasklet_struct name = { NULL, 0, ATOMIC_INIT(0), func, data }</P>
<P>#define DECLARE_TASKLET_DISABLED(name, func, data) \<BR>struct tasklet_struct name = { NULL, 0, ATOMIC_INIT(1), func, data }</P>
<P><BR>enum<BR>{<BR>&nbsp;TASKLET_STATE_SCHED,&nbsp;/* Tasklet is scheduled for execution */<BR>&nbsp;TASKLET_STATE_RUN&nbsp;/* Tasklet is running (SMP only) */<BR>};</P>
<P>#ifdef CONFIG_SMP<BR>static inline int tasklet_trylock(struct tasklet_struct *t)<BR>{<BR>&nbsp;return !test_and_set_bit(TASKLET_STATE_RUN, &amp;(t)-&gt;state);<BR>}</P>
<P>static inline void tasklet_unlock(struct tasklet_struct *t)<BR>{<BR>&nbsp;smp_mb__before_atomic();<BR>&nbsp;clear_bit(TASKLET_STATE_RUN, &amp;(t)-&gt;state);<BR>}</P>
<P>static inline void tasklet_unlock_wait(struct tasklet_struct *t)<BR>{<BR>&nbsp;while (test_bit(TASKLET_STATE_RUN, &amp;(t)-&gt;state)) { barrier(); }<BR>}<BR>#else<BR>#define tasklet_trylock(t) 1<BR>#define tasklet_unlock_wait(t) do { } while (0)<BR>#define tasklet_unlock(t) do { } while (0)<BR>#endif</P>
<P>extern void __tasklet_schedule(struct tasklet_struct *t);</P>
<P>static inline void tasklet_schedule(struct tasklet_struct *t)<BR>{<BR>&nbsp;if (!test_and_set_bit(TASKLET_STATE_SCHED, &amp;t-&gt;state))<BR>&nbsp;&nbsp;__tasklet_schedule(t);<BR>}</P>
<P>extern void __tasklet_hi_schedule(struct tasklet_struct *t);</P>
<P>static inline void tasklet_hi_schedule(struct tasklet_struct *t)<BR>{<BR>&nbsp;if (!test_and_set_bit(TASKLET_STATE_SCHED, &amp;t-&gt;state))<BR>&nbsp;&nbsp;__tasklet_hi_schedule(t);<BR>}</P>
<P>extern void __tasklet_hi_schedule_first(struct tasklet_struct *t);</P>
<P>/*<BR>&nbsp;* This version avoids touching any other tasklets. Needed for kmemcheck<BR>&nbsp;* in order not to take any page faults while enqueueing this tasklet;<BR>&nbsp;* consider VERY carefully whether you really need this or<BR>&nbsp;* tasklet_hi_schedule()...<BR>&nbsp;*/<BR>static inline void tasklet_hi_schedule_first(struct tasklet_struct *t)<BR>{<BR>&nbsp;if (!test_and_set_bit(TASKLET_STATE_SCHED, &amp;t-&gt;state))<BR>&nbsp;&nbsp;__tasklet_hi_schedule_first(t);<BR>}</P>
<P><BR>static inline void tasklet_disable_nosync(struct tasklet_struct *t)<BR>{<BR>&nbsp;atomic_inc(&amp;t-&gt;count);<BR>&nbsp;smp_mb__after_atomic();<BR>}</P>
<P>static inline void tasklet_disable(struct tasklet_struct *t)<BR>{<BR>&nbsp;tasklet_disable_nosync(t);<BR>&nbsp;tasklet_unlock_wait(t);<BR>&nbsp;smp_mb();<BR>}</P>
<P>static inline void tasklet_enable(struct tasklet_struct *t)<BR>{<BR>&nbsp;smp_mb__before_atomic();<BR>&nbsp;atomic_dec(&amp;t-&gt;count);<BR>}</P>
<P>extern void tasklet_kill(struct tasklet_struct *t);<BR>extern void tasklet_kill_immediate(struct tasklet_struct *t, unsigned int cpu);<BR>extern void tasklet_init(struct tasklet_struct *t,<BR>&nbsp;&nbsp;&nbsp; void (*func)(unsigned long), unsigned long data);</P>
<P>struct tasklet_hrtimer {<BR>&nbsp;struct hrtimer&nbsp;&nbsp;timer;<BR>&nbsp;struct tasklet_struct&nbsp;tasklet;<BR>&nbsp;enum hrtimer_restart&nbsp;(*function)(struct hrtimer *);<BR>};</P>
<P>extern void<BR>tasklet_hrtimer_init(struct tasklet_hrtimer *ttimer,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; enum hrtimer_restart (*function)(struct hrtimer *),<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; clockid_t which_clock, enum hrtimer_mode mode);</P>
<P>static inline<BR>void tasklet_hrtimer_start(struct tasklet_hrtimer *ttimer, ktime_t time,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; const enum hrtimer_mode mode)<BR>{<BR>&nbsp;hrtimer_start(&amp;ttimer-&gt;timer, time, mode);<BR>}</P>
<P>static inline<BR>void tasklet_hrtimer_cancel(struct tasklet_hrtimer *ttimer)<BR>{<BR>&nbsp;hrtimer_cancel(&amp;ttimer-&gt;timer);<BR>&nbsp;tasklet_kill(&amp;ttimer-&gt;tasklet);<BR>}</P>
<P>/*<BR>&nbsp;* Autoprobing for irqs:<BR>&nbsp;*<BR>&nbsp;* probe_irq_on() and probe_irq_off() provide robust primitives<BR>&nbsp;* for accurate IRQ probing during kernel initialization.&nbsp; They are<BR>&nbsp;* reasonably simple to use, are not "fooled" by spurious interrupts,<BR>&nbsp;* and, unlike other attempts at IRQ probing, they do not get hung on<BR>&nbsp;* stuck interrupts (such as unused PS2 mouse interfaces on ASUS boards).<BR>&nbsp;*<BR>&nbsp;* For reasonably foolproof probing, use them as follows:<BR>&nbsp;*<BR>&nbsp;* 1. clear and/or mask the device's internal interrupt.<BR>&nbsp;* 2. sti();<BR>&nbsp;* 3. irqs = probe_irq_on();&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // "take over" all unassigned idle IRQs<BR>&nbsp;* 4. enable the device and cause it to trigger an interrupt.<BR>&nbsp;* 5. wait for the device to interrupt, using non-intrusive polling or a delay.<BR>&nbsp;* 6. irq = probe_irq_off(irqs);&nbsp; // get IRQ number, 0=none, negative=multiple<BR>&nbsp;* 7. service the device to clear its pending interrupt.<BR>&nbsp;* 8. loop again if paranoia is required.<BR>&nbsp;*<BR>&nbsp;* probe_irq_on() returns a mask of allocated irq's.<BR>&nbsp;*<BR>&nbsp;* probe_irq_off() takes the mask as a parameter,<BR>&nbsp;* and returns the irq number which occurred,<BR>&nbsp;* or zero if none occurred, or a negative irq number<BR>&nbsp;* if more than one irq occurred.<BR>&nbsp;*/</P>
<P>#if !defined(CONFIG_GENERIC_IRQ_PROBE) <BR>static inline unsigned long probe_irq_on(void)<BR>{<BR>&nbsp;return 0;<BR>}<BR>static inline int probe_irq_off(unsigned long val)<BR>{<BR>&nbsp;return 0;<BR>}<BR>static inline unsigned int probe_irq_mask(unsigned long val)<BR>{<BR>&nbsp;return 0;<BR>}<BR>#else<BR>extern unsigned long probe_irq_on(void);&nbsp;/* returns 0 on failure */<BR>extern int probe_irq_off(unsigned long);&nbsp;/* returns 0 or negative on failure */<BR>extern unsigned int probe_irq_mask(unsigned long);&nbsp;/* returns mask of ISA interrupts */<BR>#endif</P>
<P>#ifdef CONFIG_PROC_FS<BR>/* Initialize /proc/irq/ */<BR>extern void init_irq_proc(void);<BR>#else<BR>static inline void init_irq_proc(void)<BR>{<BR>}<BR>#endif</P>
<P>struct seq_file;<BR>int show_interrupts(struct seq_file *p, void *v);<BR>int arch_show_interrupts(struct seq_file *p, int prec);</P>
<P>extern int early_irq_init(void);<BR>extern int arch_probe_nr_irqs(void);<BR>extern int arch_early_irq_init(void);</P>
<P>#endif