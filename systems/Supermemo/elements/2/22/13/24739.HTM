v4.6 include/kernel/sched.h 
<P></P>
<P>#ifndef _LINUX_SCHED_H<BR>#define _LINUX_SCHED_H</P>
<P></P>
<P>#include &lt;uapi/linux/sched.h&gt;</P>
<P>#include &lt;linux/sched/prio.h&gt;</P>
<P><BR>struct sched_param {<BR>&nbsp;int sched_priority;<BR>};</P>
<P>#include &lt;asm/param.h&gt;&nbsp;/* for HZ */</P>
<P>#include &lt;linux/capability.h&gt;<BR>#include &lt;linux/threads.h&gt;<BR>#include &lt;linux/kernel.h&gt;<BR>#include &lt;linux/types.h&gt;<BR>#include &lt;linux/timex.h&gt;<BR>#include &lt;linux/jiffies.h&gt;<BR>#include &lt;linux/plist.h&gt;<BR>#include &lt;linux/rbtree.h&gt;<BR>#include &lt;linux/thread_info.h&gt;<BR>#include &lt;linux/cpumask.h&gt;<BR>#include &lt;linux/errno.h&gt;<BR>#include &lt;linux/nodemask.h&gt;<BR>#include &lt;linux/mm_types.h&gt;<BR>#include &lt;linux/preempt.h&gt;</P>
<P>#include &lt;asm/page.h&gt;<BR>#include &lt;asm/ptrace.h&gt;<BR>#include &lt;linux/cputime.h&gt;</P>
<P>#include &lt;linux/smp.h&gt;<BR>#include &lt;linux/sem.h&gt;<BR>#include &lt;linux/shm.h&gt;<BR>#include &lt;linux/signal.h&gt;<BR>#include &lt;linux/compiler.h&gt;<BR>#include &lt;linux/completion.h&gt;<BR>#include &lt;linux/pid.h&gt;<BR>#include &lt;linux/percpu.h&gt;<BR>#include &lt;linux/topology.h&gt;<BR>#include &lt;linux/seccomp.h&gt;<BR>#include &lt;linux/rcupdate.h&gt;<BR>#include &lt;linux/rculist.h&gt;<BR>#include &lt;linux/rtmutex.h&gt;</P>
<P>#include &lt;linux/time.h&gt;<BR>#include &lt;linux/param.h&gt;<BR>#include &lt;linux/resource.h&gt;<BR>#include &lt;linux/timer.h&gt;<BR>#include &lt;linux/hrtimer.h&gt;<BR>#include &lt;linux/kcov.h&gt;<BR>#include &lt;linux/task_io_accounting.h&gt;<BR>#include &lt;linux/latencytop.h&gt;<BR>#include &lt;linux/cred.h&gt;<BR>#include &lt;linux/llist.h&gt;<BR>#include &lt;linux/uidgid.h&gt;<BR>#include &lt;linux/gfp.h&gt;<BR>#include &lt;linux/magic.h&gt;<BR>#include &lt;linux/cgroup-defs.h&gt;</P>
<P>#include &lt;asm/processor.h&gt;</P>
<P>#define SCHED_ATTR_SIZE_VER0&nbsp;48&nbsp;/* sizeof first published struct */</P>
<P>/*<BR>&nbsp;* Extended scheduling parameters data structure.<BR>&nbsp;*<BR>&nbsp;* This is needed because the original struct sched_param can not be<BR>&nbsp;* altered without introducing ABI issues with legacy applications<BR>&nbsp;* (e.g., in sched_getparam()).<BR>&nbsp;*<BR>&nbsp;* However, the possibility of specifying more than just a priority for<BR>&nbsp;* the tasks may be useful for a wide variety of application fields, e.g.,<BR>&nbsp;* multimedia, streaming, automation and control, and many others.<BR>&nbsp;*<BR>&nbsp;* This variant (sched_attr) is meant at describing a so-called<BR>&nbsp;* sporadic time-constrained task. In such model a task is specified by:<BR>&nbsp;*&nbsp; - the activation period or minimum instance inter-arrival time;<BR>&nbsp;*&nbsp; - the maximum (or average, depending on the actual scheduling<BR>&nbsp;*&nbsp;&nbsp;&nbsp; discipline) computation time of all instances, a.k.a. runtime;<BR>&nbsp;*&nbsp; - the deadline (relative to the actual activation time) of each<BR>&nbsp;*&nbsp;&nbsp;&nbsp; instance.<BR>&nbsp;* Very briefly, a periodic (sporadic) task asks for the execution of<BR>&nbsp;* some specific computation --which is typically called an instance--<BR>&nbsp;* (at most) every period. Moreover, each instance typically lasts no more<BR>&nbsp;* than the runtime and must be completed by time instant t equal to<BR>&nbsp;* the instance activation time + the deadline.<BR>&nbsp;*<BR>&nbsp;* This is reflected by the actual fields of the sched_attr structure:<BR>&nbsp;*<BR>&nbsp;*&nbsp; @size&nbsp;&nbsp;size of the structure, for fwd/bwd compat.<BR>&nbsp;*<BR>&nbsp;*&nbsp; @sched_policy&nbsp;task's scheduling policy<BR>&nbsp;*&nbsp; @sched_flags&nbsp;for customizing the scheduler behaviour<BR>&nbsp;*&nbsp; @sched_nice&nbsp;&nbsp;task's nice value&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (SCHED_NORMAL/BATCH)<BR>&nbsp;*&nbsp; @sched_priority&nbsp;task's static priority (SCHED_FIFO/RR)<BR>&nbsp;*&nbsp; @sched_deadline&nbsp;representative of the task's deadline<BR>&nbsp;*&nbsp; @sched_runtime&nbsp;representative of the task's runtime<BR>&nbsp;*&nbsp; @sched_period&nbsp;representative of the task's period<BR>&nbsp;*<BR>&nbsp;* Given this task model, there are a multiplicity of scheduling algorithms<BR>&nbsp;* and policies, that can be used to ensure all the tasks will make their<BR>&nbsp;* timing constraints.<BR>&nbsp;*<BR>&nbsp;* As of now, the SCHED_DEADLINE policy (sched_dl scheduling class) is the<BR>&nbsp;* only user of this new interface. More information about the algorithm<BR>&nbsp;* available in the scheduling class file or in Documentation/.<BR>&nbsp;*/<BR>struct sched_attr {<BR>&nbsp;u32 size;</P>
<P>&nbsp;u32 sched_policy;<BR>&nbsp;u64 sched_flags;</P>
<P>&nbsp;/* SCHED_NORMAL, SCHED_BATCH */<BR>&nbsp;s32 sched_nice;</P>
<P>&nbsp;/* SCHED_FIFO, SCHED_RR */<BR>&nbsp;u32 sched_priority;</P>
<P>&nbsp;/* SCHED_DEADLINE */<BR>&nbsp;u64 sched_runtime;<BR>&nbsp;u64 sched_deadline;<BR>&nbsp;u64 sched_period;<BR>};</P>
<P>struct futex_pi_state;<BR>struct robust_list_head;<BR>struct bio_list;<BR>struct fs_struct;<BR>struct perf_event_context;<BR>struct blk_plug;<BR>struct filename;<BR>struct nameidata;</P>
<P>#define VMACACHE_BITS 2<BR>#define VMACACHE_SIZE (1U &lt;&lt; VMACACHE_BITS)<BR>#define VMACACHE_MASK (VMACACHE_SIZE - 1)</P>
<P>/*<BR>&nbsp;* These are the constant used to fake the fixed-point load-average<BR>&nbsp;* counting. Some notes:<BR>&nbsp;*&nbsp; - 11 bit fractions expand to 22 bits by the multiplies: this gives<BR>&nbsp;*&nbsp;&nbsp;&nbsp; a load-average precision of 10 bits integer + 11 bits fractional<BR>&nbsp;*&nbsp; - if you want to count load-averages more often, you need more<BR>&nbsp;*&nbsp;&nbsp;&nbsp; precision, or rounding will get you. With 2-second counting freq,<BR>&nbsp;*&nbsp;&nbsp;&nbsp; the EXP_n values would be 1981, 2034 and 2043 if still using only<BR>&nbsp;*&nbsp;&nbsp;&nbsp; 11 bit fractions.<BR>&nbsp;*/<BR>extern unsigned long avenrun[];&nbsp;&nbsp;/* Load averages */<BR>extern void get_avenrun(unsigned long *loads, unsigned long offset, int shift);</P>
<P>#define FSHIFT&nbsp;&nbsp;11&nbsp;&nbsp;/* nr of bits of precision */<BR>#define FIXED_1&nbsp;&nbsp;(1&lt;&lt;FSHIFT)&nbsp;/* 1.0 as fixed-point */<BR>#define LOAD_FREQ&nbsp;(5*HZ+1)&nbsp;/* 5 sec intervals */<BR>#define EXP_1&nbsp;&nbsp;1884&nbsp;&nbsp;/* 1/exp(5sec/1min) as fixed-point */<BR>#define EXP_5&nbsp;&nbsp;2014&nbsp;&nbsp;/* 1/exp(5sec/5min) */<BR>#define EXP_15&nbsp;&nbsp;2037&nbsp;&nbsp;/* 1/exp(5sec/15min) */</P>
<P>#define CALC_LOAD(load,exp,n) \<BR>&nbsp;load *= exp; \<BR>&nbsp;load += n*(FIXED_1-exp); \<BR>&nbsp;load &gt;&gt;= FSHIFT;</P>
<P>extern unsigned long total_forks;<BR>extern int nr_threads;<BR>DECLARE_PER_CPU(unsigned long, process_counts);<BR>extern int nr_processes(void);<BR>extern unsigned long nr_running(void);<BR>extern bool single_task_running(void);<BR>extern unsigned long nr_iowait(void);<BR>extern unsigned long nr_iowait_cpu(int cpu);<BR>extern void get_iowait_load(unsigned long *nr_waiters, unsigned long *load);</P>
<P>extern void calc_global_load(unsigned long ticks);</P>
<P>#if defined(CONFIG_SMP) &amp;&amp; defined(CONFIG_NO_HZ_COMMON)<BR>extern void cpu_load_update_nohz_start(void);<BR>extern void cpu_load_update_nohz_stop(void);<BR>#else<BR>static inline void cpu_load_update_nohz_start(void) { }<BR>static inline void cpu_load_update_nohz_stop(void) { }<BR>#endif</P>
<P>extern void dump_cpu_task(int cpu);</P>
<P>struct seq_file;<BR>struct cfs_rq;<BR>struct task_group;<BR>#ifdef CONFIG_SCHED_DEBUG<BR>extern void proc_sched_show_task(struct task_struct *p, struct seq_file *m);<BR>extern void proc_sched_set_task(struct task_struct *p);<BR>#endif</P>
<P>/*<BR>&nbsp;* Task state bitmask. NOTE! These bits are also<BR>&nbsp;* encoded in fs/proc/array.c: get_task_state().<BR>&nbsp;*<BR>&nbsp;* We have two separate sets of flags: task-&gt;state<BR>&nbsp;* is about runnability, while task-&gt;exit_state are<BR>&nbsp;* about the task exiting. Confusing, but this way<BR>&nbsp;* modifying one set can't modify the other one by<BR>&nbsp;* mistake.<BR>&nbsp;*/<BR>#define TASK_RUNNING&nbsp;&nbsp;0<BR>#define TASK_INTERRUPTIBLE&nbsp;1<BR>#define TASK_UNINTERRUPTIBLE&nbsp;2<BR>#define __TASK_STOPPED&nbsp;&nbsp;4<BR>#define __TASK_TRACED&nbsp;&nbsp;8<BR>/* in tsk-&gt;exit_state */<BR>#define EXIT_DEAD&nbsp;&nbsp;16<BR>#define EXIT_ZOMBIE&nbsp;&nbsp;32<BR>#define EXIT_TRACE&nbsp;&nbsp;(EXIT_ZOMBIE | EXIT_DEAD)<BR>/* in tsk-&gt;state again */<BR>#define TASK_DEAD&nbsp;&nbsp;64<BR>#define TASK_WAKEKILL&nbsp;&nbsp;128<BR>#define TASK_WAKING&nbsp;&nbsp;256<BR>#define TASK_PARKED&nbsp;&nbsp;512<BR>#define TASK_NOLOAD&nbsp;&nbsp;1024<BR>#define TASK_STATE_MAX&nbsp;&nbsp;2048</P>
<P>#define TASK_STATE_TO_CHAR_STR "RSDTtXZxKWPN"</P>
<P>extern char ___assert_task_state[1 - 2*!!(<BR>&nbsp;&nbsp;sizeof(TASK_STATE_TO_CHAR_STR)-1 != ilog2(TASK_STATE_MAX)+1)];</P>
<P>/* Convenience macros for the sake of set_task_state */<BR>#define TASK_KILLABLE&nbsp;&nbsp;(TASK_WAKEKILL | TASK_UNINTERRUPTIBLE)<BR>#define TASK_STOPPED&nbsp;&nbsp;(TASK_WAKEKILL | __TASK_STOPPED)<BR>#define TASK_TRACED&nbsp;&nbsp;(TASK_WAKEKILL | __TASK_TRACED)</P>
<P>#define TASK_IDLE&nbsp;&nbsp;(TASK_UNINTERRUPTIBLE | TASK_NOLOAD)</P>
<P>/* Convenience macros for the sake of wake_up */<BR>#define TASK_NORMAL&nbsp;&nbsp;(TASK_INTERRUPTIBLE | TASK_UNINTERRUPTIBLE)<BR>#define TASK_ALL&nbsp;&nbsp;(TASK_NORMAL | __TASK_STOPPED | __TASK_TRACED)</P>
<P>/* get_task_state() */<BR>#define TASK_REPORT&nbsp;&nbsp;(TASK_RUNNING | TASK_INTERRUPTIBLE | \<BR>&nbsp;&nbsp;&nbsp;&nbsp; TASK_UNINTERRUPTIBLE | __TASK_STOPPED | \<BR>&nbsp;&nbsp;&nbsp;&nbsp; __TASK_TRACED | EXIT_ZOMBIE | EXIT_DEAD)</P>
<P>#define task_is_traced(task)&nbsp;((task-&gt;state &amp; __TASK_TRACED) != 0)<BR>#define task_is_stopped(task)&nbsp;((task-&gt;state &amp; __TASK_STOPPED) != 0)<BR>#define task_is_stopped_or_traced(task)&nbsp;\<BR>&nbsp;&nbsp;&nbsp;((task-&gt;state &amp; (__TASK_STOPPED | __TASK_TRACED)) != 0)<BR>#define task_contributes_to_load(task)&nbsp;\<BR>&nbsp;&nbsp;&nbsp;&nbsp;((task-&gt;state &amp; TASK_UNINTERRUPTIBLE) != 0 &amp;&amp; \<BR>&nbsp;&nbsp;&nbsp;&nbsp; (task-&gt;flags &amp; PF_FROZEN) == 0 &amp;&amp; \<BR>&nbsp;&nbsp;&nbsp;&nbsp; (task-&gt;state &amp; TASK_NOLOAD) == 0)</P>
<P>#ifdef CONFIG_DEBUG_ATOMIC_SLEEP</P>
<P>#define __set_task_state(tsk, state_value)&nbsp;&nbsp;&nbsp;\<BR>&nbsp;do {&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;&nbsp;(tsk)-&gt;task_state_change = _THIS_IP_;&nbsp;&nbsp;\<BR>&nbsp;&nbsp;(tsk)-&gt;state = (state_value);&nbsp;&nbsp;&nbsp;\<BR>&nbsp;} while (0)<BR>#define set_task_state(tsk, state_value)&nbsp;&nbsp;&nbsp;\<BR>&nbsp;do {&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;&nbsp;(tsk)-&gt;task_state_change = _THIS_IP_;&nbsp;&nbsp;\<BR>&nbsp;&nbsp;smp_store_mb((tsk)-&gt;state, (state_value));&nbsp;&nbsp;\<BR>&nbsp;} while (0)</P>
<P>/*<BR>&nbsp;* set_current_state() includes a barrier so that the write of current-&gt;state<BR>&nbsp;* is correctly serialised wrt the caller's subsequent test of whether to<BR>&nbsp;* actually sleep:<BR>&nbsp;*<BR>&nbsp;*&nbsp;set_current_state(TASK_UNINTERRUPTIBLE);<BR>&nbsp;*&nbsp;if (do_i_need_to_sleep())<BR>&nbsp;*&nbsp;&nbsp;schedule();<BR>&nbsp;*<BR>&nbsp;* If the caller does not need such serialisation then use __set_current_state()<BR>&nbsp;*/<BR>#define __set_current_state(state_value)&nbsp;&nbsp;&nbsp;\<BR>&nbsp;do {&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;&nbsp;current-&gt;task_state_change = _THIS_IP_;&nbsp;&nbsp;\<BR>&nbsp;&nbsp;current-&gt;state = (state_value);&nbsp;&nbsp;&nbsp;\<BR>&nbsp;} while (0)<BR>#define set_current_state(state_value)&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;do {&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;&nbsp;current-&gt;task_state_change = _THIS_IP_;&nbsp;&nbsp;\<BR>&nbsp;&nbsp;smp_store_mb(current-&gt;state, (state_value));&nbsp;&nbsp;\<BR>&nbsp;} while (0)</P>
<P>#else</P>
<P>#define __set_task_state(tsk, state_value)&nbsp;&nbsp;\<BR>&nbsp;do { (tsk)-&gt;state = (state_value); } while (0)<BR>#define set_task_state(tsk, state_value)&nbsp;&nbsp;\<BR>&nbsp;smp_store_mb((tsk)-&gt;state, (state_value))</P>
<P>/*<BR>&nbsp;* set_current_state() includes a barrier so that the write of current-&gt;state<BR>&nbsp;* is correctly serialised wrt the caller's subsequent test of whether to<BR>&nbsp;* actually sleep:<BR>&nbsp;*<BR>&nbsp;*&nbsp;set_current_state(TASK_UNINTERRUPTIBLE);<BR>&nbsp;*&nbsp;if (do_i_need_to_sleep())<BR>&nbsp;*&nbsp;&nbsp;schedule();<BR>&nbsp;*<BR>&nbsp;* If the caller does not need such serialisation then use __set_current_state()<BR>&nbsp;*/<BR>#define __set_current_state(state_value)&nbsp;&nbsp;\<BR>&nbsp;do { current-&gt;state = (state_value); } while (0)<BR>#define set_current_state(state_value)&nbsp;&nbsp;&nbsp;\<BR>&nbsp;smp_store_mb(current-&gt;state, (state_value))</P>
<P>#endif</P>
<P>/* Task command name length */<BR>#define TASK_COMM_LEN 16</P>
<P>#include &lt;linux/spinlock.h&gt;</P>
<P>/*<BR>&nbsp;* This serializes "schedule()" and also protects<BR>&nbsp;* the run-queue from deletions/modifications (but<BR>&nbsp;* _adding_ to the beginning of the run-queue has<BR>&nbsp;* a separate lock).<BR>&nbsp;*/<BR>extern rwlock_t tasklist_lock;<BR>extern spinlock_t mmlist_lock;</P>
<P>struct task_struct;</P>
<P>#ifdef CONFIG_PROVE_RCU<BR>extern int lockdep_tasklist_lock_is_held(void);<BR>#endif /* #ifdef CONFIG_PROVE_RCU */</P>
<P>extern void sched_init(void);<BR>extern void sched_init_smp(void);<BR>extern asmlinkage void schedule_tail(struct task_struct *prev);<BR>extern void init_idle(struct task_struct *idle, int cpu);<BR>extern void init_idle_bootup_task(struct task_struct *idle);</P>
<P>extern cpumask_var_t cpu_isolated_map;</P>
<P>extern int runqueue_is_locked(int cpu);</P>
<P>#if defined(CONFIG_SMP) &amp;&amp; defined(CONFIG_NO_HZ_COMMON)<BR>extern void nohz_balance_enter_idle(int cpu);<BR>extern void set_cpu_sd_state_idle(void);<BR>extern int get_nohz_timer_target(void);<BR>#else<BR>static inline void nohz_balance_enter_idle(int cpu) { }<BR>static inline void set_cpu_sd_state_idle(void) { }<BR>#endif</P>
<P>/*<BR>&nbsp;* Only dump TASK_* tasks. (0 for all tasks)<BR>&nbsp;*/<BR>extern void show_state_filter(unsigned long state_filter);</P>
<P>static inline void show_state(void)<BR>{<BR>&nbsp;show_state_filter(0);<BR>}</P>
<P>extern void show_regs(struct pt_regs *);</P>
<P>/*<BR>&nbsp;* TASK is a pointer to the task whose backtrace we want to see (or NULL for current<BR>&nbsp;* task), SP is the stack pointer of the first frame that should be shown in the back<BR>&nbsp;* trace (or NULL if the entire call-chain of the task should be shown).<BR>&nbsp;*/<BR>extern void show_stack(struct task_struct *task, unsigned long *sp);</P>
<P>extern void cpu_init (void);<BR>extern void trap_init(void);<BR>extern void update_process_times(int user);<BR>extern void scheduler_tick(void);<BR>extern int sched_cpu_starting(unsigned int cpu);<BR>extern int sched_cpu_activate(unsigned int cpu);<BR>extern int sched_cpu_deactivate(unsigned int cpu);</P>
<P>#ifdef CONFIG_HOTPLUG_CPU<BR>extern int sched_cpu_dying(unsigned int cpu);<BR>#else<BR># define sched_cpu_dying&nbsp;NULL<BR>#endif</P>
<P>extern void sched_show_task(struct task_struct *p);</P>
<P>#ifdef CONFIG_LOCKUP_DETECTOR<BR>extern void touch_softlockup_watchdog_sched(void);<BR>extern void touch_softlockup_watchdog(void);<BR>extern void touch_softlockup_watchdog_sync(void);<BR>extern void touch_all_softlockup_watchdogs(void);<BR>extern int proc_dowatchdog_thresh(struct ctl_table *table, int write,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; void __user *buffer,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; size_t *lenp, loff_t *ppos);<BR>extern unsigned int&nbsp; softlockup_panic;<BR>extern unsigned int&nbsp; hardlockup_panic;<BR>void lockup_detector_init(void);<BR>#else<BR>static inline void touch_softlockup_watchdog_sched(void)<BR>{<BR>}<BR>static inline void touch_softlockup_watchdog(void)<BR>{<BR>}<BR>static inline void touch_softlockup_watchdog_sync(void)<BR>{<BR>}<BR>static inline void touch_all_softlockup_watchdogs(void)<BR>{<BR>}<BR>static inline void lockup_detector_init(void)<BR>{<BR>}<BR>#endif</P>
<P>#ifdef CONFIG_DETECT_HUNG_TASK<BR>void reset_hung_task_detector(void);<BR>#else<BR>static inline void reset_hung_task_detector(void)<BR>{<BR>}<BR>#endif</P>
<P>/* Attach to any functions which should be ignored in wchan output. */<BR>#define __sched&nbsp;&nbsp;__attribute__((__section__(".sched.text")))</P>
<P>/* Linker adds these: start and end of __sched functions */<BR>extern char __sched_text_start[], __sched_text_end[];</P>
<P>/* Is this address in the __sched functions? */<BR>extern int in_sched_functions(unsigned long addr);</P>
<P>#define&nbsp;MAX_SCHEDULE_TIMEOUT&nbsp;LONG_MAX<BR>extern signed long schedule_timeout(signed long timeout);<BR>extern signed long schedule_timeout_interruptible(signed long timeout);<BR>extern signed long schedule_timeout_killable(signed long timeout);<BR>extern signed long schedule_timeout_uninterruptible(signed long timeout);<BR>extern signed long schedule_timeout_idle(signed long timeout);<BR>asmlinkage void schedule(void);<BR>extern void schedule_preempt_disabled(void);</P>
<P>extern long io_schedule_timeout(long timeout);</P>
<P>static inline void io_schedule(void)<BR>{<BR>&nbsp;io_schedule_timeout(MAX_SCHEDULE_TIMEOUT);<BR>}</P>
<P>struct nsproxy;<BR>struct user_namespace;</P>
<P>#ifdef CONFIG_MMU<BR>extern void arch_pick_mmap_layout(struct mm_struct *mm);<BR>extern unsigned long<BR>arch_get_unmapped_area(struct file *, unsigned long, unsigned long,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; unsigned long, unsigned long);<BR>extern unsigned long<BR>arch_get_unmapped_area_topdown(struct file *filp, unsigned long addr,<BR>&nbsp;&nbsp;&nbsp;&nbsp; unsigned long len, unsigned long pgoff,<BR>&nbsp;&nbsp;&nbsp;&nbsp; unsigned long flags);<BR>#else<BR>static inline void arch_pick_mmap_layout(struct mm_struct *mm) {}<BR>#endif</P>
<P>#define SUID_DUMP_DISABLE&nbsp;0&nbsp;/* No setuid dumping */<BR>#define SUID_DUMP_USER&nbsp;&nbsp;1&nbsp;/* Dump as user of process */<BR>#define SUID_DUMP_ROOT&nbsp;&nbsp;2&nbsp;/* Dump as root */</P>
<P>/* mm flags */</P>
<P>/* for SUID_DUMP_* above */<BR>#define MMF_DUMPABLE_BITS 2<BR>#define MMF_DUMPABLE_MASK ((1 &lt;&lt; MMF_DUMPABLE_BITS) - 1)</P>
<P>extern void set_dumpable(struct mm_struct *mm, int value);<BR>/*<BR>&nbsp;* This returns the actual value of the suid_dumpable flag. For things<BR>&nbsp;* that are using this for checking for privilege transitions, it must<BR>&nbsp;* test against SUID_DUMP_USER rather than treating it as a boolean<BR>&nbsp;* value.<BR>&nbsp;*/<BR>static inline int __get_dumpable(unsigned long mm_flags)<BR>{<BR>&nbsp;return mm_flags &amp; MMF_DUMPABLE_MASK;<BR>}</P>
<P>static inline int get_dumpable(struct mm_struct *mm)<BR>{<BR>&nbsp;return __get_dumpable(mm-&gt;flags);<BR>}</P>
<P>/* coredump filter bits */<BR>#define MMF_DUMP_ANON_PRIVATE&nbsp;2<BR>#define MMF_DUMP_ANON_SHARED&nbsp;3<BR>#define MMF_DUMP_MAPPED_PRIVATE&nbsp;4<BR>#define MMF_DUMP_MAPPED_SHARED&nbsp;5<BR>#define MMF_DUMP_ELF_HEADERS&nbsp;6<BR>#define MMF_DUMP_HUGETLB_PRIVATE 7<BR>#define MMF_DUMP_HUGETLB_SHARED&nbsp; 8<BR>#define MMF_DUMP_DAX_PRIVATE&nbsp;9<BR>#define MMF_DUMP_DAX_SHARED&nbsp;10</P>
<P>#define MMF_DUMP_FILTER_SHIFT&nbsp;MMF_DUMPABLE_BITS<BR>#define MMF_DUMP_FILTER_BITS&nbsp;9<BR>#define MMF_DUMP_FILTER_MASK \<BR>&nbsp;(((1 &lt;&lt; MMF_DUMP_FILTER_BITS) - 1) &lt;&lt; MMF_DUMP_FILTER_SHIFT)<BR>#define MMF_DUMP_FILTER_DEFAULT \<BR>&nbsp;((1 &lt;&lt; MMF_DUMP_ANON_PRIVATE) |&nbsp;(1 &lt;&lt; MMF_DUMP_ANON_SHARED) |\<BR>&nbsp; (1 &lt;&lt; MMF_DUMP_HUGETLB_PRIVATE) | MMF_DUMP_MASK_DEFAULT_ELF)</P>
<P>#ifdef CONFIG_CORE_DUMP_DEFAULT_ELF_HEADERS<BR># define MMF_DUMP_MASK_DEFAULT_ELF&nbsp;(1 &lt;&lt; MMF_DUMP_ELF_HEADERS)<BR>#else<BR># define MMF_DUMP_MASK_DEFAULT_ELF&nbsp;0<BR>#endif<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;/* leave room for more dump flags */<BR>#define MMF_VM_MERGEABLE&nbsp;16&nbsp;/* KSM may merge identical pages */<BR>#define MMF_VM_HUGEPAGE&nbsp;&nbsp;17&nbsp;/* set when VM_HUGEPAGE is set on vma */<BR>#define MMF_EXE_FILE_CHANGED&nbsp;18&nbsp;/* see prctl_set_mm_exe_file() */</P>
<P>#define MMF_HAS_UPROBES&nbsp;&nbsp;19&nbsp;/* has uprobes */<BR>#define MMF_RECALC_UPROBES&nbsp;20&nbsp;/* MMF_HAS_UPROBES can be wrong */<BR>#define MMF_OOM_REAPED&nbsp;&nbsp;21&nbsp;/* mm has been already reaped */</P>
<P>#define MMF_INIT_MASK&nbsp;&nbsp;(MMF_DUMPABLE_MASK | MMF_DUMP_FILTER_MASK)</P>
<P>struct sighand_struct {<BR>&nbsp;atomic_t&nbsp;&nbsp;count;<BR>&nbsp;struct k_sigaction&nbsp;action[_NSIG];<BR>&nbsp;spinlock_t&nbsp;&nbsp;siglock;<BR>&nbsp;wait_queue_head_t&nbsp;signalfd_wqh;<BR>};</P>
<P>struct pacct_struct {<BR>&nbsp;int&nbsp;&nbsp;&nbsp;ac_flag;<BR>&nbsp;long&nbsp;&nbsp;&nbsp;ac_exitcode;<BR>&nbsp;unsigned long&nbsp;&nbsp;ac_mem;<BR>&nbsp;cputime_t&nbsp;&nbsp;ac_utime, ac_stime;<BR>&nbsp;unsigned long&nbsp;&nbsp;ac_minflt, ac_majflt;<BR>};</P>
<P>struct cpu_itimer {<BR>&nbsp;cputime_t expires;<BR>&nbsp;cputime_t incr;<BR>&nbsp;u32 error;<BR>&nbsp;u32 incr_error;<BR>};</P>
<P>/**<BR>&nbsp;* struct prev_cputime - snaphsot of system and user cputime<BR>&nbsp;* @utime: time spent in user mode<BR>&nbsp;* @stime: time spent in system mode<BR>&nbsp;* @lock: protects the above two fields<BR>&nbsp;*<BR>&nbsp;* Stores previous user/system time values such that we can guarantee<BR>&nbsp;* monotonicity.<BR>&nbsp;*/<BR>struct prev_cputime {<BR>#ifndef CONFIG_VIRT_CPU_ACCOUNTING_NATIVE<BR>&nbsp;cputime_t utime;<BR>&nbsp;cputime_t stime;<BR>&nbsp;raw_spinlock_t lock;<BR>#endif<BR>};</P>
<P>static inline void prev_cputime_init(struct prev_cputime *prev)<BR>{<BR>#ifndef CONFIG_VIRT_CPU_ACCOUNTING_NATIVE<BR>&nbsp;prev-&gt;utime = prev-&gt;stime = 0;<BR>&nbsp;raw_spin_lock_init(&amp;prev-&gt;lock);<BR>#endif<BR>}</P>
<P>/**<BR>&nbsp;* struct task_cputime - collected CPU time counts<BR>&nbsp;* @utime:&nbsp;&nbsp;time spent in user mode, in &amp;cputime_t units<BR>&nbsp;* @stime:&nbsp;&nbsp;time spent in kernel mode, in &amp;cputime_t units<BR>&nbsp;* @sum_exec_runtime:&nbsp;total time spent on the CPU, in nanoseconds<BR>&nbsp;*<BR>&nbsp;* This structure groups together three kinds of CPU time that are tracked for<BR>&nbsp;* threads and thread groups.&nbsp; Most things considering CPU time want to group<BR>&nbsp;* these counts together and treat all three of them in parallel.<BR>&nbsp;*/<BR>struct task_cputime {<BR>&nbsp;cputime_t utime;<BR>&nbsp;cputime_t stime;<BR>&nbsp;unsigned long long sum_exec_runtime;<BR>};</P>
<P>/* Alternate field names when used to cache expirations. */<BR>#define virt_exp&nbsp;utime<BR>#define prof_exp&nbsp;stime<BR>#define sched_exp&nbsp;sum_exec_runtime</P>
<P>#define INIT_CPUTIME&nbsp;\<BR>&nbsp;(struct task_cputime) {&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;&nbsp;.utime = 0,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;&nbsp;.stime = 0,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;&nbsp;.sum_exec_runtime = 0,&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;}</P>
<P>/*<BR>&nbsp;* This is the atomic variant of task_cputime, which can be used for<BR>&nbsp;* storing and updating task_cputime statistics without locking.<BR>&nbsp;*/<BR>struct task_cputime_atomic {<BR>&nbsp;atomic64_t utime;<BR>&nbsp;atomic64_t stime;<BR>&nbsp;atomic64_t sum_exec_runtime;<BR>};</P>
<P>#define INIT_CPUTIME_ATOMIC \<BR>&nbsp;(struct task_cputime_atomic) {&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;&nbsp;.utime = ATOMIC64_INIT(0),&nbsp;&nbsp;&nbsp;\<BR>&nbsp;&nbsp;.stime = ATOMIC64_INIT(0),&nbsp;&nbsp;&nbsp;\<BR>&nbsp;&nbsp;.sum_exec_runtime = ATOMIC64_INIT(0),&nbsp;&nbsp;\<BR>&nbsp;}</P>
<P>#define PREEMPT_DISABLED&nbsp;(PREEMPT_DISABLE_OFFSET + PREEMPT_ENABLED)</P>
<P>/*<BR>&nbsp;* Disable preemption until the scheduler is running -- use an unconditional<BR>&nbsp;* value so that it also works on !PREEMPT_COUNT kernels.<BR>&nbsp;*<BR>&nbsp;* Reset by start_kernel()-&gt;sched_init()-&gt;init_idle()-&gt;init_idle_preempt_count().<BR>&nbsp;*/<BR>#define INIT_PREEMPT_COUNT&nbsp;PREEMPT_OFFSET</P>
<P>/*<BR>&nbsp;* Initial preempt_count value; reflects the preempt_count schedule invariant<BR>&nbsp;* which states that during context switches:<BR>&nbsp;*<BR>&nbsp;*&nbsp;&nbsp;&nbsp; preempt_count() == 2*PREEMPT_DISABLE_OFFSET<BR>&nbsp;*<BR>&nbsp;* Note: PREEMPT_DISABLE_OFFSET is 0 for !PREEMPT_COUNT kernels.<BR>&nbsp;* Note: See finish_task_switch().<BR>&nbsp;*/<BR>#define FORK_PREEMPT_COUNT&nbsp;(2*PREEMPT_DISABLE_OFFSET + PREEMPT_ENABLED)</P>
<P>/**<BR>&nbsp;* struct thread_group_cputimer - thread group interval timer counts<BR>&nbsp;* @cputime_atomic:&nbsp;atomic thread group interval timers.<BR>&nbsp;* @running:&nbsp;&nbsp;true when there are timers running and<BR>&nbsp;*&nbsp;&nbsp;&nbsp;@cputime_atomic receives updates.<BR>&nbsp;* @checking_timer:&nbsp;true when a thread in the group is in the<BR>&nbsp;*&nbsp;&nbsp;&nbsp;process of checking for thread group timers.<BR>&nbsp;*<BR>&nbsp;* This structure contains the version of task_cputime, above, that is<BR>&nbsp;* used for thread group CPU timer calculations.<BR>&nbsp;*/<BR>struct thread_group_cputimer {<BR>&nbsp;struct task_cputime_atomic cputime_atomic;<BR>&nbsp;bool running;<BR>&nbsp;bool checking_timer;<BR>};</P>
<P>#include &lt;linux/rwsem.h&gt;<BR>struct autogroup;</P>
<P>/*<BR>&nbsp;* NOTE! "signal_struct" does not have its own<BR>&nbsp;* locking, because a shared signal_struct always<BR>&nbsp;* implies a shared sighand_struct, so locking<BR>&nbsp;* sighand_struct is always a proper superset of<BR>&nbsp;* the locking of signal_struct.<BR>&nbsp;*/<BR>struct signal_struct {<BR>&nbsp;atomic_t&nbsp;&nbsp;sigcnt;<BR>&nbsp;atomic_t&nbsp;&nbsp;live;<BR>&nbsp;int&nbsp;&nbsp;&nbsp;nr_threads;<BR>&nbsp;atomic_t oom_victims; /* # of TIF_MEDIE threads in this thread group */<BR>&nbsp;struct list_head&nbsp;thread_head;</P>
<P>&nbsp;wait_queue_head_t&nbsp;wait_chldexit;&nbsp;/* for wait4() */</P>
<P>&nbsp;/* current thread group signal load-balancing target: */<BR>&nbsp;struct task_struct&nbsp;*curr_target;</P>
<P>&nbsp;/* shared signal handling: */<BR>&nbsp;struct sigpending&nbsp;shared_pending;</P>
<P>&nbsp;/* thread group exit support */<BR>&nbsp;int&nbsp;&nbsp;&nbsp;group_exit_code;<BR>&nbsp;/* overloaded:<BR>&nbsp; * - notify group_exit_task when -&gt;count is equal to notify_count<BR>&nbsp; * - everyone except group_exit_task is stopped during signal delivery<BR>&nbsp; *&nbsp;&nbsp; of fatal signals, group_exit_task processes the signal.<BR>&nbsp; */<BR>&nbsp;int&nbsp;&nbsp;&nbsp;notify_count;<BR>&nbsp;struct task_struct&nbsp;*group_exit_task;</P>
<P>&nbsp;/* thread group stop support, overloads group_exit_code too */<BR>&nbsp;int&nbsp;&nbsp;&nbsp;group_stop_count;<BR>&nbsp;unsigned int&nbsp;&nbsp;flags; /* see SIGNAL_* flags below */</P>
<P>&nbsp;/*<BR>&nbsp; * PR_SET_CHILD_SUBREAPER marks a process, like a service<BR>&nbsp; * manager, to re-parent orphan (double-forking) child processes<BR>&nbsp; * to this process instead of 'init'. The service manager is<BR>&nbsp; * able to receive SIGCHLD signals and is able to investigate<BR>&nbsp; * the process until it calls wait(). All children of this<BR>&nbsp; * process will inherit a flag if they should look for a<BR>&nbsp; * child_subreaper process at exit.<BR>&nbsp; */<BR>&nbsp;unsigned int&nbsp;&nbsp;is_child_subreaper:1;<BR>&nbsp;unsigned int&nbsp;&nbsp;has_child_subreaper:1;</P>
<P>&nbsp;/* POSIX.1b Interval Timers */<BR>&nbsp;int&nbsp;&nbsp;&nbsp;posix_timer_id;<BR>&nbsp;struct list_head&nbsp;posix_timers;</P>
<P>&nbsp;/* ITIMER_REAL timer for the process */<BR>&nbsp;struct hrtimer real_timer;<BR>&nbsp;struct pid *leader_pid;<BR>&nbsp;ktime_t it_real_incr;</P>
<P>&nbsp;/*<BR>&nbsp; * ITIMER_PROF and ITIMER_VIRTUAL timers for the process, we use<BR>&nbsp; * CPUCLOCK_PROF and CPUCLOCK_VIRT for indexing array as these<BR>&nbsp; * values are defined to 0 and 1 respectively<BR>&nbsp; */<BR>&nbsp;struct cpu_itimer it[2];</P>
<P>&nbsp;/*<BR>&nbsp; * Thread group totals for process CPU timers.<BR>&nbsp; * See thread_group_cputimer(), et al, for details.<BR>&nbsp; */<BR>&nbsp;struct thread_group_cputimer cputimer;</P>
<P>&nbsp;/* Earliest-expiration cache. */<BR>&nbsp;struct task_cputime cputime_expires;</P>
<P>#ifdef CONFIG_NO_HZ_FULL<BR>&nbsp;atomic_t tick_dep_mask;<BR>#endif</P>
<P>&nbsp;struct list_head cpu_timers[3];</P>
<P>&nbsp;struct pid *tty_old_pgrp;</P>
<P>&nbsp;/* boolean value for session group leader */<BR>&nbsp;int leader;</P>
<P>&nbsp;struct tty_struct *tty; /* NULL if no tty */</P>
<P>#ifdef CONFIG_SCHED_AUTOGROUP<BR>&nbsp;struct autogroup *autogroup;<BR>#endif<BR>&nbsp;/*<BR>&nbsp; * Cumulative resource counters for dead threads in the group,<BR>&nbsp; * and for reaped dead child processes forked by this group.<BR>&nbsp; * Live threads maintain their own counters and add to these<BR>&nbsp; * in __exit_signal, except for the group leader.<BR>&nbsp; */<BR>&nbsp;seqlock_t stats_lock;<BR>&nbsp;cputime_t utime, stime, cutime, cstime;<BR>&nbsp;cputime_t gtime;<BR>&nbsp;cputime_t cgtime;<BR>&nbsp;struct prev_cputime prev_cputime;<BR>&nbsp;unsigned long nvcsw, nivcsw, cnvcsw, cnivcsw;<BR>&nbsp;unsigned long min_flt, maj_flt, cmin_flt, cmaj_flt;<BR>&nbsp;unsigned long inblock, oublock, cinblock, coublock;<BR>&nbsp;unsigned long maxrss, cmaxrss;<BR>&nbsp;struct task_io_accounting ioac;</P>
<P>&nbsp;/*<BR>&nbsp; * Cumulative ns of schedule CPU time fo dead threads in the<BR>&nbsp; * group, not including a zombie group leader, (This only differs<BR>&nbsp; * from jiffies_to_ns(utime + stime) if sched_clock uses something<BR>&nbsp; * other than jiffies.)<BR>&nbsp; */<BR>&nbsp;unsigned long long sum_sched_runtime;</P>
<P>&nbsp;/*<BR>&nbsp; * We don't bother to synchronize most readers of this at all,<BR>&nbsp; * because there is no reader checking a limit that actually needs<BR>&nbsp; * to get both rlim_cur and rlim_max atomically, and either one<BR>&nbsp; * alone is a single word that can safely be read normally.<BR>&nbsp; * getrlimit/setrlimit use task_lock(current-&gt;group_leader) to<BR>&nbsp; * protect this instead of the siglock, because they really<BR>&nbsp; * have no need to disable irqs.<BR>&nbsp; */<BR>&nbsp;struct rlimit rlim[RLIM_NLIMITS];</P>
<P>#ifdef CONFIG_BSD_PROCESS_ACCT<BR>&nbsp;struct pacct_struct pacct;&nbsp;/* per-process accounting information */<BR>#endif<BR>#ifdef CONFIG_TASKSTATS<BR>&nbsp;struct taskstats *stats;<BR>#endif<BR>#ifdef CONFIG_AUDIT<BR>&nbsp;unsigned audit_tty;<BR>&nbsp;struct tty_audit_buf *tty_audit_buf;<BR>#endif</P>
<P>&nbsp;oom_flags_t oom_flags;<BR>&nbsp;short oom_score_adj;&nbsp;&nbsp;/* OOM kill score adjustment */<BR>&nbsp;short oom_score_adj_min;&nbsp;/* OOM kill score adjustment min value.<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; * Only settable by CAP_SYS_RESOURCE. */</P>
<P>&nbsp;struct mutex cred_guard_mutex;&nbsp;/* guard against foreign influences on<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; * credential calculations<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; * (notably. ptrace) */<BR>};</P>
<P>/*<BR>&nbsp;* Bits in flags field of signal_struct.<BR>&nbsp;*/<BR>#define SIGNAL_STOP_STOPPED&nbsp;0x00000001 /* job control stop in effect */<BR>#define SIGNAL_STOP_CONTINUED&nbsp;0x00000002 /* SIGCONT since WCONTINUED reap */<BR>#define SIGNAL_GROUP_EXIT&nbsp;0x00000004 /* group exit in progress */<BR>#define SIGNAL_GROUP_COREDUMP&nbsp;0x00000008 /* coredump in progress */<BR>/*<BR>&nbsp;* Pending notifications to parent.<BR>&nbsp;*/<BR>#define SIGNAL_CLD_STOPPED&nbsp;0x00000010<BR>#define SIGNAL_CLD_CONTINUED&nbsp;0x00000020<BR>#define SIGNAL_CLD_MASK&nbsp;&nbsp;(SIGNAL_CLD_STOPPED|SIGNAL_CLD_CONTINUED)</P>
<P>#define SIGNAL_UNKILLABLE&nbsp;0x00000040 /* for init: ignore fatal signals */</P>
<P>/* If true, all threads except -&gt;group_exit_task have pending SIGKILL */<BR>static inline int signal_group_exit(const struct signal_struct *sig)<BR>{<BR>&nbsp;return&nbsp;(sig-&gt;flags &amp; SIGNAL_GROUP_EXIT) ||<BR>&nbsp;&nbsp;(sig-&gt;group_exit_task != NULL);<BR>}</P>
<P>/*<BR>&nbsp;* Some day this will be a full-fledged user tracking system..<BR>&nbsp;*/<BR>struct user_struct {<BR>&nbsp;atomic_t __count;&nbsp;/* reference count */<BR>&nbsp;atomic_t processes;&nbsp;/* How many processes does this user have? */<BR>&nbsp;atomic_t sigpending;&nbsp;/* How many pending signals does this user have? */<BR>#ifdef CONFIG_INOTIFY_USER<BR>&nbsp;atomic_t inotify_watches; /* How many inotify watches does this user have? */<BR>&nbsp;atomic_t inotify_devs;&nbsp;/* How many inotify devs does this user have opened? */<BR>#endif<BR>#ifdef CONFIG_FANOTIFY<BR>&nbsp;atomic_t fanotify_listeners;<BR>#endif<BR>#ifdef CONFIG_EPOLL<BR>&nbsp;atomic_long_t epoll_watches; /* The number of file descriptors currently watched */<BR>#endif<BR>#ifdef CONFIG_POSIX_MQUEUE<BR>&nbsp;/* protected by mq_lock&nbsp;*/<BR>&nbsp;unsigned long mq_bytes;&nbsp;/* How many bytes can be allocated to mqueue? */<BR>#endif<BR>&nbsp;unsigned long locked_shm; /* How many pages of mlocked shm ? */<BR>&nbsp;unsigned long unix_inflight;&nbsp;/* How many files in flight in unix sockets */<BR>&nbsp;atomic_long_t pipe_bufs;&nbsp; /* how many pages are allocated in pipe buffers */</P>
<P>#ifdef CONFIG_KEYS<BR>&nbsp;struct key *uid_keyring;&nbsp;/* UID specific keyring */<BR>&nbsp;struct key *session_keyring;&nbsp;/* UID's default session keyring */<BR>#endif</P>
<P>&nbsp;/* Hash table maintenance information */<BR>&nbsp;struct hlist_node uidhash_node;<BR>&nbsp;kuid_t uid;</P>
<P>#if defined(CONFIG_PERF_EVENTS) || defined(CONFIG_BPF_SYSCALL)<BR>&nbsp;atomic_long_t locked_vm;<BR>#endif<BR>};</P>
<P>extern int uids_sysfs_init(void);</P>
<P>extern struct user_struct *find_user(kuid_t);</P>
<P>extern struct user_struct root_user;<BR>#define INIT_USER (&amp;root_user)</P>
<P><BR>struct backing_dev_info;<BR>struct reclaim_state;</P>
<P>#ifdef CONFIG_SCHED_INFO<BR>struct sched_info {<BR>&nbsp;/* cumulative counters */<BR>&nbsp;unsigned long pcount;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; /* # of times run on this cpu */<BR>&nbsp;unsigned long long run_delay; /* time spent waiting on a runqueue */</P>
<P>&nbsp;/* timestamps */<BR>&nbsp;unsigned long long last_arrival,/* when we last ran on a cpu */<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; last_queued;&nbsp;/* when we were last queued to run */<BR>};<BR>#endif /* CONFIG_SCHED_INFO */</P>
<P>#ifdef CONFIG_TASK_DELAY_ACCT<BR>struct task_delay_info {<BR>&nbsp;spinlock_t&nbsp;lock;<BR>&nbsp;unsigned int&nbsp;flags;&nbsp;/* Private per-task flags */</P>
<P>&nbsp;/* For each stat XXX, add following, aligned appropriately<BR>&nbsp; *<BR>&nbsp; * struct timespec XXX_start, XXX_end;<BR>&nbsp; * u64 XXX_delay;<BR>&nbsp; * u32 XXX_count;<BR>&nbsp; *<BR>&nbsp; * Atomicity of updates to XXX_delay, XXX_count protected by<BR>&nbsp; * single lock above (split into XXX_lock if contention is an issue).<BR>&nbsp; */</P>
<P>&nbsp;/*<BR>&nbsp; * XXX_count is incremented on every XXX operation, the delay<BR>&nbsp; * associated with the operation is added to XXX_delay.<BR>&nbsp; * XXX_delay contains the accumulated delay time in nanoseconds.<BR>&nbsp; */<BR>&nbsp;u64 blkio_start;&nbsp;/* Shared by blkio, swapin */<BR>&nbsp;u64 blkio_delay;&nbsp;/* wait for sync block io completion */<BR>&nbsp;u64 swapin_delay;&nbsp;/* wait for swapin block io completion */<BR>&nbsp;u32 blkio_count;&nbsp;/* total count of the number of sync block */<BR>&nbsp;&nbsp;&nbsp;&nbsp;/* io operations performed */<BR>&nbsp;u32 swapin_count;&nbsp;/* total count of the number of swapin block */<BR>&nbsp;&nbsp;&nbsp;&nbsp;/* io operations performed */</P>
<P>&nbsp;u64 freepages_start;<BR>&nbsp;u64 freepages_delay;&nbsp;/* wait for memory reclaim */<BR>&nbsp;u32 freepages_count;&nbsp;/* total count of memory reclaim */<BR>};<BR>#endif&nbsp;/* CONFIG_TASK_DELAY_ACCT */</P>
<P>static inline int sched_info_on(void)<BR>{<BR>#ifdef CONFIG_SCHEDSTATS<BR>&nbsp;return 1;<BR>#elif defined(CONFIG_TASK_DELAY_ACCT)<BR>&nbsp;extern int delayacct_on;<BR>&nbsp;return delayacct_on;<BR>#else<BR>&nbsp;return 0;<BR>#endif<BR>}</P>
<P>#ifdef CONFIG_SCHEDSTATS<BR>void force_schedstat_enabled(void);<BR>#endif</P>
<P>enum cpu_idle_type {<BR>&nbsp;CPU_IDLE,<BR>&nbsp;CPU_NOT_IDLE,<BR>&nbsp;CPU_NEWLY_IDLE,<BR>&nbsp;CPU_MAX_IDLE_TYPES<BR>};</P>
<P>/*<BR>&nbsp;* Integer metrics need fixed point arithmetic, e.g., sched/fair<BR>&nbsp;* has a few: load, load_avg, util_avg, freq, and capacity.<BR>&nbsp;*<BR>&nbsp;* We define a basic fixed point arithmetic range, and then formalize<BR>&nbsp;* all these metrics based on that basic range.<BR>&nbsp;*/<BR># define SCHED_FIXEDPOINT_SHIFT&nbsp;10<BR># define SCHED_FIXEDPOINT_SCALE&nbsp;(1L &lt;&lt; SCHED_FIXEDPOINT_SHIFT)</P>
<P>/*<BR>&nbsp;* Increase resolution of cpu_capacity calculations<BR>&nbsp;*/<BR>#define SCHED_CAPACITY_SHIFT&nbsp;SCHED_FIXEDPOINT_SHIFT<BR>#define SCHED_CAPACITY_SCALE&nbsp;(1L &lt;&lt; SCHED_CAPACITY_SHIFT)</P>
<P>/*<BR>&nbsp;* Wake-queues are lists of tasks with a pending wakeup, whose<BR>&nbsp;* callers have already marked the task as woken internally,<BR>&nbsp;* and can thus carry on. A common use case is being able to<BR>&nbsp;* do the wakeups once the corresponding user lock as been<BR>&nbsp;* released.<BR>&nbsp;*<BR>&nbsp;* We hold reference to each task in the list across the wakeup,<BR>&nbsp;* thus guaranteeing that the memory is still valid by the time<BR>&nbsp;* the actual wakeups are performed in wake_up_q().<BR>&nbsp;*<BR>&nbsp;* One per task suffices, because there's never a need for a task to be<BR>&nbsp;* in two wake queues simultaneously; it is forbidden to abandon a task<BR>&nbsp;* in a wake queue (a call to wake_up_q() _must_ follow), so if a task is<BR>&nbsp;* already in a wake queue, the wakeup will happen soon and the second<BR>&nbsp;* waker can just skip it.<BR>&nbsp;*<BR>&nbsp;* The WAKE_Q macro declares and initializes the list head.<BR>&nbsp;* wake_up_q() does NOT reinitialize the list; it's expected to be<BR>&nbsp;* called near the end of a function, where the fact that the queue is<BR>&nbsp;* not used again will be easy to see by inspection.<BR>&nbsp;*<BR>&nbsp;* Note that this can cause spurious wakeups. schedule() callers<BR>&nbsp;* must ensure the call is done inside a loop, confirming that the<BR>&nbsp;* wakeup condition has in fact occurred.<BR>&nbsp;*/<BR>struct wake_q_node {<BR>&nbsp;struct wake_q_node *next;<BR>};</P>
<P>struct wake_q_head {<BR>&nbsp;struct wake_q_node *first;<BR>&nbsp;struct wake_q_node **lastp;<BR>};</P>
<P>#define WAKE_Q_TAIL ((struct wake_q_node *) 0x01)</P>
<P>#define WAKE_Q(name)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;struct wake_q_head name = { WAKE_Q_TAIL, &amp;name.first }</P>
<P>extern void wake_q_add(struct wake_q_head *head,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; struct task_struct *task);<BR>extern void wake_up_q(struct wake_q_head *head);</P>
<P>/*<BR>&nbsp;* sched-domains (multiprocessor balancing) declarations:<BR>&nbsp;*/<BR>#ifdef CONFIG_SMP<BR>#define SD_LOAD_BALANCE&nbsp;&nbsp;0x0001&nbsp;/* Do load balancing on this domain. */<BR>#define SD_BALANCE_NEWIDLE&nbsp;0x0002&nbsp;/* Balance when about to become idle */<BR>#define SD_BALANCE_EXEC&nbsp;&nbsp;0x0004&nbsp;/* Balance on exec */<BR>#define SD_BALANCE_FORK&nbsp;&nbsp;0x0008&nbsp;/* Balance on fork, clone */<BR>#define SD_BALANCE_WAKE&nbsp;&nbsp;0x0010&nbsp; /* Balance on wakeup */<BR>#define SD_WAKE_AFFINE&nbsp;&nbsp;0x0020&nbsp;/* Wake task to waking CPU */<BR>#define SD_SHARE_CPUCAPACITY&nbsp;0x0080&nbsp;/* Domain members share cpu power */<BR>#define SD_SHARE_POWERDOMAIN&nbsp;0x0100&nbsp;/* Domain members share power domain */<BR>#define SD_SHARE_PKG_RESOURCES&nbsp;0x0200&nbsp;/* Domain members share cpu pkg resources */<BR>#define SD_SERIALIZE&nbsp;&nbsp;0x0400&nbsp;/* Only a single load balancing instance */<BR>#define SD_ASYM_PACKING&nbsp;&nbsp;0x0800&nbsp; /* Place busy groups earlier in the domain */<BR>#define SD_PREFER_SIBLING&nbsp;0x1000&nbsp;/* Prefer to place tasks in a sibling domain */<BR>#define SD_OVERLAP&nbsp;&nbsp;0x2000&nbsp;/* sched_domains of this level overlap */<BR>#define SD_NUMA&nbsp;&nbsp;&nbsp;0x4000&nbsp;/* cross-node balancing */</P>
<P>#ifdef CONFIG_SCHED_SMT<BR>static inline int cpu_smt_flags(void)<BR>{<BR>&nbsp;return SD_SHARE_CPUCAPACITY | SD_SHARE_PKG_RESOURCES;<BR>}<BR>#endif</P>
<P>#ifdef CONFIG_SCHED_MC<BR>static inline int cpu_core_flags(void)<BR>{<BR>&nbsp;return SD_SHARE_PKG_RESOURCES;<BR>}<BR>#endif</P>
<P>#ifdef CONFIG_NUMA<BR>static inline int cpu_numa_flags(void)<BR>{<BR>&nbsp;return SD_NUMA;<BR>}<BR>#endif</P>
<P>struct sched_domain_attr {<BR>&nbsp;int relax_domain_level;<BR>};</P>
<P>#define SD_ATTR_INIT&nbsp;(struct sched_domain_attr) {&nbsp;\<BR>&nbsp;.relax_domain_level = -1,&nbsp;&nbsp;&nbsp;\<BR>}</P>
<P>extern int sched_domain_level_max;</P>
<P>struct sched_group;</P>
<P>struct sched_domain {<BR>&nbsp;/* These fields must be setup */<BR>&nbsp;struct sched_domain *parent;&nbsp;/* top domain must be null terminated */<BR>&nbsp;struct sched_domain *child;&nbsp;/* bottom domain must be null terminated */<BR>&nbsp;struct sched_group *groups;&nbsp;/* the balancing groups of the domain */<BR>&nbsp;unsigned long min_interval;&nbsp;/* Minimum balance interval ms */<BR>&nbsp;unsigned long max_interval;&nbsp;/* Maximum balance interval ms */<BR>&nbsp;unsigned int busy_factor;&nbsp;/* less balancing by factor if busy */<BR>&nbsp;unsigned int imbalance_pct;&nbsp;/* No balance until over watermark */<BR>&nbsp;unsigned int cache_nice_tries;&nbsp;/* Leave cache hot tasks for # tries */<BR>&nbsp;unsigned int busy_idx;<BR>&nbsp;unsigned int idle_idx;<BR>&nbsp;unsigned int newidle_idx;<BR>&nbsp;unsigned int wake_idx;<BR>&nbsp;unsigned int forkexec_idx;<BR>&nbsp;unsigned int smt_gain;</P>
<P>&nbsp;int nohz_idle;&nbsp;&nbsp;&nbsp;/* NOHZ IDLE status */<BR>&nbsp;int flags;&nbsp;&nbsp;&nbsp;/* See SD_* */<BR>&nbsp;int level;</P>
<P>&nbsp;/* Runtime fields. */<BR>&nbsp;unsigned long last_balance;&nbsp;/* init to jiffies. units in jiffies */<BR>&nbsp;unsigned int balance_interval;&nbsp;/* initialise to 1. units in ms. */<BR>&nbsp;unsigned int nr_balance_failed; /* initialise to 0 */</P>
<P>&nbsp;/* idle_balance() stats */<BR>&nbsp;u64 max_newidle_lb_cost;<BR>&nbsp;unsigned long next_decay_max_lb_cost;</P>
<P>#ifdef CONFIG_SCHEDSTATS<BR>&nbsp;/* load_balance() stats */<BR>&nbsp;unsigned int lb_count[CPU_MAX_IDLE_TYPES];<BR>&nbsp;unsigned int lb_failed[CPU_MAX_IDLE_TYPES];<BR>&nbsp;unsigned int lb_balanced[CPU_MAX_IDLE_TYPES];<BR>&nbsp;unsigned int lb_imbalance[CPU_MAX_IDLE_TYPES];<BR>&nbsp;unsigned int lb_gained[CPU_MAX_IDLE_TYPES];<BR>&nbsp;unsigned int lb_hot_gained[CPU_MAX_IDLE_TYPES];<BR>&nbsp;unsigned int lb_nobusyg[CPU_MAX_IDLE_TYPES];<BR>&nbsp;unsigned int lb_nobusyq[CPU_MAX_IDLE_TYPES];</P>
<P>&nbsp;/* Active load balancing */<BR>&nbsp;unsigned int alb_count;<BR>&nbsp;unsigned int alb_failed;<BR>&nbsp;unsigned int alb_pushed;</P>
<P>&nbsp;/* SD_BALANCE_EXEC stats */<BR>&nbsp;unsigned int sbe_count;<BR>&nbsp;unsigned int sbe_balanced;<BR>&nbsp;unsigned int sbe_pushed;</P>
<P>&nbsp;/* SD_BALANCE_FORK stats */<BR>&nbsp;unsigned int sbf_count;<BR>&nbsp;unsigned int sbf_balanced;<BR>&nbsp;unsigned int sbf_pushed;</P>
<P>&nbsp;/* try_to_wake_up() stats */<BR>&nbsp;unsigned int ttwu_wake_remote;<BR>&nbsp;unsigned int ttwu_move_affine;<BR>&nbsp;unsigned int ttwu_move_balance;<BR>#endif<BR>#ifdef CONFIG_SCHED_DEBUG<BR>&nbsp;char *name;<BR>#endif<BR>&nbsp;union {<BR>&nbsp;&nbsp;void *private;&nbsp;&nbsp;/* used during construction */<BR>&nbsp;&nbsp;struct rcu_head rcu;&nbsp;/* used during destruction */<BR>&nbsp;};</P>
<P>&nbsp;unsigned int span_weight;<BR>&nbsp;/*<BR>&nbsp; * Span of all CPUs in this domain.<BR>&nbsp; *<BR>&nbsp; * NOTE: this field is variable length. (Allocated dynamically<BR>&nbsp; * by attaching extra space to the end of the structure,<BR>&nbsp; * depending on how many CPUs the kernel has booted up with)<BR>&nbsp; */<BR>&nbsp;unsigned long span[0];<BR>};</P>
<P>static inline struct cpumask *sched_domain_span(struct sched_domain *sd)<BR>{<BR>&nbsp;return to_cpumask(sd-&gt;span);<BR>}</P>
<P>extern void partition_sched_domains(int ndoms_new, cpumask_var_t doms_new[],<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; struct sched_domain_attr *dattr_new);</P>
<P>/* Allocate an array of sched domains, for partition_sched_domains(). */<BR>cpumask_var_t *alloc_sched_domains(unsigned int ndoms);<BR>void free_sched_domains(cpumask_var_t doms[], unsigned int ndoms);</P>
<P>bool cpus_share_cache(int this_cpu, int that_cpu);</P>
<P>typedef const struct cpumask *(*sched_domain_mask_f)(int cpu);<BR>typedef int (*sched_domain_flags_f)(void);</P>
<P>#define SDTL_OVERLAP&nbsp;0x01</P>
<P>struct sd_data {<BR>&nbsp;struct sched_domain **__percpu sd;<BR>&nbsp;struct sched_group **__percpu sg;<BR>&nbsp;struct sched_group_capacity **__percpu sgc;<BR>};</P>
<P>struct sched_domain_topology_level {<BR>&nbsp;sched_domain_mask_f mask;<BR>&nbsp;sched_domain_flags_f sd_flags;<BR>&nbsp;int&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; flags;<BR>&nbsp;int&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; numa_level;<BR>&nbsp;struct sd_data&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; data;<BR>#ifdef CONFIG_SCHED_DEBUG<BR>&nbsp;char&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; *name;<BR>#endif<BR>};</P>
<P>extern void set_sched_topology(struct sched_domain_topology_level *tl);<BR>extern void wake_up_if_idle(int cpu);</P>
<P>#ifdef CONFIG_SCHED_DEBUG<BR># define SD_INIT_NAME(type)&nbsp;&nbsp;.name = #type<BR>#else<BR># define SD_INIT_NAME(type)<BR>#endif</P>
<P>#else /* CONFIG_SMP */</P>
<P>struct sched_domain_attr;</P>
<P>static inline void<BR>partition_sched_domains(int ndoms_new, cpumask_var_t doms_new[],<BR>&nbsp;&nbsp;&nbsp;struct sched_domain_attr *dattr_new)<BR>{<BR>}</P>
<P>static inline bool cpus_share_cache(int this_cpu, int that_cpu)<BR>{<BR>&nbsp;return true;<BR>}</P>
<P>#endif&nbsp;/* !CONFIG_SMP */</P>
<P><BR>struct io_context;&nbsp;&nbsp;&nbsp;/* See blkdev.h */</P>
<P><BR>#ifdef ARCH_HAS_PREFETCH_SWITCH_STACK<BR>extern void prefetch_stack(struct task_struct *t);<BR>#else<BR>static inline void prefetch_stack(struct task_struct *t) { }<BR>#endif</P>
<P>struct audit_context;&nbsp;&nbsp;/* See audit.c */<BR>struct mempolicy;<BR>struct pipe_inode_info;<BR>struct uts_namespace;</P>
<P>struct load_weight {<BR>&nbsp;unsigned long weight;<BR>&nbsp;u32 inv_weight;<BR>};</P>
<P>/*<BR>&nbsp;* The load_avg/util_avg accumulates an infinite geometric series<BR>&nbsp;* (see __update_load_avg() in kernel/sched/fair.c).<BR>&nbsp;*<BR>&nbsp;* [load_avg definition]<BR>&nbsp;*<BR>&nbsp;*&nbsp;&nbsp; load_avg = runnable% * scale_load_down(load)<BR>&nbsp;*<BR>&nbsp;* where runnable% is the time ratio that a sched_entity is runnable.<BR>&nbsp;* For cfs_rq, it is the aggregated load_avg of all runnable and<BR>&nbsp;* blocked sched_entities.<BR>&nbsp;*<BR>&nbsp;* load_avg may also take frequency scaling into account:<BR>&nbsp;*<BR>&nbsp;*&nbsp;&nbsp; load_avg = runnable% * scale_load_down(load) * freq%<BR>&nbsp;*<BR>&nbsp;* where freq% is the CPU frequency normalized to the highest frequency.<BR>&nbsp;*<BR>&nbsp;* [util_avg definition]<BR>&nbsp;*<BR>&nbsp;*&nbsp;&nbsp; util_avg = running% * SCHED_CAPACITY_SCALE<BR>&nbsp;*<BR>&nbsp;* where running% is the time ratio that a sched_entity is running on<BR>&nbsp;* a CPU. For cfs_rq, it is the aggregated util_avg of all runnable<BR>&nbsp;* and blocked sched_entities.<BR>&nbsp;*<BR>&nbsp;* util_avg may also factor frequency scaling and CPU capacity scaling:<BR>&nbsp;*<BR>&nbsp;*&nbsp;&nbsp; util_avg = running% * SCHED_CAPACITY_SCALE * freq% * capacity%<BR>&nbsp;*<BR>&nbsp;* where freq% is the same as above, and capacity% is the CPU capacity<BR>&nbsp;* normalized to the greatest capacity (due to uarch differences, etc).<BR>&nbsp;*<BR>&nbsp;* N.B., the above ratios (runnable%, running%, freq%, and capacity%)<BR>&nbsp;* themselves are in the range of [0, 1]. To do fixed point arithmetics,<BR>&nbsp;* we therefore scale them to as large a range as necessary. This is for<BR>&nbsp;* example reflected by util_avg's SCHED_CAPACITY_SCALE.<BR>&nbsp;*<BR>&nbsp;* [Overflow issue]<BR>&nbsp;*<BR>&nbsp;* The 64-bit load_sum can have 4353082796 (=2^64/47742/88761) entities<BR>&nbsp;* with the highest load (=88761), always runnable on a single cfs_rq,<BR>&nbsp;* and should not overflow as the number already hits PID_MAX_LIMIT.<BR>&nbsp;*<BR>&nbsp;* For all other cases (including 32-bit kernels), struct load_weight's<BR>&nbsp;* weight will overflow first before we do, because:<BR>&nbsp;*<BR>&nbsp;*&nbsp;&nbsp;&nbsp; Max(load_avg) &lt;= Max(load.weight)<BR>&nbsp;*<BR>&nbsp;* Then it is the load_weight's responsibility to consider overflow<BR>&nbsp;* issues.<BR>&nbsp;*/<BR>struct sched_avg {<BR>&nbsp;u64 last_update_time, load_sum;<BR>&nbsp;u32 util_sum, period_contrib;<BR>&nbsp;unsigned long load_avg, util_avg;<BR>};</P>
<P>#ifdef CONFIG_SCHEDSTATS<BR>struct sched_statistics {<BR>&nbsp;u64&nbsp;&nbsp;&nbsp;wait_start;<BR>&nbsp;u64&nbsp;&nbsp;&nbsp;wait_max;<BR>&nbsp;u64&nbsp;&nbsp;&nbsp;wait_count;<BR>&nbsp;u64&nbsp;&nbsp;&nbsp;wait_sum;<BR>&nbsp;u64&nbsp;&nbsp;&nbsp;iowait_count;<BR>&nbsp;u64&nbsp;&nbsp;&nbsp;iowait_sum;</P>
<P>&nbsp;u64&nbsp;&nbsp;&nbsp;sleep_start;<BR>&nbsp;u64&nbsp;&nbsp;&nbsp;sleep_max;<BR>&nbsp;s64&nbsp;&nbsp;&nbsp;sum_sleep_runtime;</P>
<P>&nbsp;u64&nbsp;&nbsp;&nbsp;block_start;<BR>&nbsp;u64&nbsp;&nbsp;&nbsp;block_max;<BR>&nbsp;u64&nbsp;&nbsp;&nbsp;exec_max;<BR>&nbsp;u64&nbsp;&nbsp;&nbsp;slice_max;</P>
<P>&nbsp;u64&nbsp;&nbsp;&nbsp;nr_migrations_cold;<BR>&nbsp;u64&nbsp;&nbsp;&nbsp;nr_failed_migrations_affine;<BR>&nbsp;u64&nbsp;&nbsp;&nbsp;nr_failed_migrations_running;<BR>&nbsp;u64&nbsp;&nbsp;&nbsp;nr_failed_migrations_hot;<BR>&nbsp;u64&nbsp;&nbsp;&nbsp;nr_forced_migrations;</P>
<P>&nbsp;u64&nbsp;&nbsp;&nbsp;nr_wakeups;<BR>&nbsp;u64&nbsp;&nbsp;&nbsp;nr_wakeups_sync;<BR>&nbsp;u64&nbsp;&nbsp;&nbsp;nr_wakeups_migrate;<BR>&nbsp;u64&nbsp;&nbsp;&nbsp;nr_wakeups_local;<BR>&nbsp;u64&nbsp;&nbsp;&nbsp;nr_wakeups_remote;<BR>&nbsp;u64&nbsp;&nbsp;&nbsp;nr_wakeups_affine;<BR>&nbsp;u64&nbsp;&nbsp;&nbsp;nr_wakeups_affine_attempts;<BR>&nbsp;u64&nbsp;&nbsp;&nbsp;nr_wakeups_passive;<BR>&nbsp;u64&nbsp;&nbsp;&nbsp;nr_wakeups_idle;<BR>};<BR>#endif</P>
<P>struct sched_entity {<BR>&nbsp;struct load_weight&nbsp;load;&nbsp;&nbsp;/* for load-balancing */<BR>&nbsp;struct rb_node&nbsp;&nbsp;run_node;<BR>&nbsp;struct list_head&nbsp;group_node;<BR>&nbsp;unsigned int&nbsp;&nbsp;on_rq;</P>
<P>&nbsp;u64&nbsp;&nbsp;&nbsp;exec_start;<BR>&nbsp;u64&nbsp;&nbsp;&nbsp;sum_exec_runtime;<BR>&nbsp;u64&nbsp;&nbsp;&nbsp;vruntime;<BR>&nbsp;u64&nbsp;&nbsp;&nbsp;prev_sum_exec_runtime;</P>
<P>&nbsp;u64&nbsp;&nbsp;&nbsp;nr_migrations;</P>
<P>#ifdef CONFIG_SCHEDSTATS<BR>&nbsp;struct sched_statistics statistics;<BR>#endif</P>
<P>#ifdef CONFIG_FAIR_GROUP_SCHED<BR>&nbsp;int&nbsp;&nbsp;&nbsp;depth;<BR>&nbsp;struct sched_entity&nbsp;*parent;<BR>&nbsp;/* rq on which this entity is (to be) queued: */<BR>&nbsp;struct cfs_rq&nbsp;&nbsp;*cfs_rq;<BR>&nbsp;/* rq "owned" by this entity/group: */<BR>&nbsp;struct cfs_rq&nbsp;&nbsp;*my_q;<BR>#endif</P>
<P>#ifdef CONFIG_SMP<BR>&nbsp;/*<BR>&nbsp; * Per entity load average tracking.<BR>&nbsp; *<BR>&nbsp; * Put into separate cache line so it does not<BR>&nbsp; * collide with read-mostly values above.<BR>&nbsp; */<BR>&nbsp;struct sched_avg&nbsp;avg ____cacheline_aligned_in_smp;<BR>#endif<BR>};</P>
<P>struct sched_rt_entity {<BR>&nbsp;struct list_head run_list;<BR>&nbsp;unsigned long timeout;<BR>&nbsp;unsigned long watchdog_stamp;<BR>&nbsp;unsigned int time_slice;<BR>&nbsp;unsigned short on_rq;<BR>&nbsp;unsigned short on_list;</P>
<P>&nbsp;struct sched_rt_entity *back;<BR>#ifdef CONFIG_RT_GROUP_SCHED<BR>&nbsp;struct sched_rt_entity&nbsp;*parent;<BR>&nbsp;/* rq on which this entity is (to be) queued: */<BR>&nbsp;struct rt_rq&nbsp;&nbsp;*rt_rq;<BR>&nbsp;/* rq "owned" by this entity/group: */<BR>&nbsp;struct rt_rq&nbsp;&nbsp;*my_q;<BR>#endif<BR>};</P>
<P>struct sched_dl_entity {<BR>&nbsp;struct rb_node&nbsp;rb_node;</P>
<P>&nbsp;/*<BR>&nbsp; * Original scheduling parameters. Copied here from sched_attr<BR>&nbsp; * during sched_setattr(), they will remain the same until<BR>&nbsp; * the next sched_setattr().<BR>&nbsp; */<BR>&nbsp;u64 dl_runtime;&nbsp;&nbsp;/* maximum runtime for each instance&nbsp;*/<BR>&nbsp;u64 dl_deadline;&nbsp;/* relative deadline of each instance&nbsp;*/<BR>&nbsp;u64 dl_period;&nbsp;&nbsp;/* separation of two instances (period) */<BR>&nbsp;u64 dl_bw;&nbsp;&nbsp;/* dl_runtime / dl_deadline&nbsp;&nbsp;*/</P>
<P>&nbsp;/*<BR>&nbsp; * Actual scheduling parameters. Initialized with the values above,<BR>&nbsp; * they are continously updated during task execution. Note that<BR>&nbsp; * the remaining runtime could be &lt; 0 in case we are in overrun.<BR>&nbsp; */<BR>&nbsp;s64 runtime;&nbsp;&nbsp;/* remaining runtime for this instance&nbsp;*/<BR>&nbsp;u64 deadline;&nbsp;&nbsp;/* absolute deadline for this instance&nbsp;*/<BR>&nbsp;unsigned int flags;&nbsp;/* specifying the scheduler behaviour&nbsp;*/</P>
<P>&nbsp;/*<BR>&nbsp; * Some bool flags:<BR>&nbsp; *<BR>&nbsp; * @dl_throttled tells if we exhausted the runtime. If so, the<BR>&nbsp; * task has to wait for a replenishment to be performed at the<BR>&nbsp; * next firing of dl_timer.<BR>&nbsp; *<BR>&nbsp; * @dl_boosted tells if we are boosted due to DI. If so we are<BR>&nbsp; * outside bandwidth enforcement mechanism (but only until we<BR>&nbsp; * exit the critical section);<BR>&nbsp; *<BR>&nbsp; * @dl_yielded tells if task gave up the cpu before consuming<BR>&nbsp; * all its available runtime during the last job.<BR>&nbsp; */<BR>&nbsp;int dl_throttled, dl_boosted, dl_yielded;</P>
<P>&nbsp;/*<BR>&nbsp; * Bandwidth enforcement timer. Each -deadline task has its<BR>&nbsp; * own bandwidth to be enforced, thus we need one timer per task.<BR>&nbsp; */<BR>&nbsp;struct hrtimer dl_timer;<BR>};</P>
<P>union rcu_special {<BR>&nbsp;struct {<BR>&nbsp;&nbsp;u8 blocked;<BR>&nbsp;&nbsp;u8 need_qs;<BR>&nbsp;&nbsp;u8 exp_need_qs;<BR>&nbsp;&nbsp;u8 pad;&nbsp;/* Otherwise the compiler can store garbage here. */<BR>&nbsp;} b; /* Bits. */<BR>&nbsp;u32 s; /* Set of bits. */<BR>};<BR>struct rcu_node;</P>
<P>enum perf_event_task_context {<BR>&nbsp;perf_invalid_context = -1,<BR>&nbsp;perf_hw_context = 0,<BR>&nbsp;perf_sw_context,<BR>&nbsp;perf_nr_task_contexts,<BR>};</P>
<P>/* Track pages that require TLB flushes */<BR>struct tlbflush_unmap_batch {<BR>&nbsp;/*<BR>&nbsp; * Each bit set is a CPU that potentially has a TLB entry for one of<BR>&nbsp; * the PFNs being flushed. See set_tlb_ubc_flush_pending().<BR>&nbsp; */<BR>&nbsp;struct cpumask cpumask;</P>
<P>&nbsp;/* True if any bit in cpumask is set */<BR>&nbsp;bool flush_required;</P>
<P>&nbsp;/*<BR>&nbsp; * If true then the PTE was dirty when unmapped. The entry must be<BR>&nbsp; * flushed before IO is initiated or a stale TLB entry potentially<BR>&nbsp; * allows an update without redirtying the page.<BR>&nbsp; */<BR>&nbsp;bool writable;<BR>};</P>
<P>struct task_struct {<BR>&nbsp;volatile long state;&nbsp;/* -1 unrunnable, 0 runnable, &gt;0 stopped */<BR>&nbsp;void *stack;<BR>&nbsp;atomic_t usage;<BR>&nbsp;unsigned int flags;&nbsp;/* per process flags, defined below */<BR>&nbsp;unsigned int ptrace;</P>
<P>#ifdef CONFIG_SMP<BR>&nbsp;struct llist_node wake_entry;<BR>&nbsp;int on_cpu;<BR>&nbsp;unsigned int wakee_flips;<BR>&nbsp;unsigned long wakee_flip_decay_ts;<BR>&nbsp;struct task_struct *last_wakee;</P>
<P>&nbsp;int wake_cpu;<BR>#endif<BR>&nbsp;int on_rq;</P>
<P>&nbsp;int prio, static_prio, normal_prio;<BR>&nbsp;unsigned int rt_priority;<BR>&nbsp;const struct sched_class *sched_class;<BR>&nbsp;struct sched_entity se;<BR>&nbsp;struct sched_rt_entity rt;<BR>#ifdef CONFIG_CGROUP_SCHED<BR>&nbsp;struct task_group *sched_task_group;<BR>#endif<BR>&nbsp;struct sched_dl_entity dl;</P>
<P>#ifdef CONFIG_PREEMPT_NOTIFIERS<BR>&nbsp;/* list of struct preempt_notifier: */<BR>&nbsp;struct hlist_head preempt_notifiers;<BR>#endif</P>
<P>#ifdef CONFIG_BLK_DEV_IO_TRACE<BR>&nbsp;unsigned int btrace_seq;<BR>#endif</P>
<P>&nbsp;unsigned int policy;<BR>&nbsp;int nr_cpus_allowed;<BR>&nbsp;cpumask_t cpus_allowed;</P>
<P>#ifdef CONFIG_PREEMPT_RCU<BR>&nbsp;int rcu_read_lock_nesting;<BR>&nbsp;union rcu_special rcu_read_unlock_special;<BR>&nbsp;struct list_head rcu_node_entry;<BR>&nbsp;struct rcu_node *rcu_blocked_node;<BR>#endif /* #ifdef CONFIG_PREEMPT_RCU */<BR>#ifdef CONFIG_TASKS_RCU<BR>&nbsp;unsigned long rcu_tasks_nvcsw;<BR>&nbsp;bool rcu_tasks_holdout;<BR>&nbsp;struct list_head rcu_tasks_holdout_list;<BR>&nbsp;int rcu_tasks_idle_cpu;<BR>#endif /* #ifdef CONFIG_TASKS_RCU */</P>
<P>#ifdef CONFIG_SCHED_INFO<BR>&nbsp;struct sched_info sched_info;<BR>#endif</P>
<P>&nbsp;struct list_head tasks;<BR>#ifdef CONFIG_SMP<BR>&nbsp;struct plist_node pushable_tasks;<BR>&nbsp;struct rb_node pushable_dl_tasks;<BR>#endif</P>
<P>&nbsp;struct mm_struct *mm, *active_mm;<BR>&nbsp;/* per-thread vma caching */<BR>&nbsp;u32 vmacache_seqnum;<BR>&nbsp;struct vm_area_struct *vmacache[VMACACHE_SIZE];<BR>#if defined(SPLIT_RSS_COUNTING)<BR>&nbsp;struct task_rss_stat&nbsp;rss_stat;<BR>#endif<BR>/* task state */<BR>&nbsp;int exit_state;<BR>&nbsp;int exit_code, exit_signal;<BR>&nbsp;int pdeath_signal;&nbsp; /*&nbsp; The signal sent when the parent dies&nbsp; */<BR>&nbsp;unsigned long jobctl;&nbsp;/* JOBCTL_*, siglock protected */</P>
<P>&nbsp;/* Used for emulating ABI behavior of previous Linux versions */<BR>&nbsp;unsigned int personality;</P>
<P>&nbsp;/* scheduler bits, serialized by scheduler locks */<BR>&nbsp;unsigned sched_reset_on_fork:1;<BR>&nbsp;unsigned sched_contributes_to_load:1;<BR>&nbsp;unsigned sched_migrated:1;<BR>&nbsp;unsigned :0; /* force alignment to the next boundary */</P>
<P>&nbsp;/* unserialized, strictly 'current' */<BR>&nbsp;unsigned in_execve:1; /* bit to tell LSMs we're in execve */<BR>&nbsp;unsigned in_iowait:1;<BR>#ifdef CONFIG_MEMCG<BR>&nbsp;unsigned memcg_may_oom:1;<BR>#ifndef CONFIG_SLOB<BR>&nbsp;unsigned memcg_kmem_skip_account:1;<BR>#endif<BR>#endif<BR>#ifdef CONFIG_COMPAT_BRK<BR>&nbsp;unsigned brk_randomized:1;<BR>#endif</P>
<P>&nbsp;unsigned long atomic_flags; /* Flags needing atomic access. */</P>
<P>&nbsp;struct restart_block restart_block;</P>
<P>&nbsp;pid_t pid;<BR>&nbsp;pid_t tgid;</P>
<P>#ifdef CONFIG_CC_STACKPROTECTOR<BR>&nbsp;/* Canary value for the -fstack-protector gcc feature */<BR>&nbsp;unsigned long stack_canary;<BR>#endif<BR>&nbsp;/*<BR>&nbsp; * pointers to (original) parent process, youngest child, younger sibling,<BR>&nbsp; * older sibling, respectively.&nbsp; (p-&gt;father can be replaced with<BR>&nbsp; * p-&gt;real_parent-&gt;pid)<BR>&nbsp; */<BR>&nbsp;struct task_struct __rcu *real_parent; /* real parent process */<BR>&nbsp;struct task_struct __rcu *parent; /* recipient of SIGCHLD, wait4() reports */<BR>&nbsp;/*<BR>&nbsp; * children/sibling forms the list of my natural children<BR>&nbsp; */<BR>&nbsp;struct list_head children;&nbsp;/* list of my children */<BR>&nbsp;struct list_head sibling;&nbsp;/* linkage in my parent's children list */<BR>&nbsp;struct task_struct *group_leader;&nbsp;/* threadgroup leader */</P>
<P>&nbsp;/*<BR>&nbsp; * ptraced is the list of tasks this task is using ptrace on.<BR>&nbsp; * This includes both natural children and PTRACE_ATTACH targets.<BR>&nbsp; * p-&gt;ptrace_entry is p's link on the p-&gt;parent-&gt;ptraced list.<BR>&nbsp; */<BR>&nbsp;struct list_head ptraced;<BR>&nbsp;struct list_head ptrace_entry;</P>
<P>&nbsp;/* PID/PID hash table linkage. */<BR>&nbsp;struct pid_link pids[PIDTYPE_MAX];<BR>&nbsp;struct list_head thread_group;<BR>&nbsp;struct list_head thread_node;</P>
<P>&nbsp;struct completion *vfork_done;&nbsp;&nbsp;/* for vfork() */<BR>&nbsp;int __user *set_child_tid;&nbsp;&nbsp;/* CLONE_CHILD_SETTID */<BR>&nbsp;int __user *clear_child_tid;&nbsp;&nbsp;/* CLONE_CHILD_CLEARTID */</P>
<P>&nbsp;cputime_t utime, stime, utimescaled, stimescaled;<BR>&nbsp;cputime_t gtime;<BR>&nbsp;struct prev_cputime prev_cputime;<BR>#ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN<BR>&nbsp;seqcount_t vtime_seqcount;<BR>&nbsp;unsigned long long vtime_snap;<BR>&nbsp;enum {<BR>&nbsp;&nbsp;/* Task is sleeping or running in a CPU with VTIME inactive */<BR>&nbsp;&nbsp;VTIME_INACTIVE = 0,<BR>&nbsp;&nbsp;/* Task runs in userspace in a CPU with VTIME active */<BR>&nbsp;&nbsp;VTIME_USER,<BR>&nbsp;&nbsp;/* Task runs in kernelspace in a CPU with VTIME active */<BR>&nbsp;&nbsp;VTIME_SYS,<BR>&nbsp;} vtime_snap_whence;<BR>#endif</P>
<P>#ifdef CONFIG_NO_HZ_FULL<BR>&nbsp;atomic_t tick_dep_mask;<BR>#endif<BR>&nbsp;unsigned long nvcsw, nivcsw; /* context switch counts */<BR>&nbsp;u64 start_time;&nbsp;&nbsp;/* monotonic time in nsec */<BR>&nbsp;u64 real_start_time;&nbsp;/* boot based time in nsec */<BR>/* mm fault and swap info: this can arguably be seen as either mm-specific or thread-specific */<BR>&nbsp;unsigned long min_flt, maj_flt;</P>
<P>&nbsp;struct task_cputime cputime_expires;<BR>&nbsp;struct list_head cpu_timers[3];</P>
<P>/* process credentials */<BR>&nbsp;const struct cred __rcu *real_cred; /* objective and real subjective task<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; * credentials (COW) */<BR>&nbsp;const struct cred __rcu *cred;&nbsp;/* effective (overridable) subjective task<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; * credentials (COW) */<BR>&nbsp;char comm[TASK_COMM_LEN]; /* executable name excluding path<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - access with [gs]et_task_comm (which lock<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; it with task_lock())<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - initialized normally by setup_new_exec */<BR>/* file system info */<BR>&nbsp;struct nameidata *nameidata;<BR>#ifdef CONFIG_SYSVIPC<BR>/* ipc stuff */<BR>&nbsp;struct sysv_sem sysvsem;<BR>&nbsp;struct sysv_shm sysvshm;<BR>#endif<BR>#ifdef CONFIG_DETECT_HUNG_TASK<BR>/* hung task detection */<BR>&nbsp;unsigned long last_switch_count;<BR>#endif<BR>/* filesystem information */<BR>&nbsp;struct fs_struct *fs;<BR>/* open file information */<BR>&nbsp;struct files_struct *files;<BR>/* namespaces */<BR>&nbsp;struct nsproxy *nsproxy;<BR>/* signal handlers */<BR>&nbsp;struct signal_struct *signal;<BR>&nbsp;struct sighand_struct *sighand;</P>
<P>&nbsp;sigset_t blocked, real_blocked;<BR>&nbsp;sigset_t saved_sigmask;&nbsp;/* restored if set_restore_sigmask() was used */<BR>&nbsp;struct sigpending pending;</P>
<P>&nbsp;unsigned long sas_ss_sp;<BR>&nbsp;size_t sas_ss_size;<BR>&nbsp;unsigned sas_ss_flags;</P>
<P>&nbsp;struct callback_head *task_works;</P>
<P>&nbsp;struct audit_context *audit_context;<BR>#ifdef CONFIG_AUDITSYSCALL<BR>&nbsp;kuid_t loginuid;<BR>&nbsp;unsigned int sessionid;<BR>#endif<BR>&nbsp;struct seccomp seccomp;</P>
<P>/* Thread group tracking */<BR>&nbsp;&nbsp; &nbsp;u32 parent_exec_id;<BR>&nbsp;&nbsp; &nbsp;u32 self_exec_id;<BR>/* Protection of (de-)allocation: mm, files, fs, tty, keyrings, mems_allowed,<BR>&nbsp;* mempolicy */<BR>&nbsp;spinlock_t alloc_lock;</P>
<P>&nbsp;/* Protection of the PI data structures: */<BR>&nbsp;raw_spinlock_t pi_lock;</P>
<P>&nbsp;struct wake_q_node wake_q;</P>
<P>#ifdef CONFIG_RT_MUTEXES<BR>&nbsp;/* PI waiters blocked on a rt_mutex held by this task */<BR>&nbsp;struct rb_root pi_waiters;<BR>&nbsp;struct rb_node *pi_waiters_leftmost;<BR>&nbsp;/* Deadlock detection and priority inheritance handling */<BR>&nbsp;struct rt_mutex_waiter *pi_blocked_on;<BR>#endif</P>
<P>#ifdef CONFIG_DEBUG_MUTEXES<BR>&nbsp;/* mutex deadlock detection */<BR>&nbsp;struct mutex_waiter *blocked_on;<BR>#endif<BR>#ifdef CONFIG_TRACE_IRQFLAGS<BR>&nbsp;unsigned int irq_events;<BR>&nbsp;unsigned long hardirq_enable_ip;<BR>&nbsp;unsigned long hardirq_disable_ip;<BR>&nbsp;unsigned int hardirq_enable_event;<BR>&nbsp;unsigned int hardirq_disable_event;<BR>&nbsp;int hardirqs_enabled;<BR>&nbsp;int hardirq_context;<BR>&nbsp;unsigned long softirq_disable_ip;<BR>&nbsp;unsigned long softirq_enable_ip;<BR>&nbsp;unsigned int softirq_disable_event;<BR>&nbsp;unsigned int softirq_enable_event;<BR>&nbsp;int softirqs_enabled;<BR>&nbsp;int softirq_context;<BR>#endif<BR>#ifdef CONFIG_LOCKDEP<BR># define MAX_LOCK_DEPTH 48UL<BR>&nbsp;u64 curr_chain_key;<BR>&nbsp;int lockdep_depth;<BR>&nbsp;unsigned int lockdep_recursion;<BR>&nbsp;struct held_lock held_locks[MAX_LOCK_DEPTH];<BR>&nbsp;gfp_t lockdep_reclaim_gfp;<BR>#endif<BR>#ifdef CONFIG_UBSAN<BR>&nbsp;unsigned int in_ubsan;<BR>#endif</P>
<P>/* journalling filesystem info */<BR>&nbsp;void *journal_info;</P>
<P>/* stacked block device info */<BR>&nbsp;struct bio_list *bio_list;</P>
<P>#ifdef CONFIG_BLOCK<BR>/* stack plugging */<BR>&nbsp;struct blk_plug *plug;<BR>#endif</P>
<P>/* VM state */<BR>&nbsp;struct reclaim_state *reclaim_state;</P>
<P>&nbsp;struct backing_dev_info *backing_dev_info;</P>
<P>&nbsp;struct io_context *io_context;</P>
<P>&nbsp;unsigned long ptrace_message;<BR>&nbsp;siginfo_t *last_siginfo; /* For ptrace use.&nbsp; */<BR>&nbsp;struct task_io_accounting ioac;<BR>#if defined(CONFIG_TASK_XACCT)<BR>&nbsp;u64 acct_rss_mem1;&nbsp;/* accumulated rss usage */<BR>&nbsp;u64 acct_vm_mem1;&nbsp;/* accumulated virtual memory usage */<BR>&nbsp;cputime_t acct_timexpd;&nbsp;/* stime + utime since last update */<BR>#endif<BR>#ifdef CONFIG_CPUSETS<BR>&nbsp;nodemask_t mems_allowed;&nbsp;/* Protected by alloc_lock */<BR>&nbsp;seqcount_t mems_allowed_seq;&nbsp;/* Seqence no to catch updates */<BR>&nbsp;int cpuset_mem_spread_rotor;<BR>&nbsp;int cpuset_slab_spread_rotor;<BR>#endif<BR>#ifdef CONFIG_CGROUPS<BR>&nbsp;/* Control Group info protected by css_set_lock */<BR>&nbsp;struct css_set __rcu *cgroups;<BR>&nbsp;/* cg_list protected by css_set_lock and tsk-&gt;alloc_lock */<BR>&nbsp;struct list_head cg_list;<BR>#endif<BR>#ifdef CONFIG_FUTEX<BR>&nbsp;struct robust_list_head __user *robust_list;<BR>#ifdef CONFIG_COMPAT<BR>&nbsp;struct compat_robust_list_head __user *compat_robust_list;<BR>#endif<BR>&nbsp;struct list_head pi_state_list;<BR>&nbsp;struct futex_pi_state *pi_state_cache;<BR>#endif<BR>#ifdef CONFIG_PERF_EVENTS<BR>&nbsp;struct perf_event_context *perf_event_ctxp[perf_nr_task_contexts];<BR>&nbsp;struct mutex perf_event_mutex;<BR>&nbsp;struct list_head perf_event_list;<BR>#endif<BR>#ifdef CONFIG_DEBUG_PREEMPT<BR>&nbsp;unsigned long preempt_disable_ip;<BR>#endif<BR>#ifdef CONFIG_NUMA<BR>&nbsp;struct mempolicy *mempolicy;&nbsp;/* Protected by alloc_lock */<BR>&nbsp;short il_next;<BR>&nbsp;short pref_node_fork;<BR>#endif<BR>#ifdef CONFIG_NUMA_BALANCING<BR>&nbsp;int numa_scan_seq;<BR>&nbsp;unsigned int numa_scan_period;<BR>&nbsp;unsigned int numa_scan_period_max;<BR>&nbsp;int numa_preferred_nid;<BR>&nbsp;unsigned long numa_migrate_retry;<BR>&nbsp;u64 node_stamp;&nbsp;&nbsp;&nbsp;/* migration stamp&nbsp; */<BR>&nbsp;u64 last_task_numa_placement;<BR>&nbsp;u64 last_sum_exec_runtime;<BR>&nbsp;struct callback_head numa_work;</P>
<P>&nbsp;struct list_head numa_entry;<BR>&nbsp;struct numa_group *numa_group;</P>
<P>&nbsp;/*<BR>&nbsp; * numa_faults is an array split into four regions:<BR>&nbsp; * faults_memory, faults_cpu, faults_memory_buffer, faults_cpu_buffer<BR>&nbsp; * in this precise order.<BR>&nbsp; *<BR>&nbsp; * faults_memory: Exponential decaying average of faults on a per-node<BR>&nbsp; * basis. Scheduling placement decisions are made based on these<BR>&nbsp; * counts. The values remain static for the duration of a PTE scan.<BR>&nbsp; * faults_cpu: Track the nodes the process was running on when a NUMA<BR>&nbsp; * hinting fault was incurred.<BR>&nbsp; * faults_memory_buffer and faults_cpu_buffer: Record faults per node<BR>&nbsp; * during the current scan window. When the scan completes, the counts<BR>&nbsp; * in faults_memory and faults_cpu decay and these values are copied.<BR>&nbsp; */<BR>&nbsp;unsigned long *numa_faults;<BR>&nbsp;unsigned long total_numa_faults;</P>
<P>&nbsp;/*<BR>&nbsp; * numa_faults_locality tracks if faults recorded during the last<BR>&nbsp; * scan window were remote/local or failed to migrate. The task scan<BR>&nbsp; * period is adapted based on the locality of the faults with different<BR>&nbsp; * weights depending on whether they were shared or private faults<BR>&nbsp; */<BR>&nbsp;unsigned long numa_faults_locality[3];</P>
<P>&nbsp;unsigned long numa_pages_migrated;<BR>#endif /* CONFIG_NUMA_BALANCING */</P>
<P>#ifdef CONFIG_ARCH_WANT_BATCHED_UNMAP_TLB_FLUSH<BR>&nbsp;struct tlbflush_unmap_batch tlb_ubc;<BR>#endif</P>
<P>&nbsp;struct rcu_head rcu;</P>
<P>&nbsp;/*<BR>&nbsp; * cache last used pipe for splice<BR>&nbsp; */<BR>&nbsp;struct pipe_inode_info *splice_pipe;</P>
<P>&nbsp;struct page_frag task_frag;</P>
<P>#ifdef&nbsp;CONFIG_TASK_DELAY_ACCT<BR>&nbsp;struct task_delay_info *delays;<BR>#endif<BR>#ifdef CONFIG_FAULT_INJECTION<BR>&nbsp;int make_it_fail;<BR>#endif<BR>&nbsp;/*<BR>&nbsp; * when (nr_dirtied &gt;= nr_dirtied_pause), it's time to call<BR>&nbsp; * balance_dirty_pages() for some dirty throttling pause<BR>&nbsp; */<BR>&nbsp;int nr_dirtied;<BR>&nbsp;int nr_dirtied_pause;<BR>&nbsp;unsigned long dirty_paused_when; /* start of a write-and-pause period */</P>
<P>#ifdef CONFIG_LATENCYTOP<BR>&nbsp;int latency_record_count;<BR>&nbsp;struct latency_record latency_record[LT_SAVECOUNT];<BR>#endif<BR>&nbsp;/*<BR>&nbsp; * time slack values; these are used to round up poll() and<BR>&nbsp; * select() etc timeout values. These are in nanoseconds.<BR>&nbsp; */<BR>&nbsp;u64 timer_slack_ns;<BR>&nbsp;u64 default_timer_slack_ns;</P>
<P>#ifdef CONFIG_KASAN<BR>&nbsp;unsigned int kasan_depth;<BR>#endif<BR>#ifdef CONFIG_FUNCTION_GRAPH_TRACER<BR>&nbsp;/* Index of current stored address in ret_stack */<BR>&nbsp;int curr_ret_stack;<BR>&nbsp;/* Stack of return addresses for return function tracing */<BR>&nbsp;struct ftrace_ret_stack&nbsp;*ret_stack;<BR>&nbsp;/* time stamp for last schedule */<BR>&nbsp;unsigned long long ftrace_timestamp;<BR>&nbsp;/*<BR>&nbsp; * Number of functions that haven't been traced<BR>&nbsp; * because of depth overrun.<BR>&nbsp; */<BR>&nbsp;atomic_t trace_overrun;<BR>&nbsp;/* Pause for the tracing */<BR>&nbsp;atomic_t tracing_graph_pause;<BR>#endif<BR>#ifdef CONFIG_TRACING<BR>&nbsp;/* state flags for use by tracers */<BR>&nbsp;unsigned long trace;<BR>&nbsp;/* bitmask and counter of trace recursion */<BR>&nbsp;unsigned long trace_recursion;<BR>#endif /* CONFIG_TRACING */<BR>#ifdef CONFIG_KCOV<BR>&nbsp;/* Coverage collection mode enabled for this task (0 if disabled). */<BR>&nbsp;enum kcov_mode kcov_mode;<BR>&nbsp;/* Size of the kcov_area. */<BR>&nbsp;unsigned&nbsp;kcov_size;<BR>&nbsp;/* Buffer for coverage collection. */<BR>&nbsp;void&nbsp;&nbsp;*kcov_area;<BR>&nbsp;/* kcov desciptor wired with this task or NULL. */<BR>&nbsp;struct kcov&nbsp;*kcov;<BR>#endif<BR>#ifdef CONFIG_MEMCG<BR>&nbsp;struct mem_cgroup *memcg_in_oom;<BR>&nbsp;gfp_t memcg_oom_gfp_mask;<BR>&nbsp;int memcg_oom_order;</P>
<P>&nbsp;/* number of pages to reclaim on returning to userland */<BR>&nbsp;unsigned int memcg_nr_pages_over_high;<BR>#endif<BR>#ifdef CONFIG_UPROBES<BR>&nbsp;struct uprobe_task *utask;<BR>#endif<BR>#if defined(CONFIG_BCACHE) || defined(CONFIG_BCACHE_MODULE)<BR>&nbsp;unsigned int&nbsp;sequential_io;<BR>&nbsp;unsigned int&nbsp;sequential_io_avg;<BR>#endif<BR>#ifdef CONFIG_DEBUG_ATOMIC_SLEEP<BR>&nbsp;unsigned long&nbsp;task_state_change;<BR>#endif<BR>&nbsp;int pagefault_disabled;<BR>#ifdef CONFIG_MMU<BR>&nbsp;struct task_struct *oom_reaper_list;<BR>#endif<BR>/* CPU-specific state of this task */<BR>&nbsp;struct thread_struct thread;<BR>/*<BR>&nbsp;* WARNING: on x86, 'thread_struct' contains a variable-sized<BR>&nbsp;* structure.&nbsp; It *MUST* be at the end of 'task_struct'.<BR>&nbsp;*<BR>&nbsp;* Do not put anything below here!<BR>&nbsp;*/<BR>};</P>
<P>#ifdef CONFIG_ARCH_WANTS_DYNAMIC_TASK_STRUCT<BR>extern int arch_task_struct_size __read_mostly;<BR>#else<BR># define arch_task_struct_size (sizeof(struct task_struct))<BR>#endif</P>
<P>/* Future-safe accessor for struct task_struct's cpus_allowed. */<BR>#define tsk_cpus_allowed(tsk) (&amp;(tsk)-&gt;cpus_allowed)</P>
<P>static inline int tsk_nr_cpus_allowed(struct task_struct *p)<BR>{<BR>&nbsp;return p-&gt;nr_cpus_allowed;<BR>}</P>
<P>#define TNF_MIGRATED&nbsp;0x01<BR>#define TNF_NO_GROUP&nbsp;0x02<BR>#define TNF_SHARED&nbsp;0x04<BR>#define TNF_FAULT_LOCAL&nbsp;0x08<BR>#define TNF_MIGRATE_FAIL 0x10</P>
<P>#ifdef CONFIG_NUMA_BALANCING<BR>extern void task_numa_fault(int last_node, int node, int pages, int flags);<BR>extern pid_t task_numa_group_id(struct task_struct *p);<BR>extern void set_numabalancing_state(bool enabled);<BR>extern void task_numa_free(struct task_struct *p);<BR>extern bool should_numa_migrate_memory(struct task_struct *p, struct page *page,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;int src_nid, int dst_cpu);<BR>#else<BR>static inline void task_numa_fault(int last_node, int node, int pages,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; int flags)<BR>{<BR>}<BR>static inline pid_t task_numa_group_id(struct task_struct *p)<BR>{<BR>&nbsp;return 0;<BR>}<BR>static inline void set_numabalancing_state(bool enabled)<BR>{<BR>}<BR>static inline void task_numa_free(struct task_struct *p)<BR>{<BR>}<BR>static inline bool should_numa_migrate_memory(struct task_struct *p,<BR>&nbsp;&nbsp;&nbsp;&nbsp;struct page *page, int src_nid, int dst_cpu)<BR>{<BR>&nbsp;return true;<BR>}<BR>#endif</P>
<P>static inline struct pid *task_pid(struct task_struct *task)<BR>{<BR>&nbsp;return task-&gt;pids[PIDTYPE_PID].pid;<BR>}</P>
<P>static inline struct pid *task_tgid(struct task_struct *task)<BR>{<BR>&nbsp;return task-&gt;group_leader-&gt;pids[PIDTYPE_PID].pid;<BR>}</P>
<P>/*<BR>&nbsp;* Without tasklist or rcu lock it is not safe to dereference<BR>&nbsp;* the result of task_pgrp/task_session even if task == current,<BR>&nbsp;* we can race with another thread doing sys_setsid/sys_setpgid.<BR>&nbsp;*/<BR>static inline struct pid *task_pgrp(struct task_struct *task)<BR>{<BR>&nbsp;return task-&gt;group_leader-&gt;pids[PIDTYPE_PGID].pid;<BR>}</P>
<P>static inline struct pid *task_session(struct task_struct *task)<BR>{<BR>&nbsp;return task-&gt;group_leader-&gt;pids[PIDTYPE_SID].pid;<BR>}</P>
<P>struct pid_namespace;</P>
<P>/*<BR>&nbsp;* the helpers to get the task's different pids as they are seen<BR>&nbsp;* from various namespaces<BR>&nbsp;*<BR>&nbsp;* task_xid_nr()&nbsp;&nbsp;&nbsp;&nbsp; : global id, i.e. the id seen from the init namespace;<BR>&nbsp;* task_xid_vnr()&nbsp;&nbsp;&nbsp; : virtual id, i.e. the id seen from the pid namespace of<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; current.<BR>&nbsp;* task_xid_nr_ns()&nbsp; : id seen from the ns specified;<BR>&nbsp;*<BR>&nbsp;* set_task_vxid()&nbsp;&nbsp; : assigns a virtual id to a task;<BR>&nbsp;*<BR>&nbsp;* see also pid_nr() etc in include/linux/pid.h<BR>&nbsp;*/<BR>pid_t __task_pid_nr_ns(struct task_struct *task, enum pid_type type,<BR>&nbsp;&nbsp;&nbsp;struct pid_namespace *ns);</P>
<P>static inline pid_t task_pid_nr(struct task_struct *tsk)<BR>{<BR>&nbsp;return tsk-&gt;pid;<BR>}</P>
<P>static inline pid_t task_pid_nr_ns(struct task_struct *tsk,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;struct pid_namespace *ns)<BR>{<BR>&nbsp;return __task_pid_nr_ns(tsk, PIDTYPE_PID, ns);<BR>}</P>
<P>static inline pid_t task_pid_vnr(struct task_struct *tsk)<BR>{<BR>&nbsp;return __task_pid_nr_ns(tsk, PIDTYPE_PID, NULL);<BR>}</P>
<P><BR>static inline pid_t task_tgid_nr(struct task_struct *tsk)<BR>{<BR>&nbsp;return tsk-&gt;tgid;<BR>}</P>
<P>pid_t task_tgid_nr_ns(struct task_struct *tsk, struct pid_namespace *ns);</P>
<P>static inline pid_t task_tgid_vnr(struct task_struct *tsk)<BR>{<BR>&nbsp;return pid_vnr(task_tgid(tsk));<BR>}</P>
<P><BR>static inline int pid_alive(const struct task_struct *p);<BR>static inline pid_t task_ppid_nr_ns(const struct task_struct *tsk, struct pid_namespace *ns)<BR>{<BR>&nbsp;pid_t pid = 0;</P>
<P>&nbsp;rcu_read_lock();<BR>&nbsp;if (pid_alive(tsk))<BR>&nbsp;&nbsp;pid = task_tgid_nr_ns(rcu_dereference(tsk-&gt;real_parent), ns);<BR>&nbsp;rcu_read_unlock();</P>
<P>&nbsp;return pid;<BR>}</P>
<P>static inline pid_t task_ppid_nr(const struct task_struct *tsk)<BR>{<BR>&nbsp;return task_ppid_nr_ns(tsk, &amp;init_pid_ns);<BR>}</P>
<P>static inline pid_t task_pgrp_nr_ns(struct task_struct *tsk,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;struct pid_namespace *ns)<BR>{<BR>&nbsp;return __task_pid_nr_ns(tsk, PIDTYPE_PGID, ns);<BR>}</P>
<P>static inline pid_t task_pgrp_vnr(struct task_struct *tsk)<BR>{<BR>&nbsp;return __task_pid_nr_ns(tsk, PIDTYPE_PGID, NULL);<BR>}</P>
<P><BR>static inline pid_t task_session_nr_ns(struct task_struct *tsk,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;struct pid_namespace *ns)<BR>{<BR>&nbsp;return __task_pid_nr_ns(tsk, PIDTYPE_SID, ns);<BR>}</P>
<P>static inline pid_t task_session_vnr(struct task_struct *tsk)<BR>{<BR>&nbsp;return __task_pid_nr_ns(tsk, PIDTYPE_SID, NULL);<BR>}</P>
<P>/* obsolete, do not use */<BR>static inline pid_t task_pgrp_nr(struct task_struct *tsk)<BR>{<BR>&nbsp;return task_pgrp_nr_ns(tsk, &amp;init_pid_ns);<BR>}</P>
<P>/**<BR>&nbsp;* pid_alive - check that a task structure is not stale<BR>&nbsp;* @p: Task structure to be checked.<BR>&nbsp;*<BR>&nbsp;* Test if a process is not yet dead (at most zombie state)<BR>&nbsp;* If pid_alive fails, then pointers within the task structure<BR>&nbsp;* can be stale and must not be dereferenced.<BR>&nbsp;*<BR>&nbsp;* Return: 1 if the process is alive. 0 otherwise.<BR>&nbsp;*/<BR>static inline int pid_alive(const struct task_struct *p)<BR>{<BR>&nbsp;return p-&gt;pids[PIDTYPE_PID].pid != NULL;<BR>}</P>
<P>/**<BR>&nbsp;* is_global_init - check if a task structure is init. Since init<BR>&nbsp;* is free to have sub-threads we need to check tgid.<BR>&nbsp;* @tsk: Task structure to be checked.<BR>&nbsp;*<BR>&nbsp;* Check if a task structure is the first user space task the kernel created.<BR>&nbsp;*<BR>&nbsp;* Return: 1 if the task structure is init. 0 otherwise.<BR>&nbsp;*/<BR>static inline int is_global_init(struct task_struct *tsk)<BR>{<BR>&nbsp;return task_tgid_nr(tsk) == 1;<BR>}</P>
<P>extern struct pid *cad_pid;</P>
<P>extern void free_task(struct task_struct *tsk);<BR>#define get_task_struct(tsk) do { atomic_inc(&amp;(tsk)-&gt;usage); } while(0)</P>
<P>extern void __put_task_struct(struct task_struct *t);</P>
<P>static inline void put_task_struct(struct task_struct *t)<BR>{<BR>&nbsp;if (atomic_dec_and_test(&amp;t-&gt;usage))<BR>&nbsp;&nbsp;__put_task_struct(t);<BR>}</P>
<P>#ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN<BR>extern void task_cputime(struct task_struct *t,<BR>&nbsp;&nbsp;&nbsp; cputime_t *utime, cputime_t *stime);<BR>extern void task_cputime_scaled(struct task_struct *t,<BR>&nbsp;&nbsp;&nbsp;&nbsp;cputime_t *utimescaled, cputime_t *stimescaled);<BR>extern cputime_t task_gtime(struct task_struct *t);<BR>#else<BR>static inline void task_cputime(struct task_struct *t,<BR>&nbsp;&nbsp;&nbsp;&nbsp;cputime_t *utime, cputime_t *stime)<BR>{<BR>&nbsp;if (utime)<BR>&nbsp;&nbsp;*utime = t-&gt;utime;<BR>&nbsp;if (stime)<BR>&nbsp;&nbsp;*stime = t-&gt;stime;<BR>}</P>
<P>static inline void task_cputime_scaled(struct task_struct *t,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; cputime_t *utimescaled,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; cputime_t *stimescaled)<BR>{<BR>&nbsp;if (utimescaled)<BR>&nbsp;&nbsp;*utimescaled = t-&gt;utimescaled;<BR>&nbsp;if (stimescaled)<BR>&nbsp;&nbsp;*stimescaled = t-&gt;stimescaled;<BR>}</P>
<P>static inline cputime_t task_gtime(struct task_struct *t)<BR>{<BR>&nbsp;return t-&gt;gtime;<BR>}<BR>#endif<BR>extern void task_cputime_adjusted(struct task_struct *p, cputime_t *ut, cputime_t *st);<BR>extern void thread_group_cputime_adjusted(struct task_struct *p, cputime_t *ut, cputime_t *st);</P>
<P>/*<BR>&nbsp;* Per process flags<BR>&nbsp;*/<BR>#define PF_EXITING&nbsp;0x00000004&nbsp;/* getting shut down */<BR>#define PF_EXITPIDONE&nbsp;0x00000008&nbsp;/* pi exit done on shut down */<BR>#define PF_VCPU&nbsp;&nbsp;0x00000010&nbsp;/* I'm a virtual CPU */<BR>#define PF_WQ_WORKER&nbsp;0x00000020&nbsp;/* I'm a workqueue worker */<BR>#define PF_FORKNOEXEC&nbsp;0x00000040&nbsp;/* forked but didn't exec */<BR>#define PF_MCE_PROCESS&nbsp; 0x00000080&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; /* process policy on mce errors */<BR>#define PF_SUPERPRIV&nbsp;0x00000100&nbsp;/* used super-user privileges */<BR>#define PF_DUMPCORE&nbsp;0x00000200&nbsp;/* dumped core */<BR>#define PF_SIGNALED&nbsp;0x00000400&nbsp;/* killed by a signal */<BR>#define PF_MEMALLOC&nbsp;0x00000800&nbsp;/* Allocating memory */<BR>#define PF_NPROC_EXCEEDED 0x00001000&nbsp;/* set_user noticed that RLIMIT_NPROC was exceeded */<BR>#define PF_USED_MATH&nbsp;0x00002000&nbsp;/* if unset the fpu must be initialized before use */<BR>#define PF_USED_ASYNC&nbsp;0x00004000&nbsp;/* used async_schedule*(), used by module init */<BR>#define PF_NOFREEZE&nbsp;0x00008000&nbsp;/* this thread should not be frozen */<BR>#define PF_FROZEN&nbsp;0x00010000&nbsp;/* frozen for system suspend */<BR>#define PF_FSTRANS&nbsp;0x00020000&nbsp;/* inside a filesystem transaction */<BR>#define PF_KSWAPD&nbsp;0x00040000&nbsp;/* I am kswapd */<BR>#define PF_MEMALLOC_NOIO 0x00080000&nbsp;/* Allocating memory without IO involved */<BR>#define PF_LESS_THROTTLE 0x00100000&nbsp;/* Throttle me less: I clean memory */<BR>#define PF_KTHREAD&nbsp;0x00200000&nbsp;/* I am a kernel thread */<BR>#define PF_RANDOMIZE&nbsp;0x00400000&nbsp;/* randomize virtual address space */<BR>#define PF_SWAPWRITE&nbsp;0x00800000&nbsp;/* Allowed to write to swap */<BR>#define PF_NO_SETAFFINITY 0x04000000&nbsp;/* Userland is not allowed to meddle with cpus_allowed */<BR>#define PF_MCE_EARLY&nbsp;&nbsp;&nbsp; 0x08000000&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; /* Early kill for mce process policy */<BR>#define PF_MUTEX_TESTER&nbsp;0x20000000&nbsp;/* Thread belongs to the rt mutex tester */<BR>#define PF_FREEZER_SKIP&nbsp;0x40000000&nbsp;/* Freezer should not count it as freezable */<BR>#define PF_SUSPEND_TASK 0x80000000&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; /* this thread called freeze_processes and should not be frozen */</P>
<P>/*<BR>&nbsp;* Only the _current_ task can read/write to tsk-&gt;flags, but other<BR>&nbsp;* tasks can access tsk-&gt;flags in readonly mode for example<BR>&nbsp;* with tsk_used_math (like during threaded core dumping).<BR>&nbsp;* There is however an exception to this rule during ptrace<BR>&nbsp;* or during fork: the ptracer task is allowed to write to the<BR>&nbsp;* child-&gt;flags of its traced child (same goes for fork, the parent<BR>&nbsp;* can write to the child-&gt;flags), because we're guaranteed the<BR>&nbsp;* child is not running and in turn not changing child-&gt;flags<BR>&nbsp;* at the same time the parent does it.<BR>&nbsp;*/<BR>#define clear_stopped_child_used_math(child) do { (child)-&gt;flags &amp;= ~PF_USED_MATH; } while (0)<BR>#define set_stopped_child_used_math(child) do { (child)-&gt;flags |= PF_USED_MATH; } while (0)<BR>#define clear_used_math() clear_stopped_child_used_math(current)<BR>#define set_used_math() set_stopped_child_used_math(current)<BR>#define conditional_stopped_child_used_math(condition, child) \<BR>&nbsp;do { (child)-&gt;flags &amp;= ~PF_USED_MATH, (child)-&gt;flags |= (condition) ? PF_USED_MATH : 0; } while (0)<BR>#define conditional_used_math(condition) \<BR>&nbsp;conditional_stopped_child_used_math(condition, current)<BR>#define copy_to_stopped_child_used_math(child) \<BR>&nbsp;do { (child)-&gt;flags &amp;= ~PF_USED_MATH, (child)-&gt;flags |= current-&gt;flags &amp; PF_USED_MATH; } while (0)<BR>/* NOTE: this will return 0 or PF_USED_MATH, it will never return 1 */<BR>#define tsk_used_math(p) ((p)-&gt;flags &amp; PF_USED_MATH)<BR>#define used_math() tsk_used_math(current)</P>
<P>/* __GFP_IO isn't allowed if PF_MEMALLOC_NOIO is set in current-&gt;flags<BR>&nbsp;* __GFP_FS is also cleared as it implies __GFP_IO.<BR>&nbsp;*/<BR>static inline gfp_t memalloc_noio_flags(gfp_t flags)<BR>{<BR>&nbsp;if (unlikely(current-&gt;flags &amp; PF_MEMALLOC_NOIO))<BR>&nbsp;&nbsp;flags &amp;= ~(__GFP_IO | __GFP_FS);<BR>&nbsp;return flags;<BR>}</P>
<P>static inline unsigned int memalloc_noio_save(void)<BR>{<BR>&nbsp;unsigned int flags = current-&gt;flags &amp; PF_MEMALLOC_NOIO;<BR>&nbsp;current-&gt;flags |= PF_MEMALLOC_NOIO;<BR>&nbsp;return flags;<BR>}</P>
<P>static inline void memalloc_noio_restore(unsigned int flags)<BR>{<BR>&nbsp;current-&gt;flags = (current-&gt;flags &amp; ~PF_MEMALLOC_NOIO) | flags;<BR>}</P>
<P>/* Per-process atomic flags. */<BR>#define PFA_NO_NEW_PRIVS 0&nbsp;/* May not gain new privileges. */<BR>#define PFA_SPREAD_PAGE&nbsp; 1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; /* Spread page cache over cpuset */<BR>#define PFA_SPREAD_SLAB&nbsp; 2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; /* Spread some slab caches over cpuset */<BR>#define PFA_LMK_WAITING&nbsp; 3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; /* Lowmemorykiller is waiting */</P>
<P><BR>#define TASK_PFA_TEST(name, func)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;static inline bool task_##func(struct task_struct *p)&nbsp;&nbsp;\<BR>&nbsp;{ return test_bit(PFA_##name, &amp;p-&gt;atomic_flags); }<BR>#define TASK_PFA_SET(name, func)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;static inline void task_set_##func(struct task_struct *p)&nbsp;\<BR>&nbsp;{ set_bit(PFA_##name, &amp;p-&gt;atomic_flags); }<BR>#define TASK_PFA_CLEAR(name, func)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;static inline void task_clear_##func(struct task_struct *p)&nbsp;\<BR>&nbsp;{ clear_bit(PFA_##name, &amp;p-&gt;atomic_flags); }</P>
<P>TASK_PFA_TEST(NO_NEW_PRIVS, no_new_privs)<BR>TASK_PFA_SET(NO_NEW_PRIVS, no_new_privs)</P>
<P>TASK_PFA_TEST(SPREAD_PAGE, spread_page)<BR>TASK_PFA_SET(SPREAD_PAGE, spread_page)<BR>TASK_PFA_CLEAR(SPREAD_PAGE, spread_page)</P>
<P>TASK_PFA_TEST(SPREAD_SLAB, spread_slab)<BR>TASK_PFA_SET(SPREAD_SLAB, spread_slab)<BR>TASK_PFA_CLEAR(SPREAD_SLAB, spread_slab)</P>
<P>TASK_PFA_TEST(LMK_WAITING, lmk_waiting)<BR>TASK_PFA_SET(LMK_WAITING, lmk_waiting)</P>
<P>/*<BR>&nbsp;* task-&gt;jobctl flags<BR>&nbsp;*/<BR>#define JOBCTL_STOP_SIGMASK&nbsp;0xffff&nbsp;/* signr of the last group stop */</P>
<P>#define JOBCTL_STOP_DEQUEUED_BIT 16&nbsp;/* stop signal dequeued */<BR>#define JOBCTL_STOP_PENDING_BIT&nbsp;17&nbsp;/* task should stop for group stop */<BR>#define JOBCTL_STOP_CONSUME_BIT&nbsp;18&nbsp;/* consume group stop count */<BR>#define JOBCTL_TRAP_STOP_BIT&nbsp;19&nbsp;/* trap for STOP */<BR>#define JOBCTL_TRAP_NOTIFY_BIT&nbsp;20&nbsp;/* trap for NOTIFY */<BR>#define JOBCTL_TRAPPING_BIT&nbsp;21&nbsp;/* switching to TRACED */<BR>#define JOBCTL_LISTENING_BIT&nbsp;22&nbsp;/* ptracer is listening for events */</P>
<P>#define JOBCTL_STOP_DEQUEUED&nbsp;(1UL &lt;&lt; JOBCTL_STOP_DEQUEUED_BIT)<BR>#define JOBCTL_STOP_PENDING&nbsp;(1UL &lt;&lt; JOBCTL_STOP_PENDING_BIT)<BR>#define JOBCTL_STOP_CONSUME&nbsp;(1UL &lt;&lt; JOBCTL_STOP_CONSUME_BIT)<BR>#define JOBCTL_TRAP_STOP&nbsp;(1UL &lt;&lt; JOBCTL_TRAP_STOP_BIT)<BR>#define JOBCTL_TRAP_NOTIFY&nbsp;(1UL &lt;&lt; JOBCTL_TRAP_NOTIFY_BIT)<BR>#define JOBCTL_TRAPPING&nbsp;&nbsp;(1UL &lt;&lt; JOBCTL_TRAPPING_BIT)<BR>#define JOBCTL_LISTENING&nbsp;(1UL &lt;&lt; JOBCTL_LISTENING_BIT)</P>
<P>#define JOBCTL_TRAP_MASK&nbsp;(JOBCTL_TRAP_STOP | JOBCTL_TRAP_NOTIFY)<BR>#define JOBCTL_PENDING_MASK&nbsp;(JOBCTL_STOP_PENDING | JOBCTL_TRAP_MASK)</P>
<P>extern bool task_set_jobctl_pending(struct task_struct *task,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; unsigned long mask);<BR>extern void task_clear_jobctl_trapping(struct task_struct *task);<BR>extern void task_clear_jobctl_pending(struct task_struct *task,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; unsigned long mask);</P>
<P>static inline void rcu_copy_process(struct task_struct *p)<BR>{<BR>#ifdef CONFIG_PREEMPT_RCU<BR>&nbsp;p-&gt;rcu_read_lock_nesting = 0;<BR>&nbsp;p-&gt;rcu_read_unlock_special.s = 0;<BR>&nbsp;p-&gt;rcu_blocked_node = NULL;<BR>&nbsp;INIT_LIST_HEAD(&amp;p-&gt;rcu_node_entry);<BR>#endif /* #ifdef CONFIG_PREEMPT_RCU */<BR>#ifdef CONFIG_TASKS_RCU<BR>&nbsp;p-&gt;rcu_tasks_holdout = false;<BR>&nbsp;INIT_LIST_HEAD(&amp;p-&gt;rcu_tasks_holdout_list);<BR>&nbsp;p-&gt;rcu_tasks_idle_cpu = -1;<BR>#endif /* #ifdef CONFIG_TASKS_RCU */<BR>}</P>
<P>static inline void tsk_restore_flags(struct task_struct *task,<BR>&nbsp;&nbsp;&nbsp;&nbsp;unsigned long orig_flags, unsigned long flags)<BR>{<BR>&nbsp;task-&gt;flags &amp;= ~flags;<BR>&nbsp;task-&gt;flags |= orig_flags &amp; flags;<BR>}</P>
<P>extern int cpuset_cpumask_can_shrink(const struct cpumask *cur,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; const struct cpumask *trial);<BR>extern int task_can_attach(struct task_struct *p,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; const struct cpumask *cs_cpus_allowed);<BR>#ifdef CONFIG_SMP<BR>extern void do_set_cpus_allowed(struct task_struct *p,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; const struct cpumask *new_mask);</P>
<P>extern int set_cpus_allowed_ptr(struct task_struct *p,<BR>&nbsp;&nbsp;&nbsp;&nbsp;const struct cpumask *new_mask);<BR>#else<BR>static inline void do_set_cpus_allowed(struct task_struct *p,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; const struct cpumask *new_mask)<BR>{<BR>}<BR>static inline int set_cpus_allowed_ptr(struct task_struct *p,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; const struct cpumask *new_mask)<BR>{<BR>&nbsp;if (!cpumask_test_cpu(0, new_mask))<BR>&nbsp;&nbsp;return -EINVAL;<BR>&nbsp;return 0;<BR>}<BR>#endif</P>
<P>#ifdef CONFIG_NO_HZ_COMMON<BR>void calc_load_enter_idle(void);<BR>void calc_load_exit_idle(void);<BR>#else<BR>static inline void calc_load_enter_idle(void) { }<BR>static inline void calc_load_exit_idle(void) { }<BR>#endif /* CONFIG_NO_HZ_COMMON */</P>
<P>/*<BR>&nbsp;* Do not use outside of architecture code which knows its limitations.<BR>&nbsp;*<BR>&nbsp;* sched_clock() has no promise of monotonicity or bounded drift between<BR>&nbsp;* CPUs, use (which you should not) requires disabling IRQs.<BR>&nbsp;*<BR>&nbsp;* Please use one of the three interfaces below.<BR>&nbsp;*/<BR>extern unsigned long long notrace sched_clock(void);<BR>/*<BR>&nbsp;* See the comment in kernel/sched/clock.c<BR>&nbsp;*/<BR>extern u64 running_clock(void);<BR>extern u64 sched_clock_cpu(int cpu);</P>
<P><BR>extern void sched_clock_init(void);</P>
<P>#ifndef CONFIG_HAVE_UNSTABLE_SCHED_CLOCK<BR>static inline void sched_clock_tick(void)<BR>{<BR>}</P>
<P>static inline void sched_clock_idle_sleep_event(void)<BR>{<BR>}</P>
<P>static inline void sched_clock_idle_wakeup_event(u64 delta_ns)<BR>{<BR>}</P>
<P>static inline u64 cpu_clock(int cpu)<BR>{<BR>&nbsp;return sched_clock();<BR>}</P>
<P>static inline u64 local_clock(void)<BR>{<BR>&nbsp;return sched_clock();<BR>}<BR>#else<BR>/*<BR>&nbsp;* Architectures can set this to 1 if they have specified<BR>&nbsp;* CONFIG_HAVE_UNSTABLE_SCHED_CLOCK in their arch Kconfig,<BR>&nbsp;* but then during bootup it turns out that sched_clock()<BR>&nbsp;* is reliable after all:<BR>&nbsp;*/<BR>extern int sched_clock_stable(void);<BR>extern void set_sched_clock_stable(void);<BR>extern void clear_sched_clock_stable(void);</P>
<P>extern void sched_clock_tick(void);<BR>extern void sched_clock_idle_sleep_event(void);<BR>extern void sched_clock_idle_wakeup_event(u64 delta_ns);</P>
<P>/*<BR>&nbsp;* As outlined in clock.c, provides a fast, high resolution, nanosecond<BR>&nbsp;* time source that is monotonic per cpu argument and has bounded drift<BR>&nbsp;* between cpus.<BR>&nbsp;*<BR>&nbsp;* ######################### BIG FAT WARNING ##########################<BR>&nbsp;* # when comparing cpu_clock(i) to cpu_clock(j) for i != j, time can #<BR>&nbsp;* # go backwards !!&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; #<BR>&nbsp;* ####################################################################<BR>&nbsp;*/<BR>static inline u64 cpu_clock(int cpu)<BR>{<BR>&nbsp;return sched_clock_cpu(cpu);<BR>}</P>
<P>static inline u64 local_clock(void)<BR>{<BR>&nbsp;return sched_clock_cpu(raw_smp_processor_id());<BR>}<BR>#endif</P>
<P>#ifdef CONFIG_IRQ_TIME_ACCOUNTING<BR>/*<BR>&nbsp;* An i/f to runtime opt-in for irq time accounting based off of sched_clock.<BR>&nbsp;* The reason for this explicit opt-in is not to have perf penalty with<BR>&nbsp;* slow sched_clocks.<BR>&nbsp;*/<BR>extern void enable_sched_clock_irqtime(void);<BR>extern void disable_sched_clock_irqtime(void);<BR>#else<BR>static inline void enable_sched_clock_irqtime(void) {}<BR>static inline void disable_sched_clock_irqtime(void) {}<BR>#endif</P>
<P>extern unsigned long long<BR>task_sched_runtime(struct task_struct *task);</P>
<P>/* sched_exec is called by processes performing an exec */<BR>#ifdef CONFIG_SMP<BR>extern void sched_exec(void);<BR>#else<BR>#define sched_exec()&nbsp;&nbsp; {}<BR>#endif</P>
<P>extern void sched_clock_idle_sleep_event(void);<BR>extern void sched_clock_idle_wakeup_event(u64 delta_ns);</P>
<P>#ifdef CONFIG_HOTPLUG_CPU<BR>extern void idle_task_exit(void);<BR>#else<BR>static inline void idle_task_exit(void) {}<BR>#endif</P>
<P>#if defined(CONFIG_NO_HZ_COMMON) &amp;&amp; defined(CONFIG_SMP)<BR>extern void wake_up_nohz_cpu(int cpu);<BR>#else<BR>static inline void wake_up_nohz_cpu(int cpu) { }<BR>#endif</P>
<P>#ifdef CONFIG_NO_HZ_FULL<BR>extern u64 scheduler_tick_max_deferment(void);<BR>#endif</P>
<P>#ifdef CONFIG_SCHED_AUTOGROUP<BR>extern void sched_autogroup_create_attach(struct task_struct *p);<BR>extern void sched_autogroup_detach(struct task_struct *p);<BR>extern void sched_autogroup_fork(struct signal_struct *sig);<BR>extern void sched_autogroup_exit(struct signal_struct *sig);<BR>#ifdef CONFIG_PROC_FS<BR>extern void proc_sched_autogroup_show_task(struct task_struct *p, struct seq_file *m);<BR>extern int proc_sched_autogroup_set_nice(struct task_struct *p, int nice);<BR>#endif<BR>#else<BR>static inline void sched_autogroup_create_attach(struct task_struct *p) { }<BR>static inline void sched_autogroup_detach(struct task_struct *p) { }<BR>static inline void sched_autogroup_fork(struct signal_struct *sig) { }<BR>static inline void sched_autogroup_exit(struct signal_struct *sig) { }<BR>#endif</P>
<P>extern int yield_to(struct task_struct *p, bool preempt);<BR>extern void set_user_nice(struct task_struct *p, long nice);<BR>extern int task_prio(const struct task_struct *p);<BR>/**<BR>&nbsp;* task_nice - return the nice value of a given task.<BR>&nbsp;* @p: the task in question.<BR>&nbsp;*<BR>&nbsp;* Return: The nice value [ -20 ... 0 ... 19 ].<BR>&nbsp;*/<BR>static inline int task_nice(const struct task_struct *p)<BR>{<BR>&nbsp;return PRIO_TO_NICE((p)-&gt;static_prio);<BR>}<BR>extern int can_nice(const struct task_struct *p, const int nice);<BR>extern int task_curr(const struct task_struct *p);<BR>extern int idle_cpu(int cpu);<BR>extern int sched_setscheduler(struct task_struct *, int,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; const struct sched_param *);<BR>extern int sched_setscheduler_nocheck(struct task_struct *, int,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; const struct sched_param *);<BR>extern int sched_setattr(struct task_struct *,<BR>&nbsp;&nbsp;&nbsp; const struct sched_attr *);<BR>extern struct task_struct *idle_task(int cpu);<BR>/**<BR>&nbsp;* is_idle_task - is the specified task an idle task?<BR>&nbsp;* @p: the task in question.<BR>&nbsp;*<BR>&nbsp;* Return: 1 if @p is an idle task. 0 otherwise.<BR>&nbsp;*/<BR>static inline bool is_idle_task(const struct task_struct *p)<BR>{<BR>&nbsp;return p-&gt;pid == 0;<BR>}<BR>extern struct task_struct *curr_task(int cpu);<BR>extern void set_curr_task(int cpu, struct task_struct *p);</P>
<P>void yield(void);</P>
<P>union thread_union {<BR>&nbsp;struct thread_info thread_info;<BR>&nbsp;unsigned long stack[THREAD_SIZE/sizeof(long)];<BR>};</P>
<P>#ifndef __HAVE_ARCH_KSTACK_END<BR>static inline int kstack_end(void *addr)<BR>{<BR>&nbsp;/* Reliable end of stack detection:<BR>&nbsp; * Some APM bios versions misalign the stack<BR>&nbsp; */<BR>&nbsp;return !(((unsigned long)addr+sizeof(void*)-1) &amp; (THREAD_SIZE-sizeof(void*)));<BR>}<BR>#endif</P>
<P>extern union thread_union init_thread_union;<BR>extern struct task_struct init_task;</P>
<P>extern struct&nbsp;&nbsp; mm_struct init_mm;</P>
<P>extern struct pid_namespace init_pid_ns;</P>
<P>/*<BR>&nbsp;* find a task by one of its numerical ids<BR>&nbsp;*<BR>&nbsp;* find_task_by_pid_ns():<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; finds a task by its pid in the specified namespace<BR>&nbsp;* find_task_by_vpid():<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; finds a task by its virtual pid<BR>&nbsp;*<BR>&nbsp;* see also find_vpid() etc in include/linux/pid.h<BR>&nbsp;*/</P>
<P>extern struct task_struct *find_task_by_vpid(pid_t nr);<BR>extern struct task_struct *find_task_by_pid_ns(pid_t nr,<BR>&nbsp;&nbsp;struct pid_namespace *ns);</P>
<P>/* per-UID process charging. */<BR>extern struct user_struct * alloc_uid(kuid_t);<BR>static inline struct user_struct *get_uid(struct user_struct *u)<BR>{<BR>&nbsp;atomic_inc(&amp;u-&gt;__count);<BR>&nbsp;return u;<BR>}<BR>extern void free_uid(struct user_struct *);</P>
<P>#include &lt;asm/current.h&gt;</P>
<P>extern void xtime_update(unsigned long ticks);</P>
<P>extern int wake_up_state(struct task_struct *tsk, unsigned int state);<BR>extern int wake_up_process(struct task_struct *tsk);<BR>extern void wake_up_new_task(struct task_struct *tsk);<BR>#ifdef CONFIG_SMP<BR>&nbsp;extern void kick_process(struct task_struct *tsk);<BR>#else<BR>&nbsp;static inline void kick_process(struct task_struct *tsk) { }<BR>#endif<BR>extern int sched_fork(unsigned long clone_flags, struct task_struct *p);<BR>extern void sched_dead(struct task_struct *p);</P>
<P>extern void proc_caches_init(void);<BR>extern void flush_signals(struct task_struct *);<BR>extern void ignore_signals(struct task_struct *);<BR>extern void flush_signal_handlers(struct task_struct *, int force_default);<BR>extern int dequeue_signal(struct task_struct *tsk, sigset_t *mask, siginfo_t *info);</P>
<P>static inline int kernel_dequeue_signal(siginfo_t *info)<BR>{<BR>&nbsp;struct task_struct *tsk = current;<BR>&nbsp;siginfo_t __info;<BR>&nbsp;int ret;</P>
<P>&nbsp;spin_lock_irq(&amp;tsk-&gt;sighand-&gt;siglock);<BR>&nbsp;ret = dequeue_signal(tsk, &amp;tsk-&gt;blocked, info ?: &amp;__info);<BR>&nbsp;spin_unlock_irq(&amp;tsk-&gt;sighand-&gt;siglock);</P>
<P>&nbsp;return ret;<BR>}</P>
<P>static inline void kernel_signal_stop(void)<BR>{<BR>&nbsp;spin_lock_irq(&amp;current-&gt;sighand-&gt;siglock);<BR>&nbsp;if (current-&gt;jobctl &amp; JOBCTL_STOP_DEQUEUED)<BR>&nbsp;&nbsp;__set_current_state(TASK_STOPPED);<BR>&nbsp;spin_unlock_irq(&amp;current-&gt;sighand-&gt;siglock);</P>
<P>&nbsp;schedule();<BR>}</P>
<P>extern void release_task(struct task_struct * p);<BR>extern int send_sig_info(int, struct siginfo *, struct task_struct *);<BR>extern int force_sigsegv(int, struct task_struct *);<BR>extern int force_sig_info(int, struct siginfo *, struct task_struct *);<BR>extern int __kill_pgrp_info(int sig, struct siginfo *info, struct pid *pgrp);<BR>extern int kill_pid_info(int sig, struct siginfo *info, struct pid *pid);<BR>extern int kill_pid_info_as_cred(int, struct siginfo *, struct pid *,<BR>&nbsp;&nbsp;&nbsp;&nbsp;const struct cred *, u32);<BR>extern int kill_pgrp(struct pid *pid, int sig, int priv);<BR>extern int kill_pid(struct pid *pid, int sig, int priv);<BR>extern int kill_proc_info(int, struct siginfo *, pid_t);<BR>extern __must_check bool do_notify_parent(struct task_struct *, int);<BR>extern void __wake_up_parent(struct task_struct *p, struct task_struct *parent);<BR>extern void force_sig(int, struct task_struct *);<BR>extern int send_sig(int, struct task_struct *, int);<BR>extern int zap_other_threads(struct task_struct *p);<BR>extern struct sigqueue *sigqueue_alloc(void);<BR>extern void sigqueue_free(struct sigqueue *);<BR>extern int send_sigqueue(struct sigqueue *,&nbsp; struct task_struct *, int group);<BR>extern int do_sigaction(int, struct k_sigaction *, struct k_sigaction *);</P>
<P>static inline void restore_saved_sigmask(void)<BR>{<BR>&nbsp;if (test_and_clear_restore_sigmask())<BR>&nbsp;&nbsp;__set_current_blocked(&amp;current-&gt;saved_sigmask);<BR>}</P>
<P>static inline sigset_t *sigmask_to_save(void)<BR>{<BR>&nbsp;sigset_t *res = &amp;current-&gt;blocked;<BR>&nbsp;if (unlikely(test_restore_sigmask()))<BR>&nbsp;&nbsp;res = &amp;current-&gt;saved_sigmask;<BR>&nbsp;return res;<BR>}</P>
<P>static inline int kill_cad_pid(int sig, int priv)<BR>{<BR>&nbsp;return kill_pid(cad_pid, sig, priv);<BR>}</P>
<P>/* These can be the second arg to send_sig_info/send_group_sig_info.&nbsp; */<BR>#define SEND_SIG_NOINFO ((struct siginfo *) 0)<BR>#define SEND_SIG_PRIV&nbsp;((struct siginfo *) 1)<BR>#define SEND_SIG_FORCED&nbsp;((struct siginfo *) 2)</P>
<P>/*<BR>&nbsp;* True if we are on the alternate signal stack.<BR>&nbsp;*/<BR>static inline int on_sig_stack(unsigned long sp)<BR>{<BR>&nbsp;/*<BR>&nbsp; * If the signal stack is SS_AUTODISARM then, by construction, we<BR>&nbsp; * can't be on the signal stack unless user code deliberately set<BR>&nbsp; * SS_AUTODISARM when we were already on it.<BR>&nbsp; *<BR>&nbsp; * This improves reliability: if user state gets corrupted such that<BR>&nbsp; * the stack pointer points very close to the end of the signal stack,<BR>&nbsp; * then this check will enable the signal to be handled anyway.<BR>&nbsp; */<BR>&nbsp;if (current-&gt;sas_ss_flags &amp; SS_AUTODISARM)<BR>&nbsp;&nbsp;return 0;</P>
<P>#ifdef CONFIG_STACK_GROWSUP<BR>&nbsp;return sp &gt;= current-&gt;sas_ss_sp &amp;&amp;<BR>&nbsp;&nbsp;sp - current-&gt;sas_ss_sp &lt; current-&gt;sas_ss_size;<BR>#else<BR>&nbsp;return sp &gt; current-&gt;sas_ss_sp &amp;&amp;<BR>&nbsp;&nbsp;sp - current-&gt;sas_ss_sp &lt;= current-&gt;sas_ss_size;<BR>#endif<BR>}</P>
<P>static inline int sas_ss_flags(unsigned long sp)<BR>{<BR>&nbsp;if (!current-&gt;sas_ss_size)<BR>&nbsp;&nbsp;return SS_DISABLE;</P>
<P>&nbsp;return on_sig_stack(sp) ? SS_ONSTACK : 0;<BR>}</P>
<P>static inline void sas_ss_reset(struct task_struct *p)<BR>{<BR>&nbsp;p-&gt;sas_ss_sp = 0;<BR>&nbsp;p-&gt;sas_ss_size = 0;<BR>&nbsp;p-&gt;sas_ss_flags = SS_DISABLE;<BR>}</P>
<P>static inline unsigned long sigsp(unsigned long sp, struct ksignal *ksig)<BR>{<BR>&nbsp;if (unlikely((ksig-&gt;ka.sa.sa_flags &amp; SA_ONSTACK)) &amp;&amp; ! sas_ss_flags(sp))<BR>#ifdef CONFIG_STACK_GROWSUP<BR>&nbsp;&nbsp;return current-&gt;sas_ss_sp;<BR>#else<BR>&nbsp;&nbsp;return current-&gt;sas_ss_sp + current-&gt;sas_ss_size;<BR>#endif<BR>&nbsp;return sp;<BR>}</P>
<P>/*<BR>&nbsp;* Routines for handling mm_structs<BR>&nbsp;*/<BR>extern struct mm_struct * mm_alloc(void);</P>
<P>/* mmdrop drops the mm and the page tables */<BR>extern void __mmdrop(struct mm_struct *);<BR>static inline void mmdrop(struct mm_struct *mm)<BR>{<BR>&nbsp;if (unlikely(atomic_dec_and_test(&amp;mm-&gt;mm_count)))<BR>&nbsp;&nbsp;__mmdrop(mm);<BR>}</P>
<P>static inline bool mmget_not_zero(struct mm_struct *mm)<BR>{<BR>&nbsp;return atomic_inc_not_zero(&amp;mm-&gt;mm_users);<BR>}</P>
<P>/* mmput gets rid of the mappings and all user-space */<BR>extern void mmput(struct mm_struct *);<BR>/* same as above but performs the slow path from the async kontext. Can<BR>&nbsp;* be called from the atomic context as well<BR>&nbsp;*/<BR>extern void mmput_async(struct mm_struct *);</P>
<P>/* Grab a reference to a task's mm, if it is not already going away */<BR>extern struct mm_struct *get_task_mm(struct task_struct *task);<BR>/*<BR>&nbsp;* Grab a reference to a task's mm, if it is not already going away<BR>&nbsp;* and ptrace_may_access with the mode parameter passed to it<BR>&nbsp;* succeeds.<BR>&nbsp;*/<BR>extern struct mm_struct *mm_access(struct task_struct *task, unsigned int mode);<BR>/* Remove the current tasks stale references to the old mm_struct */<BR>extern void mm_release(struct task_struct *, struct mm_struct *);</P>
<P>#ifdef CONFIG_HAVE_COPY_THREAD_TLS<BR>extern int copy_thread_tls(unsigned long, unsigned long, unsigned long,<BR>&nbsp;&nbsp;&nbsp;struct task_struct *, unsigned long);<BR>#else<BR>extern int copy_thread(unsigned long, unsigned long, unsigned long,<BR>&nbsp;&nbsp;&nbsp;struct task_struct *);</P>
<P>/* Architectures that haven't opted into copy_thread_tls get the tls argument<BR>&nbsp;* via pt_regs, so ignore the tls argument passed via C. */<BR>static inline int copy_thread_tls(<BR>&nbsp;&nbsp;unsigned long clone_flags, unsigned long sp, unsigned long arg,<BR>&nbsp;&nbsp;struct task_struct *p, unsigned long tls)<BR>{<BR>&nbsp;return copy_thread(clone_flags, sp, arg, p);<BR>}<BR>#endif<BR>extern void flush_thread(void);</P>
<P>#ifdef CONFIG_HAVE_EXIT_THREAD<BR>extern void exit_thread(struct task_struct *tsk);<BR>#else<BR>static inline void exit_thread(struct task_struct *tsk)<BR>{<BR>}<BR>#endif</P>
<P>extern void exit_files(struct task_struct *);<BR>extern void __cleanup_sighand(struct sighand_struct *);</P>
<P>extern void exit_itimers(struct signal_struct *);<BR>extern void flush_itimer_signals(void);</P>
<P>extern void do_group_exit(int);</P>
<P>extern int do_execve(struct filename *,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; const char __user * const __user *,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; const char __user * const __user *);<BR>extern int do_execveat(int, struct filename *,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; const char __user * const __user *,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; const char __user * const __user *,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; int);<BR>extern long _do_fork(unsigned long, unsigned long, unsigned long, int __user *, int __user *, unsigned long);<BR>extern long do_fork(unsigned long, unsigned long, unsigned long, int __user *, int __user *);<BR>struct task_struct *fork_idle(int);<BR>extern pid_t kernel_thread(int (*fn)(void *), void *arg, unsigned long flags);</P>
<P>extern void __set_task_comm(struct task_struct *tsk, const char *from, bool exec);<BR>static inline void set_task_comm(struct task_struct *tsk, const char *from)<BR>{<BR>&nbsp;__set_task_comm(tsk, from, false);<BR>}<BR>extern char *get_task_comm(char *to, struct task_struct *tsk);</P>
<P>#ifdef CONFIG_SMP<BR>void scheduler_ipi(void);<BR>extern unsigned long wait_task_inactive(struct task_struct *, long match_state);<BR>#else<BR>static inline void scheduler_ipi(void) { }<BR>static inline unsigned long wait_task_inactive(struct task_struct *p,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; long match_state)<BR>{<BR>&nbsp;return 1;<BR>}<BR>#endif</P>
<P>#define tasklist_empty() \<BR>&nbsp;list_empty(&amp;init_task.tasks)</P>
<P>#define next_task(p) \<BR>&nbsp;list_entry_rcu((p)-&gt;tasks.next, struct task_struct, tasks)</P>
<P>#define for_each_process(p) \<BR>&nbsp;for (p = &amp;init_task ; (p = next_task(p)) != &amp;init_task ; )</P>
<P>extern bool current_is_single_threaded(void);</P>
<P>/*<BR>&nbsp;* Careful: do_each_thread/while_each_thread is a double loop so<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 'break' will not work as expected - use goto instead.<BR>&nbsp;*/<BR>#define do_each_thread(g, t) \<BR>&nbsp;for (g = t = &amp;init_task ; (g = t = next_task(g)) != &amp;init_task ; ) do</P>
<P>#define while_each_thread(g, t) \<BR>&nbsp;while ((t = next_thread(t)) != g)</P>
<P>#define __for_each_thread(signal, t)&nbsp;\<BR>&nbsp;list_for_each_entry_rcu(t, &amp;(signal)-&gt;thread_head, thread_node)</P>
<P>#define for_each_thread(p, t)&nbsp;&nbsp;\<BR>&nbsp;__for_each_thread((p)-&gt;signal, t)</P>
<P>/* Careful: this is a double loop, 'break' won't work as expected. */<BR>#define for_each_process_thread(p, t)&nbsp;\<BR>&nbsp;for_each_process(p) for_each_thread(p, t)</P>
<P>static inline int get_nr_threads(struct task_struct *tsk)<BR>{<BR>&nbsp;return tsk-&gt;signal-&gt;nr_threads;<BR>}</P>
<P>static inline bool thread_group_leader(struct task_struct *p)<BR>{<BR>&nbsp;return p-&gt;exit_signal &gt;= 0;<BR>}</P>
<P>/* Do to the insanities of de_thread it is possible for a process<BR>&nbsp;* to have the pid of the thread group leader without actually being<BR>&nbsp;* the thread group leader.&nbsp; For iteration through the pids in proc<BR>&nbsp;* all we care about is that we have a task with the appropriate<BR>&nbsp;* pid, we don't actually care if we have the right task.<BR>&nbsp;*/<BR>static inline bool has_group_leader_pid(struct task_struct *p)<BR>{<BR>&nbsp;return task_pid(p) == p-&gt;signal-&gt;leader_pid;<BR>}</P>
<P>static inline<BR>bool same_thread_group(struct task_struct *p1, struct task_struct *p2)<BR>{<BR>&nbsp;return p1-&gt;signal == p2-&gt;signal;<BR>}</P>
<P>static inline struct task_struct *next_thread(const struct task_struct *p)<BR>{<BR>&nbsp;return list_entry_rcu(p-&gt;thread_group.next,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; struct task_struct, thread_group);<BR>}</P>
<P>static inline int thread_group_empty(struct task_struct *p)<BR>{<BR>&nbsp;return list_empty(&amp;p-&gt;thread_group);<BR>}</P>
<P>#define delay_group_leader(p) \<BR>&nbsp;&nbsp;(thread_group_leader(p) &amp;&amp; !thread_group_empty(p))</P>
<P>/*<BR>&nbsp;* Protects -&gt;fs, -&gt;files, -&gt;mm, -&gt;group_info, -&gt;comm, keyring<BR>&nbsp;* subscriptions and synchronises with wait4().&nbsp; Also used in procfs.&nbsp; Also<BR>&nbsp;* pins the final release of task.io_context.&nbsp; Also protects -&gt;cpuset and<BR>&nbsp;* -&gt;cgroup.subsys[]. And -&gt;vfork_done.<BR>&nbsp;*<BR>&nbsp;* Nests both inside and outside of read_lock(&amp;tasklist_lock).<BR>&nbsp;* It must not be nested with write_lock_irq(&amp;tasklist_lock),<BR>&nbsp;* neither inside nor outside.<BR>&nbsp;*/<BR>static inline void task_lock(struct task_struct *p)<BR>{<BR>&nbsp;spin_lock(&amp;p-&gt;alloc_lock);<BR>}</P>
<P>static inline void task_unlock(struct task_struct *p)<BR>{<BR>&nbsp;spin_unlock(&amp;p-&gt;alloc_lock);<BR>}</P>
<P>extern struct sighand_struct *__lock_task_sighand(struct task_struct *tsk,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;unsigned long *flags);</P>
<P>static inline struct sighand_struct *lock_task_sighand(struct task_struct *tsk,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; unsigned long *flags)<BR>{<BR>&nbsp;struct sighand_struct *ret;</P>
<P>&nbsp;ret = __lock_task_sighand(tsk, flags);<BR>&nbsp;(void)__cond_lock(&amp;tsk-&gt;sighand-&gt;siglock, ret);<BR>&nbsp;return ret;<BR>}</P>
<P>static inline void unlock_task_sighand(struct task_struct *tsk,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;unsigned long *flags)<BR>{<BR>&nbsp;spin_unlock_irqrestore(&amp;tsk-&gt;sighand-&gt;siglock, *flags);<BR>}</P>
<P>/**<BR>&nbsp;* threadgroup_change_begin - mark the beginning of changes to a threadgroup<BR>&nbsp;* @tsk: task causing the changes<BR>&nbsp;*<BR>&nbsp;* All operations which modify a threadgroup - a new thread joining the<BR>&nbsp;* group, death of a member thread (the assertion of PF_EXITING) and<BR>&nbsp;* exec(2) dethreading the process and replacing the leader - are wrapped<BR>&nbsp;* by threadgroup_change_{begin|end}().&nbsp; This is to provide a place which<BR>&nbsp;* subsystems needing threadgroup stability can hook into for<BR>&nbsp;* synchronization.<BR>&nbsp;*/<BR>static inline void threadgroup_change_begin(struct task_struct *tsk)<BR>{<BR>&nbsp;might_sleep();<BR>&nbsp;cgroup_threadgroup_change_begin(tsk);<BR>}</P>
<P>/**<BR>&nbsp;* threadgroup_change_end - mark the end of changes to a threadgroup<BR>&nbsp;* @tsk: task causing the changes<BR>&nbsp;*<BR>&nbsp;* See threadgroup_change_begin().<BR>&nbsp;*/<BR>static inline void threadgroup_change_end(struct task_struct *tsk)<BR>{<BR>&nbsp;cgroup_threadgroup_change_end(tsk);<BR>}</P>
<P>#ifndef __HAVE_THREAD_FUNCTIONS</P>
<P>#define task_thread_info(task)&nbsp;((struct thread_info *)(task)-&gt;stack)<BR>#define task_stack_page(task)&nbsp;((task)-&gt;stack)</P>
<P>static inline void setup_thread_stack(struct task_struct *p, struct task_struct *org)<BR>{<BR>&nbsp;*task_thread_info(p) = *task_thread_info(org);<BR>&nbsp;task_thread_info(p)-&gt;task = p;<BR>}</P>
<P>/*<BR>&nbsp;* Return the address of the last usable long on the stack.<BR>&nbsp;*<BR>&nbsp;* When the stack grows down, this is just above the thread<BR>&nbsp;* info struct. Going any lower will corrupt the threadinfo.<BR>&nbsp;*<BR>&nbsp;* When the stack grows up, this is the highest address.<BR>&nbsp;* Beyond that position, we corrupt data on the next page.<BR>&nbsp;*/<BR>static inline unsigned long *end_of_stack(struct task_struct *p)<BR>{<BR>#ifdef CONFIG_STACK_GROWSUP<BR>&nbsp;return (unsigned long *)((unsigned long)task_thread_info(p) + THREAD_SIZE) - 1;<BR>#else<BR>&nbsp;return (unsigned long *)(task_thread_info(p) + 1);<BR>#endif<BR>}</P>
<P>#endif<BR>#define task_stack_end_corrupted(task) \<BR>&nbsp;&nbsp;(*(end_of_stack(task)) != STACK_END_MAGIC)</P>
<P>static inline int object_is_on_stack(void *obj)<BR>{<BR>&nbsp;void *stack = task_stack_page(current);</P>
<P>&nbsp;return (obj &gt;= stack) &amp;&amp; (obj &lt; (stack + THREAD_SIZE));<BR>}</P>
<P>extern void thread_info_cache_init(void);</P>
<P>#ifdef CONFIG_DEBUG_STACK_USAGE<BR>static inline unsigned long stack_not_used(struct task_struct *p)<BR>{<BR>&nbsp;unsigned long *n = end_of_stack(p);</P>
<P>&nbsp;do { &nbsp;/* Skip over canary */<BR># ifdef CONFIG_STACK_GROWSUP<BR>&nbsp;&nbsp;n--;<BR># else<BR>&nbsp;&nbsp;n++;<BR># endif<BR>&nbsp;} while (!*n);</P>
<P># ifdef CONFIG_STACK_GROWSUP<BR>&nbsp;return (unsigned long)end_of_stack(p) - (unsigned long)n;<BR># else<BR>&nbsp;return (unsigned long)n - (unsigned long)end_of_stack(p);<BR># endif<BR>}<BR>#endif<BR>extern void set_task_stack_end_magic(struct task_struct *tsk);</P>
<P>/* set thread flags in other task's structures<BR>&nbsp;* - see asm/thread_info.h for TIF_xxxx flags available<BR>&nbsp;*/<BR>static inline void set_tsk_thread_flag(struct task_struct *tsk, int flag)<BR>{<BR>&nbsp;set_ti_thread_flag(task_thread_info(tsk), flag);<BR>}</P>
<P>static inline void clear_tsk_thread_flag(struct task_struct *tsk, int flag)<BR>{<BR>&nbsp;clear_ti_thread_flag(task_thread_info(tsk), flag);<BR>}</P>
<P>static inline int test_and_set_tsk_thread_flag(struct task_struct *tsk, int flag)<BR>{<BR>&nbsp;return test_and_set_ti_thread_flag(task_thread_info(tsk), flag);<BR>}</P>
<P>static inline int test_and_clear_tsk_thread_flag(struct task_struct *tsk, int flag)<BR>{<BR>&nbsp;return test_and_clear_ti_thread_flag(task_thread_info(tsk), flag);<BR>}</P>
<P>static inline int test_tsk_thread_flag(struct task_struct *tsk, int flag)<BR>{<BR>&nbsp;return test_ti_thread_flag(task_thread_info(tsk), flag);<BR>}</P>
<P>static inline void set_tsk_need_resched(struct task_struct *tsk)<BR>{<BR>&nbsp;set_tsk_thread_flag(tsk,TIF_NEED_RESCHED);<BR>}</P>
<P>static inline void clear_tsk_need_resched(struct task_struct *tsk)<BR>{<BR>&nbsp;clear_tsk_thread_flag(tsk,TIF_NEED_RESCHED);<BR>}</P>
<P>static inline int test_tsk_need_resched(struct task_struct *tsk)<BR>{<BR>&nbsp;return unlikely(test_tsk_thread_flag(tsk,TIF_NEED_RESCHED));<BR>}</P>
<P>static inline int restart_syscall(void)<BR>{<BR>&nbsp;set_tsk_thread_flag(current, TIF_SIGPENDING);<BR>&nbsp;return -ERESTARTNOINTR;<BR>}</P>
<P>static inline int signal_pending(struct task_struct *p)<BR>{<BR>&nbsp;return unlikely(test_tsk_thread_flag(p,TIF_SIGPENDING));<BR>}</P>
<P>static inline int __fatal_signal_pending(struct task_struct *p)<BR>{<BR>&nbsp;return unlikely(sigismember(&amp;p-&gt;pending.signal, SIGKILL));<BR>}</P>
<P>static inline int fatal_signal_pending(struct task_struct *p)<BR>{<BR>&nbsp;return signal_pending(p) &amp;&amp; __fatal_signal_pending(p);<BR>}</P>
<P>static inline int signal_pending_state(long state, struct task_struct *p)<BR>{<BR>&nbsp;if (!(state &amp; (TASK_INTERRUPTIBLE | TASK_WAKEKILL)))<BR>&nbsp;&nbsp;return 0;<BR>&nbsp;if (!signal_pending(p))<BR>&nbsp;&nbsp;return 0;</P>
<P>&nbsp;return (state &amp; TASK_INTERRUPTIBLE) || __fatal_signal_pending(p);<BR>}</P>
<P>/*<BR>&nbsp;* cond_resched() and cond_resched_lock(): latency reduction via<BR>&nbsp;* explicit rescheduling in places that are safe. The return<BR>&nbsp;* value indicates whether a reschedule was done in fact.<BR>&nbsp;* cond_resched_lock() will drop the spinlock before scheduling,<BR>&nbsp;* cond_resched_softirq() will enable bhs before scheduling.<BR>&nbsp;*/<BR>extern int _cond_resched(void);</P>
<P>#define cond_resched() ({&nbsp;&nbsp;&nbsp;\<BR>&nbsp;___might_sleep(__FILE__, __LINE__, 0);&nbsp;\<BR>&nbsp;_cond_resched();&nbsp;&nbsp;&nbsp;\<BR>})</P>
<P>extern int __cond_resched_lock(spinlock_t *lock);</P>
<P>#define cond_resched_lock(lock) ({&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;___might_sleep(__FILE__, __LINE__, PREEMPT_LOCK_OFFSET);\<BR>&nbsp;__cond_resched_lock(lock);&nbsp;&nbsp;&nbsp;&nbsp;\<BR>})</P>
<P>extern int __cond_resched_softirq(void);</P>
<P>#define cond_resched_softirq() ({&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;___might_sleep(__FILE__, __LINE__, SOFTIRQ_DISABLE_OFFSET);&nbsp;\<BR>&nbsp;__cond_resched_softirq();&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\<BR>})</P>
<P>static inline void cond_resched_rcu(void)<BR>{<BR>#if defined(CONFIG_DEBUG_ATOMIC_SLEEP) || !defined(CONFIG_PREEMPT_RCU)<BR>&nbsp;rcu_read_unlock();<BR>&nbsp;cond_resched();<BR>&nbsp;rcu_read_lock();<BR>#endif<BR>}</P>
<P>/*<BR>&nbsp;* Does a critical section need to be broken due to another<BR>&nbsp;* task waiting?: (technically does not depend on CONFIG_PREEMPT,<BR>&nbsp;* but a general need for low latency)<BR>&nbsp;*/<BR>static inline int spin_needbreak(spinlock_t *lock)<BR>{<BR>#ifdef CONFIG_PREEMPT<BR>&nbsp;return spin_is_contended(lock);<BR>#else<BR>&nbsp;return 0;<BR>#endif<BR>}</P>
<P>/*<BR>&nbsp;* Idle thread specific functions to determine the need_resched<BR>&nbsp;* polling state.<BR>&nbsp;*/<BR>#ifdef TIF_POLLING_NRFLAG<BR>static inline int tsk_is_polling(struct task_struct *p)<BR>{<BR>&nbsp;return test_tsk_thread_flag(p, TIF_POLLING_NRFLAG);<BR>}</P>
<P>static inline void __current_set_polling(void)<BR>{<BR>&nbsp;set_thread_flag(TIF_POLLING_NRFLAG);<BR>}</P>
<P>static inline bool __must_check current_set_polling_and_test(void)<BR>{<BR>&nbsp;__current_set_polling();</P>
<P>&nbsp;/*<BR>&nbsp; * Polling state must be visible before we test NEED_RESCHED,<BR>&nbsp; * paired by resched_curr()<BR>&nbsp; */<BR>&nbsp;smp_mb__after_atomic();</P>
<P>&nbsp;return unlikely(tif_need_resched());<BR>}</P>
<P>static inline void __current_clr_polling(void)<BR>{<BR>&nbsp;clear_thread_flag(TIF_POLLING_NRFLAG);<BR>}</P>
<P>static inline bool __must_check current_clr_polling_and_test(void)<BR>{<BR>&nbsp;__current_clr_polling();</P>
<P>&nbsp;/*<BR>&nbsp; * Polling state must be visible before we test NEED_RESCHED,<BR>&nbsp; * paired by resched_curr()<BR>&nbsp; */<BR>&nbsp;smp_mb__after_atomic();</P>
<P>&nbsp;return unlikely(tif_need_resched());<BR>}</P>
<P>#else<BR>static inline int tsk_is_polling(struct task_struct *p) { return 0; }<BR>static inline void __current_set_polling(void) { }<BR>static inline void __current_clr_polling(void) { }</P>
<P>static inline bool __must_check current_set_polling_and_test(void)<BR>{<BR>&nbsp;return unlikely(tif_need_resched());<BR>}<BR>static inline bool __must_check current_clr_polling_and_test(void)<BR>{<BR>&nbsp;return unlikely(tif_need_resched());<BR>}<BR>#endif</P>
<P>static inline void current_clr_polling(void)<BR>{<BR>&nbsp;__current_clr_polling();</P>
<P>&nbsp;/*<BR>&nbsp; * Ensure we check TIF_NEED_RESCHED after we clear the polling bit.<BR>&nbsp; * Once the bit is cleared, we'll get IPIs with every new<BR>&nbsp; * TIF_NEED_RESCHED and the IPI handler, scheduler_ipi(), will also<BR>&nbsp; * fold.<BR>&nbsp; */<BR>&nbsp;smp_mb(); /* paired with resched_curr() */</P>
<P>&nbsp;preempt_fold_need_resched();<BR>}</P>
<P>static __always_inline bool need_resched(void)<BR>{<BR>&nbsp;return unlikely(tif_need_resched());<BR>}</P>
<P>/*<BR>&nbsp;* Thread group CPU time accounting.<BR>&nbsp;*/<BR>void thread_group_cputime(struct task_struct *tsk, struct task_cputime *times);<BR>void thread_group_cputimer(struct task_struct *tsk, struct task_cputime *times);</P>
<P>/*<BR>&nbsp;* Reevaluate whether the task has signals pending delivery.<BR>&nbsp;* Wake the task if so.<BR>&nbsp;* This is required every time the blocked sigset_t changes.<BR>&nbsp;* callers must hold sighand-&gt;siglock.<BR>&nbsp;*/<BR>extern void recalc_sigpending_and_wake(struct task_struct *t);<BR>extern void recalc_sigpending(void);</P>
<P>extern void signal_wake_up_state(struct task_struct *t, unsigned int state);</P>
<P>static inline void signal_wake_up(struct task_struct *t, bool resume)<BR>{<BR>&nbsp;signal_wake_up_state(t, resume ? TASK_WAKEKILL : 0);<BR>}<BR>static inline void ptrace_signal_wake_up(struct task_struct *t, bool resume)<BR>{<BR>&nbsp;signal_wake_up_state(t, resume ? __TASK_TRACED : 0);<BR>}</P>
<P>/*<BR>&nbsp;* Wrappers for p-&gt;thread_info-&gt;cpu access. No-op on UP.<BR>&nbsp;*/<BR>#ifdef CONFIG_SMP</P>
<P>static inline unsigned int task_cpu(const struct task_struct *p)<BR>{<BR>&nbsp;return task_thread_info(p)-&gt;cpu;<BR>}</P>
<P>static inline int task_node(const struct task_struct *p)<BR>{<BR>&nbsp;return cpu_to_node(task_cpu(p));<BR>}</P>
<P>extern void set_task_cpu(struct task_struct *p, unsigned int cpu);</P>
<P>#else</P>
<P>static inline unsigned int task_cpu(const struct task_struct *p)<BR>{<BR>&nbsp;return 0;<BR>}</P>
<P>static inline void set_task_cpu(struct task_struct *p, unsigned int cpu)<BR>{<BR>}</P>
<P>#endif /* CONFIG_SMP */</P>
<P>extern long sched_setaffinity(pid_t pid, const struct cpumask *new_mask);<BR>extern long sched_getaffinity(pid_t pid, struct cpumask *mask);</P>
<P>#ifdef CONFIG_CGROUP_SCHED<BR>extern struct task_group root_task_group;<BR>#endif /* CONFIG_CGROUP_SCHED */</P>
<P>extern int task_can_switch_user(struct user_struct *up,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;struct task_struct *tsk);</P>
<P>#ifdef CONFIG_TASK_XACCT<BR>static inline void add_rchar(struct task_struct *tsk, ssize_t amt)<BR>{<BR>&nbsp;tsk-&gt;ioac.rchar += amt;<BR>}</P>
<P>static inline void add_wchar(struct task_struct *tsk, ssize_t amt)<BR>{<BR>&nbsp;tsk-&gt;ioac.wchar += amt;<BR>}</P>
<P>static inline void inc_syscr(struct task_struct *tsk)<BR>{<BR>&nbsp;tsk-&gt;ioac.syscr++;<BR>}</P>
<P>static inline void inc_syscw(struct task_struct *tsk)<BR>{<BR>&nbsp;tsk-&gt;ioac.syscw++;<BR>}<BR>#else<BR>static inline void add_rchar(struct task_struct *tsk, ssize_t amt)<BR>{<BR>}</P>
<P>static inline void add_wchar(struct task_struct *tsk, ssize_t amt)<BR>{<BR>}</P>
<P>static inline void inc_syscr(struct task_struct *tsk)<BR>{<BR>}</P>
<P>static inline void inc_syscw(struct task_struct *tsk)<BR>{<BR>}<BR>#endif</P>
<P>#ifndef TASK_SIZE_OF<BR>#define TASK_SIZE_OF(tsk)&nbsp;TASK_SIZE<BR>#endif</P>
<P>#ifdef CONFIG_MEMCG<BR>extern void mm_update_next_owner(struct mm_struct *mm);<BR>#else<BR>static inline void mm_update_next_owner(struct mm_struct *mm)<BR>{<BR>}<BR>#endif /* CONFIG_MEMCG */</P>
<P>static inline unsigned long task_rlimit(const struct task_struct *tsk,<BR>&nbsp;&nbsp;unsigned int limit)<BR>{<BR>&nbsp;return READ_ONCE(tsk-&gt;signal-&gt;rlim[limit].rlim_cur);<BR>}</P>
<P>static inline unsigned long task_rlimit_max(const struct task_struct *tsk,<BR>&nbsp;&nbsp;unsigned int limit)<BR>{<BR>&nbsp;return READ_ONCE(tsk-&gt;signal-&gt;rlim[limit].rlim_max);<BR>}</P>
<P>static inline unsigned long rlimit(unsigned int limit)<BR>{<BR>&nbsp;return task_rlimit(current, limit);<BR>}</P>
<P>static inline unsigned long rlimit_max(unsigned int limit)<BR>{<BR>&nbsp;return task_rlimit_max(current, limit);<BR>}</P>
<P>#ifdef CONFIG_CPU_FREQ<BR>struct update_util_data {<BR>&nbsp;void (*func)(struct update_util_data *data,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; u64 time, unsigned long util, unsigned long max);<BR>};</P>
<P>void cpufreq_add_update_util_hook(int cpu, struct update_util_data *data,<BR>&nbsp;&nbsp;&nbsp;void (*func)(struct update_util_data *data, u64 time,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; unsigned long util, unsigned long max));<BR>void cpufreq_remove_update_util_hook(int cpu);<BR>#endif /* CONFIG_CPU_FREQ */</P>
<P>#endif