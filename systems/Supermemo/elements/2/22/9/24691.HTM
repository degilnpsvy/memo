kernel/sched/wait.c 
<P></P>
<P>/*<BR>&nbsp;* Generic waiting primitives.<BR>&nbsp;*<BR>&nbsp;* (C) 2004 Nadia Yvette Chambers, Oracle<BR>&nbsp;*/<BR>#include &lt;linux/init.h&gt;<BR>#include &lt;linux/export.h&gt;<BR>#include &lt;linux/sched.h&gt;<BR>#include &lt;linux/mm.h&gt;<BR>#include &lt;linux/wait.h&gt;<BR>#include &lt;linux/hash.h&gt;<BR>#include &lt;linux/kthread.h&gt;</P>
<P></P>
<P>void __init_waitqueue_head(wait_queue_head_t *q, const char *name, struct lock_class_key *key)<BR>{<BR>&nbsp;spin_lock_init(&amp;q-&gt;lock);<BR>&nbsp;lockdep_set_class_and_name(&amp;q-&gt;lock, key, name);<BR>&nbsp;INIT_LIST_HEAD(&amp;q-&gt;task_list);<BR>}</P>
<P>EXPORT_SYMBOL(__init_waitqueue_head);</P>
<P>void add_wait_queue(wait_queue_head_t *q, wait_queue_t *wait)<BR>{<BR>&nbsp;unsigned long flags;</P>
<P>&nbsp;wait-&gt;flags &amp;= ~WQ_FLAG_EXCLUSIVE;<BR>&nbsp;spin_lock_irqsave(&amp;q-&gt;lock, flags);<BR>&nbsp;__add_wait_queue(q, wait);<BR>&nbsp;spin_unlock_irqrestore(&amp;q-&gt;lock, flags);<BR>}<BR>EXPORT_SYMBOL(add_wait_queue);</P>
<P>void add_wait_queue_exclusive(wait_queue_head_t *q, wait_queue_t *wait)<BR>{<BR>&nbsp;unsigned long flags;</P>
<P>&nbsp;wait-&gt;flags |= WQ_FLAG_EXCLUSIVE;<BR>&nbsp;spin_lock_irqsave(&amp;q-&gt;lock, flags);<BR>&nbsp;__add_wait_queue_tail(q, wait);<BR>&nbsp;spin_unlock_irqrestore(&amp;q-&gt;lock, flags);<BR>}<BR>EXPORT_SYMBOL(add_wait_queue_exclusive);</P>
<P>void remove_wait_queue(wait_queue_head_t *q, wait_queue_t *wait)<BR>{<BR>&nbsp;unsigned long flags;</P>
<P>&nbsp;spin_lock_irqsave(&amp;q-&gt;lock, flags);<BR>&nbsp;__remove_wait_queue(q, wait);<BR>&nbsp;spin_unlock_irqrestore(&amp;q-&gt;lock, flags);<BR>}<BR>EXPORT_SYMBOL(remove_wait_queue);</P>
<P><BR><FONT class=extract>/*<BR>&nbsp;* The core wakeup function. Non-exclusive wakeups (nr_exclusive == 0) just<BR>&nbsp;* wake everything up. If it's an exclusive wakeup (nr_exclusive == small +ve<BR>&nbsp;* number) then we wake all the non-exclusive tasks and one exclusive task.<BR>&nbsp;*<BR>&nbsp;* There are circumstances in which we can try to wake a task which has already<BR>&nbsp;* started to run but is not in state TASK_RUNNING. try_to_wake_up() returns<BR>&nbsp;* zero in this (rare) case, and we handle it by continuing to scan the queue.<BR>&nbsp;*/<BR>static void __wake_up_common(wait_queue_head_t *q, unsigned int mode,<BR>&nbsp;&nbsp;&nbsp;int nr_exclusive, int wake_flags, void *key)<BR>{<BR>&nbsp;wait_queue_t *curr, *next;</FONT></P>
<P><FONT class=extract>&nbsp;list_for_each_entry_safe(curr, next, &amp;q-&gt;task_list, task_list) {<BR>&nbsp;&nbsp;unsigned flags = curr-&gt;flags;</FONT></P>
<P><FONT class=extract>&nbsp;&nbsp;if (curr-&gt;func(curr, mode, wake_flags, key) &amp;&amp;<BR>&nbsp;&nbsp;&nbsp;&nbsp;(flags &amp; WQ_FLAG_EXCLUSIVE) &amp;&amp; !--nr_exclusive)<BR>&nbsp;&nbsp;&nbsp;break;<BR>&nbsp;}<BR>}</FONT></P>
<P>/**<BR>&nbsp;* __wake_up - wake up threads blocked on a waitqueue.<BR>&nbsp;* @q: the waitqueue<BR>&nbsp;* @mode: which threads<BR>&nbsp;* @nr_exclusive: how many wake-one or wake-many threads to wake up<BR>&nbsp;* @key: is directly passed to the wakeup function<BR>&nbsp;*<BR>&nbsp;* It may be assumed that this function implies a write memory barrier before<BR>&nbsp;* changing the task state if and only if any tasks are woken up.<BR>&nbsp;*/<BR>void __wake_up(wait_queue_head_t *q, unsigned int mode,<BR>&nbsp;&nbsp;&nbsp;int nr_exclusive, void *key)<BR>{<BR>&nbsp;unsigned long flags;</P>
<P>&nbsp;spin_lock_irqsave(&amp;q-&gt;lock, flags);<BR>&nbsp;__wake_up_common(q, mode, nr_exclusive, 0, key);<BR>&nbsp;spin_unlock_irqrestore(&amp;q-&gt;lock, flags);<BR>}<BR>EXPORT_SYMBOL(__wake_up);</P>
<P>/*<BR>&nbsp;* Same as __wake_up but called with the spinlock in wait_queue_head_t held.<BR>&nbsp;*/<BR>void __wake_up_locked(wait_queue_head_t *q, unsigned int mode, int nr)<BR>{<BR>&nbsp;__wake_up_common(q, mode, nr, 0, NULL);<BR>}<BR>EXPORT_SYMBOL_GPL(__wake_up_locked);</P>
<P>void __wake_up_locked_key(wait_queue_head_t *q, unsigned int mode, void *key)<BR>{<BR>&nbsp;__wake_up_common(q, mode, 1, 0, key);<BR>}<BR>EXPORT_SYMBOL_GPL(__wake_up_locked_key);</P>
<P>/**<BR>&nbsp;* __wake_up_sync_key - wake up threads blocked on a waitqueue.<BR>&nbsp;* @q: the waitqueue<BR>&nbsp;* @mode: which threads<BR>&nbsp;* @nr_exclusive: how many wake-one or wake-many threads to wake up<BR>&nbsp;* @key: opaque value to be passed to wakeup targets<BR>&nbsp;*<BR>&nbsp;* The sync wakeup differs that the waker knows that it will schedule<BR>&nbsp;* away soon, so while the target thread will be woken up, it will not<BR>&nbsp;* be migrated to another CPU - ie. the two threads are 'synchronized'<BR>&nbsp;* with each other. This can prevent needless bouncing between CPUs.<BR>&nbsp;*<BR>&nbsp;* On UP it can prevent extra preemption.<BR>&nbsp;*<BR>&nbsp;* It may be assumed that this function implies a write memory barrier before<BR>&nbsp;* changing the task state if and only if any tasks are woken up.<BR>&nbsp;*/<BR>void __wake_up_sync_key(wait_queue_head_t *q, unsigned int mode,<BR>&nbsp;&nbsp;&nbsp;int nr_exclusive, void *key)<BR>{<BR>&nbsp;unsigned long flags;<BR>&nbsp;int wake_flags = 1; /* XXX WF_SYNC */</P>
<P>&nbsp;if (unlikely(!q))<BR>&nbsp;&nbsp;return;</P>
<P>&nbsp;if (unlikely(nr_exclusive != 1))<BR>&nbsp;&nbsp;wake_flags = 0;</P>
<P>&nbsp;spin_lock_irqsave(&amp;q-&gt;lock, flags);<BR>&nbsp;__wake_up_common(q, mode, nr_exclusive, wake_flags, key);<BR>&nbsp;spin_unlock_irqrestore(&amp;q-&gt;lock, flags);<BR>}<BR>EXPORT_SYMBOL_GPL(__wake_up_sync_key);</P>
<P>/*<BR>&nbsp;* __wake_up_sync - see __wake_up_sync_key()<BR>&nbsp;*/<BR>void __wake_up_sync(wait_queue_head_t *q, unsigned int mode, int nr_exclusive)<BR>{<BR>&nbsp;__wake_up_sync_key(q, mode, nr_exclusive, NULL);<BR>}<BR>EXPORT_SYMBOL_GPL(__wake_up_sync);&nbsp;/* For internal use only */</P>
<P>/*<BR>&nbsp;* Note: we use "set_current_state()" _after_ the wait-queue add,<BR>&nbsp;* because we need a memory barrier there on SMP, so that any<BR>&nbsp;* wake-function that tests for the wait-queue being active<BR>&nbsp;* will be guaranteed to see waitqueue addition _or_ subsequent<BR>&nbsp;* tests in this thread will see the wakeup having taken place.<BR>&nbsp;*<BR>&nbsp;* The spin_unlock() itself is semi-permeable and only protects<BR>&nbsp;* one way (it only protects stuff inside the critical region and<BR>&nbsp;* stops them from bleeding out - it would still allow subsequent<BR>&nbsp;* loads to move into the critical region).<BR>&nbsp;*/<BR>void<BR>prepare_to_wait(wait_queue_head_t *q, wait_queue_t *wait, int state)<BR>{<BR>&nbsp;unsigned long flags;</P>
<P>&nbsp;wait-&gt;flags &amp;= ~WQ_FLAG_EXCLUSIVE;<BR>&nbsp;spin_lock_irqsave(&amp;q-&gt;lock, flags);<BR>&nbsp;if (list_empty(&amp;wait-&gt;task_list))<BR>&nbsp;&nbsp;__add_wait_queue(q, wait);<BR>&nbsp;set_current_state(state);<BR>&nbsp;spin_unlock_irqrestore(&amp;q-&gt;lock, flags);<BR>}<BR>EXPORT_SYMBOL(prepare_to_wait);</P>
<P>void<BR>prepare_to_wait_exclusive(wait_queue_head_t *q, wait_queue_t *wait, int state)<BR>{<BR>&nbsp;unsigned long flags;</P>
<P>&nbsp;wait-&gt;flags |= WQ_FLAG_EXCLUSIVE;<BR>&nbsp;spin_lock_irqsave(&amp;q-&gt;lock, flags);<BR>&nbsp;if (list_empty(&amp;wait-&gt;task_list))<BR>&nbsp;&nbsp;__add_wait_queue_tail(q, wait);<BR>&nbsp;set_current_state(state);<BR>&nbsp;spin_unlock_irqrestore(&amp;q-&gt;lock, flags);<BR>}<BR>EXPORT_SYMBOL(prepare_to_wait_exclusive);</P>
<P>long prepare_to_wait_event(wait_queue_head_t *q, wait_queue_t *wait, int state)<BR>{<BR>&nbsp;unsigned long flags;</P>
<P>&nbsp;if (signal_pending_state(state, current))<BR>&nbsp;&nbsp;return -ERESTARTSYS;</P>
<P>&nbsp;wait-&gt;private = current;<BR>&nbsp;wait-&gt;func = autoremove_wake_function;</P>
<P>&nbsp;spin_lock_irqsave(&amp;q-&gt;lock, flags);<BR>&nbsp;if (list_empty(&amp;wait-&gt;task_list)) {<BR>&nbsp;&nbsp;if (wait-&gt;flags &amp; WQ_FLAG_EXCLUSIVE)<BR>&nbsp;&nbsp;&nbsp;__add_wait_queue_tail(q, wait);<BR>&nbsp;&nbsp;else<BR>&nbsp;&nbsp;&nbsp;__add_wait_queue(q, wait);<BR>&nbsp;}<BR>&nbsp;set_current_state(state);<BR>&nbsp;spin_unlock_irqrestore(&amp;q-&gt;lock, flags);</P>
<P>&nbsp;return 0;<BR>}<BR>EXPORT_SYMBOL(prepare_to_wait_event);</P>
<P>/**<BR>&nbsp;* finish_wait - clean up after waiting in a queue<BR>&nbsp;* @q: waitqueue waited on<BR>&nbsp;* @wait: wait descriptor<BR>&nbsp;*<BR>&nbsp;* Sets current thread back to running state and removes<BR>&nbsp;* the wait descriptor from the given waitqueue if still<BR>&nbsp;* queued.<BR>&nbsp;*/<BR>void finish_wait(wait_queue_head_t *q, wait_queue_t *wait)<BR>{<BR>&nbsp;unsigned long flags;</P>
<P>&nbsp;__set_current_state(TASK_RUNNING);<BR>&nbsp;/*<BR>&nbsp; * We can check for list emptiness outside the lock<BR>&nbsp; * IFF:<BR>&nbsp; *&nbsp; - we use the "careful" check that verifies both<BR>&nbsp; *&nbsp;&nbsp;&nbsp; the next and prev pointers, so that there cannot<BR>&nbsp; *&nbsp;&nbsp;&nbsp; be any half-pending updates in progress on other<BR>&nbsp; *&nbsp;&nbsp;&nbsp; CPU's that we haven't seen yet (and that might<BR>&nbsp; *&nbsp;&nbsp;&nbsp; still change the stack area.<BR>&nbsp; * and<BR>&nbsp; *&nbsp; - all other users take the lock (ie we can only<BR>&nbsp; *&nbsp;&nbsp;&nbsp; have _one_ other CPU that looks at or modifies<BR>&nbsp; *&nbsp;&nbsp;&nbsp; the list).<BR>&nbsp; */<BR>&nbsp;if (!list_empty_careful(&amp;wait-&gt;task_list)) {<BR>&nbsp;&nbsp;spin_lock_irqsave(&amp;q-&gt;lock, flags);<BR>&nbsp;&nbsp;list_del_init(&amp;wait-&gt;task_list);<BR>&nbsp;&nbsp;spin_unlock_irqrestore(&amp;q-&gt;lock, flags);<BR>&nbsp;}<BR>}<BR>EXPORT_SYMBOL(finish_wait);</P>
<P>/**<BR>&nbsp;* abort_exclusive_wait - abort exclusive waiting in a queue<BR>&nbsp;* @q: waitqueue waited on<BR>&nbsp;* @wait: wait descriptor<BR>&nbsp;* @mode: runstate of the waiter to be woken<BR>&nbsp;* @key: key to identify a wait bit queue or %NULL<BR>&nbsp;*<BR>&nbsp;* Sets current thread back to running state and removes<BR>&nbsp;* the wait descriptor from the given waitqueue if still<BR>&nbsp;* queued.<BR>&nbsp;*<BR>&nbsp;* Wakes up the next waiter if the caller is concurrently<BR>&nbsp;* woken up through the queue.<BR>&nbsp;*<BR>&nbsp;* This prevents waiter starvation where an exclusive waiter<BR>&nbsp;* aborts and is woken up concurrently and no one wakes up<BR>&nbsp;* the next waiter.<BR>&nbsp;*/<BR>void abort_exclusive_wait(wait_queue_head_t *q, wait_queue_t *wait,<BR>&nbsp;&nbsp;&nbsp;unsigned int mode, void *key)<BR>{<BR>&nbsp;unsigned long flags;</P>
<P>&nbsp;__set_current_state(TASK_RUNNING);<BR>&nbsp;spin_lock_irqsave(&amp;q-&gt;lock, flags);<BR>&nbsp;if (!list_empty(&amp;wait-&gt;task_list))<BR>&nbsp;&nbsp;list_del_init(&amp;wait-&gt;task_list);<BR>&nbsp;else if (waitqueue_active(q))<BR>&nbsp;&nbsp;__wake_up_locked_key(q, mode, key);<BR>&nbsp;spin_unlock_irqrestore(&amp;q-&gt;lock, flags);<BR>}<BR>EXPORT_SYMBOL(abort_exclusive_wait);</P>
<P>int autoremove_wake_function(wait_queue_t *wait, unsigned mode, int sync, void *key)<BR>{<BR>&nbsp;int ret = default_wake_function(wait, mode, sync, key);</P>
<P>&nbsp;if (ret)<BR>&nbsp;&nbsp;list_del_init(&amp;wait-&gt;task_list);<BR>&nbsp;return ret;<BR>}<BR>EXPORT_SYMBOL(autoremove_wake_function);</P>
<P>static inline bool is_kthread_should_stop(void)<BR>{<BR>&nbsp;return (current-&gt;flags &amp; PF_KTHREAD) &amp;&amp; kthread_should_stop();<BR>}</P>
<P>/*<BR>&nbsp;* DEFINE_WAIT_FUNC(wait, woken_wake_func);<BR>&nbsp;*<BR>&nbsp;* add_wait_queue(&amp;wq, &amp;wait);<BR>&nbsp;* for (;;) {<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp; if (condition)<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; break;<BR>&nbsp;*<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp; p-&gt;state = mode;&nbsp;&nbsp;&nbsp;&nbsp;condition = true;<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp; smp_mb(); // A&nbsp;&nbsp;&nbsp;&nbsp;smp_wmb(); // C<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp; if (!wait-&gt;flags &amp; WQ_FLAG_WOKEN)&nbsp;wait-&gt;flags |= WQ_FLAG_WOKEN;<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; schedule()&nbsp;&nbsp;&nbsp;&nbsp;try_to_wake_up();<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp; p-&gt;state = TASK_RUNNING;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ~~~~~~~~~~~~~~~~~~<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp; wait-&gt;flags &amp;= ~WQ_FLAG_WOKEN;&nbsp;&nbsp;condition = true;<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp; smp_mb() // B&nbsp;&nbsp;&nbsp;&nbsp;smp_wmb(); // C<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;wait-&gt;flags |= WQ_FLAG_WOKEN;<BR>&nbsp;* }<BR>&nbsp;* remove_wait_queue(&amp;wq, &amp;wait);<BR>&nbsp;*<BR>&nbsp;*/<BR>long wait_woken(wait_queue_t *wait, unsigned mode, long timeout)<BR>{<BR>&nbsp;set_current_state(mode); /* A */<BR>&nbsp;/*<BR>&nbsp; * The above implies an smp_mb(), which matches with the smp_wmb() from<BR>&nbsp; * woken_wake_function() such that if we observe WQ_FLAG_WOKEN we must<BR>&nbsp; * also observe all state before the wakeup.<BR>&nbsp; */<BR>&nbsp;if (!(wait-&gt;flags &amp; WQ_FLAG_WOKEN) &amp;&amp; !is_kthread_should_stop())<BR>&nbsp;&nbsp;timeout = schedule_timeout(timeout);<BR>&nbsp;__set_current_state(TASK_RUNNING);</P>
<P>&nbsp;/*<BR>&nbsp; * The below implies an smp_mb(), it too pairs with the smp_wmb() from<BR>&nbsp; * woken_wake_function() such that we must either observe the wait<BR>&nbsp; * condition being true _OR_ WQ_FLAG_WOKEN such that we will not miss<BR>&nbsp; * an event.<BR>&nbsp; */<BR>&nbsp;smp_store_mb(wait-&gt;flags, wait-&gt;flags &amp; ~WQ_FLAG_WOKEN); /* B */</P>
<P>&nbsp;return timeout;<BR>}<BR>EXPORT_SYMBOL(wait_woken);</P>
<P>int woken_wake_function(wait_queue_t *wait, unsigned mode, int sync, void *key)<BR>{<BR>&nbsp;/*<BR>&nbsp; * Although this function is called under waitqueue lock, LOCK<BR>&nbsp; * doesn't imply write barrier and the users expects write<BR>&nbsp; * barrier semantics on wakeup functions.&nbsp; The following<BR>&nbsp; * smp_wmb() is equivalent to smp_wmb() in try_to_wake_up()<BR>&nbsp; * and is paired with smp_store_mb() in wait_woken().<BR>&nbsp; */<BR>&nbsp;smp_wmb(); /* C */<BR>&nbsp;wait-&gt;flags |= WQ_FLAG_WOKEN;</P>
<P>&nbsp;return default_wake_function(wait, mode, sync, key);<BR>}<BR>EXPORT_SYMBOL(woken_wake_function);</P>
<P>int wake_bit_function(wait_queue_t *wait, unsigned mode, int sync, void *arg)<BR>{<BR>&nbsp;struct wait_bit_key *key = arg;<BR>&nbsp;struct wait_bit_queue *wait_bit<BR>&nbsp;&nbsp;= container_of(wait, struct wait_bit_queue, wait);</P>
<P>&nbsp;if (wait_bit-&gt;key.flags != key-&gt;flags ||<BR>&nbsp;&nbsp;&nbsp;wait_bit-&gt;key.bit_nr != key-&gt;bit_nr ||<BR>&nbsp;&nbsp;&nbsp;test_bit(key-&gt;bit_nr, key-&gt;flags))<BR>&nbsp;&nbsp;return 0;<BR>&nbsp;else<BR>&nbsp;&nbsp;return autoremove_wake_function(wait, mode, sync, key);<BR>}<BR>EXPORT_SYMBOL(wake_bit_function);</P>
<P>/*<BR>&nbsp;* To allow interruptible waiting and asynchronous (i.e. nonblocking)<BR>&nbsp;* waiting, the actions of __wait_on_bit() and __wait_on_bit_lock() are<BR>&nbsp;* permitted return codes. Nonzero return codes halt waiting and return.<BR>&nbsp;*/<BR>int __sched<BR>__wait_on_bit(wait_queue_head_t *wq, struct wait_bit_queue *q,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; wait_bit_action_f *action, unsigned mode)<BR>{<BR>&nbsp;int ret = 0;</P>
<P>&nbsp;do {<BR>&nbsp;&nbsp;prepare_to_wait(wq, &amp;q-&gt;wait, mode);<BR>&nbsp;&nbsp;if (test_bit(q-&gt;key.bit_nr, q-&gt;key.flags))<BR>&nbsp;&nbsp;&nbsp;ret = (*action)(&amp;q-&gt;key);<BR>&nbsp;} while (test_bit(q-&gt;key.bit_nr, q-&gt;key.flags) &amp;&amp; !ret);<BR>&nbsp;finish_wait(wq, &amp;q-&gt;wait);<BR>&nbsp;return ret;<BR>}<BR>EXPORT_SYMBOL(__wait_on_bit);</P>
<P>int __sched out_of_line_wait_on_bit(void *word, int bit,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; wait_bit_action_f *action, unsigned mode)<BR>{<BR>&nbsp;wait_queue_head_t *wq = bit_waitqueue(word, bit);<BR>&nbsp;DEFINE_WAIT_BIT(wait, word, bit);</P>
<P>&nbsp;return __wait_on_bit(wq, &amp;wait, action, mode);<BR>}<BR>EXPORT_SYMBOL(out_of_line_wait_on_bit);</P>
<P>int __sched out_of_line_wait_on_bit_timeout(<BR>&nbsp;void *word, int bit, wait_bit_action_f *action,<BR>&nbsp;unsigned mode, unsigned long timeout)<BR>{<BR>&nbsp;wait_queue_head_t *wq = bit_waitqueue(word, bit);<BR>&nbsp;DEFINE_WAIT_BIT(wait, word, bit);</P>
<P>&nbsp;wait.key.timeout = jiffies + timeout;<BR>&nbsp;return __wait_on_bit(wq, &amp;wait, action, mode);<BR>}<BR>EXPORT_SYMBOL_GPL(out_of_line_wait_on_bit_timeout);</P>
<P>int __sched<BR>__wait_on_bit_lock(wait_queue_head_t *wq, struct wait_bit_queue *q,<BR>&nbsp;&nbsp;&nbsp;wait_bit_action_f *action, unsigned mode)<BR>{<BR>&nbsp;do {<BR>&nbsp;&nbsp;int ret;</P>
<P>&nbsp;&nbsp;prepare_to_wait_exclusive(wq, &amp;q-&gt;wait, mode);<BR>&nbsp;&nbsp;if (!test_bit(q-&gt;key.bit_nr, q-&gt;key.flags))<BR>&nbsp;&nbsp;&nbsp;continue;<BR>&nbsp;&nbsp;ret = action(&amp;q-&gt;key);<BR>&nbsp;&nbsp;if (!ret)<BR>&nbsp;&nbsp;&nbsp;continue;<BR>&nbsp;&nbsp;abort_exclusive_wait(wq, &amp;q-&gt;wait, mode, &amp;q-&gt;key);<BR>&nbsp;&nbsp;return ret;<BR>&nbsp;} while (test_and_set_bit(q-&gt;key.bit_nr, q-&gt;key.flags));<BR>&nbsp;finish_wait(wq, &amp;q-&gt;wait);<BR>&nbsp;return 0;<BR>}<BR>EXPORT_SYMBOL(__wait_on_bit_lock);</P>
<P>int __sched out_of_line_wait_on_bit_lock(void *word, int bit,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; wait_bit_action_f *action, unsigned mode)<BR>{<BR>&nbsp;wait_queue_head_t *wq = bit_waitqueue(word, bit);<BR>&nbsp;DEFINE_WAIT_BIT(wait, word, bit);</P>
<P>&nbsp;return __wait_on_bit_lock(wq, &amp;wait, action, mode);<BR>}<BR>EXPORT_SYMBOL(out_of_line_wait_on_bit_lock);</P>
<P>void __wake_up_bit(wait_queue_head_t *wq, void *word, int bit)<BR>{<BR>&nbsp;struct wait_bit_key key = __WAIT_BIT_KEY_INITIALIZER(word, bit);<BR>&nbsp;if (waitqueue_active(wq))<BR>&nbsp;&nbsp;__wake_up(wq, TASK_NORMAL, 1, &amp;key);<BR>}<BR>EXPORT_SYMBOL(__wake_up_bit);</P>
<P>/**<BR>&nbsp;* wake_up_bit - wake up a waiter on a bit<BR>&nbsp;* @word: the word being waited on, a kernel virtual address<BR>&nbsp;* @bit: the bit of the word being waited on<BR>&nbsp;*<BR>&nbsp;* There is a standard hashed waitqueue table for generic use. This<BR>&nbsp;* is the part of the hashtable's accessor API that wakes up waiters<BR>&nbsp;* on a bit. For instance, if one were to have waiters on a bitflag,<BR>&nbsp;* one would call wake_up_bit() after clearing the bit.<BR>&nbsp;*<BR>&nbsp;* In order for this to function properly, as it uses waitqueue_active()<BR>&nbsp;* internally, some kind of memory barrier must be done prior to calling<BR>&nbsp;* this. Typically, this will be smp_mb__after_atomic(), but in some<BR>&nbsp;* cases where bitflags are manipulated non-atomically under a lock, one<BR>&nbsp;* may need to use a less regular barrier, such fs/inode.c's smp_mb(),<BR>&nbsp;* because spin_unlock() does not guarantee a memory barrier.<BR>&nbsp;*/<BR>void wake_up_bit(void *word, int bit)<BR>{<BR>&nbsp;__wake_up_bit(bit_waitqueue(word, bit), word, bit);<BR>}<BR>EXPORT_SYMBOL(wake_up_bit);</P>
<P>wait_queue_head_t *bit_waitqueue(void *word, int bit)<BR>{<BR>&nbsp;const int shift = BITS_PER_LONG == 32 ? 5 : 6;<BR>&nbsp;const struct zone *zone = page_zone(virt_to_page(word));<BR>&nbsp;unsigned long val = (unsigned long)word &lt;&lt; shift | bit;</P>
<P>&nbsp;return &amp;zone-&gt;wait_table[hash_long(val, zone-&gt;wait_table_bits)];<BR>}<BR>EXPORT_SYMBOL(bit_waitqueue);</P>
<P>/*<BR>&nbsp;* Manipulate the atomic_t address to produce a better bit waitqueue table hash<BR>&nbsp;* index (we're keying off bit -1, but that would produce a horrible hash<BR>&nbsp;* value).<BR>&nbsp;*/<BR>static inline wait_queue_head_t *atomic_t_waitqueue(atomic_t *p)<BR>{<BR>&nbsp;if (BITS_PER_LONG == 64) {<BR>&nbsp;&nbsp;unsigned long q = (unsigned long)p;<BR>&nbsp;&nbsp;return bit_waitqueue((void *)(q &amp; ~1), q &amp; 1);<BR>&nbsp;}<BR>&nbsp;return bit_waitqueue(p, 0);<BR>}</P>
<P>static int wake_atomic_t_function(wait_queue_t *wait, unsigned mode, int sync,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; void *arg)<BR>{<BR>&nbsp;struct wait_bit_key *key = arg;<BR>&nbsp;struct wait_bit_queue *wait_bit<BR>&nbsp;&nbsp;= container_of(wait, struct wait_bit_queue, wait);<BR>&nbsp;atomic_t *val = key-&gt;flags;</P>
<P>&nbsp;if (wait_bit-&gt;key.flags != key-&gt;flags ||<BR>&nbsp;&nbsp;&nbsp;&nbsp; wait_bit-&gt;key.bit_nr != key-&gt;bit_nr ||<BR>&nbsp;&nbsp;&nbsp;&nbsp; atomic_read(val) != 0)<BR>&nbsp;&nbsp;return 0;<BR>&nbsp;return autoremove_wake_function(wait, mode, sync, key);<BR>}</P>
<P>/*<BR>&nbsp;* To allow interruptible waiting and asynchronous (i.e. nonblocking) waiting,<BR>&nbsp;* the actions of __wait_on_atomic_t() are permitted return codes.&nbsp; Nonzero<BR>&nbsp;* return codes halt waiting and return.<BR>&nbsp;*/<BR>static __sched<BR>int __wait_on_atomic_t(wait_queue_head_t *wq, struct wait_bit_queue *q,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; int (*action)(atomic_t *), unsigned mode)<BR>{<BR>&nbsp;atomic_t *val;<BR>&nbsp;int ret = 0;</P>
<P>&nbsp;do {<BR>&nbsp;&nbsp;prepare_to_wait(wq, &amp;q-&gt;wait, mode);<BR>&nbsp;&nbsp;val = q-&gt;key.flags;<BR>&nbsp;&nbsp;if (atomic_read(val) == 0)<BR>&nbsp;&nbsp;&nbsp;break;<BR>&nbsp;&nbsp;ret = (*action)(val);<BR>&nbsp;} while (!ret &amp;&amp; atomic_read(val) != 0);<BR>&nbsp;finish_wait(wq, &amp;q-&gt;wait);<BR>&nbsp;return ret;<BR>}</P>
<P>#define DEFINE_WAIT_ATOMIC_T(name, p)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;struct wait_bit_queue name = {&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;&nbsp;.key = __WAIT_ATOMIC_T_KEY_INITIALIZER(p),&nbsp;&nbsp;\<BR>&nbsp;&nbsp;.wait&nbsp;= {&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;&nbsp;&nbsp;.private&nbsp;= current,&nbsp;&nbsp;&nbsp;\<BR>&nbsp;&nbsp;&nbsp;.func&nbsp;&nbsp;= wake_atomic_t_function,&nbsp;\<BR>&nbsp;&nbsp;&nbsp;.task_list&nbsp;=&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;&nbsp;&nbsp;&nbsp;LIST_HEAD_INIT((name).wait.task_list),&nbsp;\<BR>&nbsp;&nbsp;},&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;}</P>
<P>__sched int out_of_line_wait_on_atomic_t(atomic_t *p, int (*action)(atomic_t *),<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; unsigned mode)<BR>{<BR>&nbsp;wait_queue_head_t *wq = atomic_t_waitqueue(p);<BR>&nbsp;DEFINE_WAIT_ATOMIC_T(wait, p);</P>
<P>&nbsp;return __wait_on_atomic_t(wq, &amp;wait, action, mode);<BR>}<BR>EXPORT_SYMBOL(out_of_line_wait_on_atomic_t);</P>
<P>/**<BR>&nbsp;* wake_up_atomic_t - Wake up a waiter on a atomic_t<BR>&nbsp;* @p: The atomic_t being waited on, a kernel virtual address<BR>&nbsp;*<BR>&nbsp;* Wake up anyone waiting for the atomic_t to go to zero.<BR>&nbsp;*<BR>&nbsp;* Abuse the bit-waker function and its waitqueue hash table set (the atomic_t<BR>&nbsp;* check is done by the waiter's wake function, not the by the waker itself).<BR>&nbsp;*/<BR>void wake_up_atomic_t(atomic_t *p)<BR>{<BR>&nbsp;__wake_up_bit(atomic_t_waitqueue(p), p, WAIT_ATOMIC_T_BIT_NR);<BR>}<BR>EXPORT_SYMBOL(wake_up_atomic_t);</P>
<P>__sched int bit_wait(struct wait_bit_key *word)<BR>{<BR>&nbsp;if (signal_pending_state(current-&gt;state, current))<BR>&nbsp;&nbsp;return 1;<BR>&nbsp;schedule();<BR>&nbsp;return 0;<BR>}<BR>EXPORT_SYMBOL(bit_wait);</P>
<P>__sched int bit_wait_io(struct wait_bit_key *word)<BR>{<BR>&nbsp;if (signal_pending_state(current-&gt;state, current))<BR>&nbsp;&nbsp;return 1;<BR>&nbsp;io_schedule();<BR>&nbsp;return 0;<BR>}<BR>EXPORT_SYMBOL(bit_wait_io);</P>
<P>__sched int bit_wait_timeout(struct wait_bit_key *word)<BR>{<BR>&nbsp;unsigned long now = READ_ONCE(jiffies);<BR>&nbsp;if (signal_pending_state(current-&gt;state, current))<BR>&nbsp;&nbsp;return 1;<BR>&nbsp;if (time_after_eq(now, word-&gt;timeout))<BR>&nbsp;&nbsp;return -EAGAIN;<BR>&nbsp;schedule_timeout(word-&gt;timeout - now);<BR>&nbsp;return 0;<BR>}<BR>EXPORT_SYMBOL_GPL(bit_wait_timeout);</P>
<P>__sched int bit_wait_io_timeout(struct wait_bit_key *word)<BR>{<BR>&nbsp;unsigned long now = READ_ONCE(jiffies);<BR>&nbsp;if (signal_pending_state(current-&gt;state, current))<BR>&nbsp;&nbsp;return 1;<BR>&nbsp;if (time_after_eq(now, word-&gt;timeout))<BR>&nbsp;&nbsp;return -EAGAIN;<BR>&nbsp;io_schedule_timeout(word-&gt;timeout - now);<BR>&nbsp;return 0;<BR>}<BR>EXPORT_SYMBOL_GPL(bit_wait_io_timeout);