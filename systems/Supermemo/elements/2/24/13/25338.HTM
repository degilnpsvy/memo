include/spinlock.h 
<P></P>
<P>#ifndef __LINUX_SPINLOCK_H<BR>#define __LINUX_SPINLOCK_H</P>
<P></P>
<P><FONT class=extract>/*<BR>&nbsp;* include/linux/spinlock.h - generic spinlock/rwlock declarations<BR>&nbsp;*<BR>&nbsp;* here's the role of the various spinlock/rwlock related include files:<BR>&nbsp;*<BR>&nbsp;* on SMP builds:<BR>&nbsp;*<BR>&nbsp;*&nbsp; asm/spinlock_types.h: contains the arch_spinlock_t/arch_rwlock_t and the<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; initializers<BR>&nbsp;*<BR>&nbsp;*&nbsp; linux/spinlock_types.h:<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; defines the generic type and initializers<BR>&nbsp;*<BR>&nbsp;*&nbsp; asm/spinlock.h:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; contains the arch_spin_*()/etc. lowlevel<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; implementations, mostly inline assembly code<BR>&nbsp;*<BR>&nbsp;*&nbsp;&nbsp; (also included on UP-debug builds:)<BR>&nbsp;*<BR>&nbsp;*&nbsp; linux/spinlock_api_smp.h:<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; contains the prototypes for the _spin_*() APIs.<BR>&nbsp;*<BR>&nbsp;*&nbsp; linux/spinlock.h:&nbsp;&nbsp;&nbsp;&nbsp; builds the final spin_*() APIs.<BR>&nbsp;*<BR>&nbsp;* on UP builds:<BR>&nbsp;*<BR>&nbsp;*&nbsp; linux/spinlock_type_up.h:<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; contains the generic, simplified UP spinlock type.<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (which is an empty structure on non-debug builds)<BR>&nbsp;*<BR>&nbsp;*&nbsp; linux/spinlock_types.h:<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; defines the generic type and initializers<BR>&nbsp;*<BR>&nbsp;*&nbsp; linux/spinlock_up.h:<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; contains the arch_spin_*()/etc. version of UP<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; builds. (which are NOPs on non-debug, non-preempt<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; builds)<BR>&nbsp;*<BR>&nbsp;*&nbsp;&nbsp; (included on UP-non-debug builds:)<BR>&nbsp;*<BR>&nbsp;*&nbsp; linux/spinlock_api_up.h:<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; builds the _spin_*() APIs.<BR>&nbsp;*<BR>&nbsp;*&nbsp; linux/spinlock.h:&nbsp;&nbsp;&nbsp;&nbsp; builds the final spin_*() APIs.<BR>&nbsp;*/</FONT></P>
<P>#include &lt;linux/typecheck.h&gt;<BR>#include &lt;linux/preempt.h&gt;<BR>#include &lt;linux/linkage.h&gt;<BR>#include &lt;linux/compiler.h&gt;<BR>#include &lt;linux/irqflags.h&gt;<BR>#include &lt;linux/thread_info.h&gt;<BR>#include &lt;linux/kernel.h&gt;<BR>#include &lt;linux/stringify.h&gt;<BR>#include &lt;linux/bottom_half.h&gt;<BR>#include &lt;asm/barrier.h&gt;</P>
<P><BR>/*<BR>&nbsp;* Must define these before including other files, inline functions need them<BR>&nbsp;*/<BR>#define LOCK_SECTION_NAME ".text..lock."KBUILD_BASENAME</P>
<P>#define LOCK_SECTION_START(extra)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ".subsection 1\n\t"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; extra&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ".ifndef " LOCK_SECTION_NAME "\n\t"&nbsp;&nbsp;&nbsp;&nbsp; \<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; LOCK_SECTION_NAME ":\n\t"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ".endif\n"</P>
<P>#define LOCK_SECTION_END&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ".previous\n\t"</P>
<P>#define __lockfunc __attribute__((section(".spinlock.text")))</P>
<P>/*<BR>&nbsp;* Pull the arch_spinlock_t and arch_rwlock_t definitions:<BR>&nbsp;*/<BR>#include &lt;linux/spinlock_types.h&gt;</P>
<P>/*<BR>&nbsp;* Pull the arch_spin*() functions/declarations (UP-nondebug doesn't need them):<BR>&nbsp;*/<BR>#ifdef CONFIG_SMP<BR># include &lt;asm/spinlock.h&gt;<BR>#else<BR># include &lt;linux/spinlock_up.h&gt;<BR>#endif</P>
<P>#ifdef CONFIG_DEBUG_SPINLOCK<BR>&nbsp; extern void __raw_spin_lock_init(raw_spinlock_t *lock, const char *name,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; struct lock_class_key *key);<BR># define raw_spin_lock_init(lock)&nbsp;&nbsp;&nbsp;&nbsp;\<BR>do {&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;static struct lock_class_key __key;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;__raw_spin_lock_init((lock), #lock, &amp;__key);&nbsp;&nbsp;\<BR>} while (0)</P>
<P>#else<BR># define raw_spin_lock_init(lock)&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;do { *(lock) = __RAW_SPIN_LOCK_UNLOCKED(lock); } while (0)<BR>#endif</P>
<P>#define raw_spin_is_locked(lock)&nbsp;arch_spin_is_locked(&amp;(lock)-&gt;raw_lock)</P>
<P>#ifdef CONFIG_GENERIC_LOCKBREAK<BR>#define raw_spin_is_contended(lock) ((lock)-&gt;break_lock)<BR>#else</P>
<P>#ifdef arch_spin_is_contended<BR>#define raw_spin_is_contended(lock)&nbsp;arch_spin_is_contended(&amp;(lock)-&gt;raw_lock)<BR>#else<BR>#define raw_spin_is_contended(lock)&nbsp;(((void)(lock), 0))<BR>#endif /*arch_spin_is_contended*/<BR>#endif</P>
<P>/*<BR>&nbsp;* Despite its name it doesn't necessarily has to be a full barrier.<BR>&nbsp;* It should only guarantee that a STORE before the critical section<BR>&nbsp;* can not be reordered with LOADs and STOREs inside this section.<BR>&nbsp;* spin_lock() is the one-way barrier, this LOAD can not escape out<BR>&nbsp;* of the region. So the default implementation simply ensures that<BR>&nbsp;* a STORE can not move into the critical section, smp_wmb() should<BR>&nbsp;* serialize it with another STORE done by spin_lock().<BR>&nbsp;*/<BR>#ifndef smp_mb__before_spinlock<BR>#define smp_mb__before_spinlock()&nbsp;smp_wmb()<BR>#endif</P>
<P>/*<BR>&nbsp;* Place this after a lock-acquisition primitive to guarantee that<BR>&nbsp;* an UNLOCK+LOCK pair act as a full barrier.&nbsp; This guarantee applies<BR>&nbsp;* if the UNLOCK and LOCK are executed by the same CPU or if the<BR>&nbsp;* UNLOCK and LOCK operate on the same lock variable.<BR>&nbsp;*/<BR>#ifndef smp_mb__after_unlock_lock<BR>#define smp_mb__after_unlock_lock()&nbsp;do { } while (0)<BR>#endif</P>
<P>/**<BR>&nbsp;* raw_spin_unlock_wait - wait until the spinlock gets unlocked<BR>&nbsp;* @lock: the spinlock in question.<BR>&nbsp;*/<BR>#define raw_spin_unlock_wait(lock)&nbsp;arch_spin_unlock_wait(&amp;(lock)-&gt;raw_lock)</P>
<P>#ifdef CONFIG_DEBUG_SPINLOCK<BR>&nbsp;extern void do_raw_spin_lock(raw_spinlock_t *lock) __acquires(lock);<BR>#define do_raw_spin_lock_flags(lock, flags) do_raw_spin_lock(lock)<BR>&nbsp;extern int do_raw_spin_trylock(raw_spinlock_t *lock);<BR>&nbsp;extern void do_raw_spin_unlock(raw_spinlock_t *lock) __releases(lock);<BR>#else<BR>static inline void do_raw_spin_lock(raw_spinlock_t *lock) __acquires(lock)<BR>{<BR>&nbsp;__acquire(lock);<BR>&nbsp;arch_spin_lock(&amp;lock-&gt;raw_lock);<BR>}</P>
<P>static inline void<BR>do_raw_spin_lock_flags(raw_spinlock_t *lock, unsigned long *flags) __acquires(lock)<BR>{<BR>&nbsp;__acquire(lock);<BR>&nbsp;arch_spin_lock_flags(&amp;lock-&gt;raw_lock, *flags);<BR>}</P>
<P>static inline int do_raw_spin_trylock(raw_spinlock_t *lock)<BR>{<BR>&nbsp;return arch_spin_trylock(&amp;(lock)-&gt;raw_lock);<BR>}</P>
<P>static inline void do_raw_spin_unlock(raw_spinlock_t *lock) __releases(lock)<BR>{<BR>&nbsp;arch_spin_unlock(&amp;lock-&gt;raw_lock);<BR>&nbsp;__release(lock);<BR>}<BR>#endif</P>
<P>/*<BR>&nbsp;* Define the various spin_lock methods.&nbsp; Note we define these<BR>&nbsp;* regardless of whether CONFIG_SMP or CONFIG_PREEMPT are set. The<BR>&nbsp;* various methods are defined as nops in the case they are not<BR>&nbsp;* required.<BR>&nbsp;*/<BR>#define raw_spin_trylock(lock)&nbsp;__cond_lock(lock, _raw_spin_trylock(lock))</P>
<P>#define raw_spin_lock(lock)&nbsp;_raw_spin_lock(lock)</P>
<P>#ifdef CONFIG_DEBUG_LOCK_ALLOC<BR># define raw_spin_lock_nested(lock, subclass) \<BR>&nbsp;_raw_spin_lock_nested(lock, subclass)<BR># define raw_spin_lock_bh_nested(lock, subclass) \<BR>&nbsp;_raw_spin_lock_bh_nested(lock, subclass)</P>
<P># define raw_spin_lock_nest_lock(lock, nest_lock)&nbsp;&nbsp;&nbsp;\<BR>&nbsp; do {&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;&nbsp; typecheck(struct lockdep_map *, &amp;(nest_lock)-&gt;dep_map);\<BR>&nbsp;&nbsp; _raw_spin_lock_nest_lock(lock, &amp;(nest_lock)-&gt;dep_map);&nbsp;\<BR>&nbsp; } while (0)<BR>#else<BR>/*<BR>&nbsp;* Always evaluate the 'subclass' argument to avoid that the compiler<BR>&nbsp;* warns about set-but-not-used variables when building with<BR>&nbsp;* CONFIG_DEBUG_LOCK_ALLOC=n and with W=1.<BR>&nbsp;*/<BR># define raw_spin_lock_nested(lock, subclass)&nbsp;&nbsp;\<BR>&nbsp;_raw_spin_lock(((void)(subclass), (lock)))<BR># define raw_spin_lock_nest_lock(lock, nest_lock)&nbsp;_raw_spin_lock(lock)<BR># define raw_spin_lock_bh_nested(lock, subclass)&nbsp;_raw_spin_lock_bh(lock)<BR>#endif</P>
<P>#if defined(CONFIG_SMP) || defined(CONFIG_DEBUG_SPINLOCK)</P>
<P>#define raw_spin_lock_irqsave(lock, flags)&nbsp;&nbsp;&nbsp;\<BR>&nbsp;do {&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;&nbsp;typecheck(unsigned long, flags);&nbsp;\<BR>&nbsp;&nbsp;flags = _raw_spin_lock_irqsave(lock);&nbsp;\<BR>&nbsp;} while (0)</P>
<P>#ifdef CONFIG_DEBUG_LOCK_ALLOC<BR>#define raw_spin_lock_irqsave_nested(lock, flags, subclass)&nbsp;&nbsp;\<BR>&nbsp;do {&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;&nbsp;typecheck(unsigned long, flags);&nbsp;&nbsp;&nbsp;\<BR>&nbsp;&nbsp;flags = _raw_spin_lock_irqsave_nested(lock, subclass);&nbsp;\<BR>&nbsp;} while (0)<BR>#else<BR>#define raw_spin_lock_irqsave_nested(lock, flags, subclass)&nbsp;&nbsp;\<BR>&nbsp;do {&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;&nbsp;typecheck(unsigned long, flags);&nbsp;&nbsp;&nbsp;\<BR>&nbsp;&nbsp;flags = _raw_spin_lock_irqsave(lock);&nbsp;&nbsp;&nbsp;\<BR>&nbsp;} while (0)<BR>#endif</P>
<P>#else</P>
<P>#define raw_spin_lock_irqsave(lock, flags)&nbsp;&nbsp;\<BR>&nbsp;do {&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;&nbsp;typecheck(unsigned long, flags);&nbsp;\<BR>&nbsp;&nbsp;_raw_spin_lock_irqsave(lock, flags);&nbsp;\<BR>&nbsp;} while (0)</P>
<P>#define raw_spin_lock_irqsave_nested(lock, flags, subclass)&nbsp;\<BR>&nbsp;raw_spin_lock_irqsave(lock, flags)</P>
<P>#endif</P>
<P>#define raw_spin_lock_irq(lock)&nbsp;&nbsp;_raw_spin_lock_irq(lock)<BR>#define raw_spin_lock_bh(lock)&nbsp;&nbsp;_raw_spin_lock_bh(lock)<BR>#define raw_spin_unlock(lock)&nbsp;&nbsp;_raw_spin_unlock(lock)<BR>#define raw_spin_unlock_irq(lock)&nbsp;_raw_spin_unlock_irq(lock)</P>
<P>#define raw_spin_unlock_irqrestore(lock, flags)&nbsp;&nbsp;\<BR>&nbsp;do {&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;&nbsp;typecheck(unsigned long, flags);&nbsp;&nbsp;\<BR>&nbsp;&nbsp;_raw_spin_unlock_irqrestore(lock, flags);&nbsp;\<BR>&nbsp;} while (0)<BR>#define raw_spin_unlock_bh(lock)&nbsp;_raw_spin_unlock_bh(lock)</P>
<P>#define raw_spin_trylock_bh(lock) \<BR>&nbsp;__cond_lock(lock, _raw_spin_trylock_bh(lock))</P>
<P>#define raw_spin_trylock_irq(lock) \<BR>({ \<BR>&nbsp;local_irq_disable(); \<BR>&nbsp;raw_spin_trylock(lock) ? \<BR>&nbsp;1 : ({ local_irq_enable(); 0;&nbsp; }); \<BR>})</P>
<P>#define raw_spin_trylock_irqsave(lock, flags) \<BR>({ \<BR>&nbsp;local_irq_save(flags); \<BR>&nbsp;raw_spin_trylock(lock) ? \<BR>&nbsp;1 : ({ local_irq_restore(flags); 0; }); \<BR>})</P>
<P>/**<BR>&nbsp;* raw_spin_can_lock - would raw_spin_trylock() succeed?<BR>&nbsp;* @lock: the spinlock in question.<BR>&nbsp;*/<BR>#define raw_spin_can_lock(lock)&nbsp;(!raw_spin_is_locked(lock))</P>
<P>/* Include rwlock functions */<BR>#include &lt;linux/rwlock.h&gt;</P>
<P>/*<BR>&nbsp;* Pull the _spin_*()/_read_*()/_write_*() functions/declarations:<BR>&nbsp;*/<BR>#if defined(CONFIG_SMP) || defined(CONFIG_DEBUG_SPINLOCK)<BR># include &lt;linux/spinlock_api_smp.h&gt;<BR>#else<BR># include &lt;linux/spinlock_api_up.h&gt;<BR>#endif</P>
<P>/*<BR>&nbsp;* Map the spin_lock functions to the raw variants for PREEMPT_RT=n<BR>&nbsp;*/</P>
<P>static inline raw_spinlock_t *spinlock_check(spinlock_t *lock)<BR>{<BR>&nbsp;return &amp;lock-&gt;rlock;<BR>}</P>
<P>#define spin_lock_init(_lock)&nbsp;&nbsp;&nbsp;&nbsp;\<BR>do {&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;spinlock_check(_lock);&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;raw_spin_lock_init(&amp;(_lock)-&gt;rlock);&nbsp;&nbsp;\<BR>} while (0)</P>
<P>static inline void spin_lock(spinlock_t *lock)<BR>{<BR>&nbsp;raw_spin_lock(&amp;lock-&gt;rlock);<BR>}</P>
<P>static inline void spin_lock_bh(spinlock_t *lock)<BR>{<BR>&nbsp;raw_spin_lock_bh(&amp;lock-&gt;rlock);<BR>}</P>
<P>static inline int spin_trylock(spinlock_t *lock)<BR>{<BR>&nbsp;return raw_spin_trylock(&amp;lock-&gt;rlock);<BR>}</P>
<P>#define spin_lock_nested(lock, subclass)&nbsp;&nbsp;&nbsp;\<BR>do {&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;raw_spin_lock_nested(spinlock_check(lock), subclass);&nbsp;\<BR>} while (0)</P>
<P>#define spin_lock_bh_nested(lock, subclass)&nbsp;&nbsp;&nbsp;\<BR>do {&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;raw_spin_lock_bh_nested(spinlock_check(lock), subclass);\<BR>} while (0)</P>
<P>#define spin_lock_nest_lock(lock, nest_lock)&nbsp;&nbsp;&nbsp;&nbsp;\<BR>do {&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;raw_spin_lock_nest_lock(spinlock_check(lock), nest_lock);&nbsp;\<BR>} while (0)</P>
<P>static inline void spin_lock_irq(spinlock_t *lock)<BR>{<BR>&nbsp;raw_spin_lock_irq(&amp;lock-&gt;rlock);<BR>}</P>
<P>#define spin_lock_irqsave(lock, flags)&nbsp;&nbsp;&nbsp;&nbsp;\<BR>do {&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;raw_spin_lock_irqsave(spinlock_check(lock), flags);&nbsp;\<BR>} while (0)</P>
<P>#define spin_lock_irqsave_nested(lock, flags, subclass)&nbsp;&nbsp;&nbsp;\<BR>do {&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;raw_spin_lock_irqsave_nested(spinlock_check(lock), flags, subclass); \<BR>} while (0)</P>
<P>static inline void spin_unlock(spinlock_t *lock)<BR>{<BR>&nbsp;raw_spin_unlock(&amp;lock-&gt;rlock);<BR>}</P>
<P>static inline void spin_unlock_bh(spinlock_t *lock)<BR>{<BR>&nbsp;raw_spin_unlock_bh(&amp;lock-&gt;rlock);<BR>}</P>
<P>static inline void spin_unlock_irq(spinlock_t *lock)<BR>{<BR>&nbsp;raw_spin_unlock_irq(&amp;lock-&gt;rlock);<BR>}</P>
<P>static inline void spin_unlock_irqrestore(spinlock_t *lock, unsigned long flags)<BR>{<BR>&nbsp;raw_spin_unlock_irqrestore(&amp;lock-&gt;rlock, flags);<BR>}</P>
<P>static inline int spin_trylock_bh(spinlock_t *lock)<BR>{<BR>&nbsp;return raw_spin_trylock_bh(&amp;lock-&gt;rlock);<BR>}</P>
<P>static inline int spin_trylock_irq(spinlock_t *lock)<BR>{<BR>&nbsp;return raw_spin_trylock_irq(&amp;lock-&gt;rlock);<BR>}</P>
<P>#define spin_trylock_irqsave(lock, flags)&nbsp;&nbsp;&nbsp;\<BR>({&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;raw_spin_trylock_irqsave(spinlock_check(lock), flags); \<BR>})</P>
<P>static inline void spin_unlock_wait(spinlock_t *lock)<BR>{<BR>&nbsp;raw_spin_unlock_wait(&amp;lock-&gt;rlock);<BR>}</P>
<P>static inline int spin_is_locked(spinlock_t *lock)<BR>{<BR>&nbsp;return raw_spin_is_locked(&amp;lock-&gt;rlock);<BR>}</P>
<P>static inline int spin_is_contended(spinlock_t *lock)<BR>{<BR>&nbsp;return raw_spin_is_contended(&amp;lock-&gt;rlock);<BR>}</P>
<P>static inline int spin_can_lock(spinlock_t *lock)<BR>{<BR>&nbsp;return raw_spin_can_lock(&amp;lock-&gt;rlock);<BR>}</P>
<P>#define assert_spin_locked(lock)&nbsp;assert_raw_spin_locked(&amp;(lock)-&gt;rlock)</P>
<P>/*<BR>&nbsp;* Pull the atomic_t declaration:<BR>&nbsp;* (asm-mips/atomic.h needs above definitions)<BR>&nbsp;*/<BR>#include &lt;linux/atomic.h&gt;<BR>/**<BR>&nbsp;* atomic_dec_and_lock - lock on reaching reference count zero<BR>&nbsp;* @atomic: the atomic counter<BR>&nbsp;* @lock: the spinlock in question<BR>&nbsp;*<BR>&nbsp;* Decrements @atomic by 1.&nbsp; If the result is 0, returns true and locks<BR>&nbsp;* @lock.&nbsp; Returns false for all other cases.<BR>&nbsp;*/<BR>extern int _atomic_dec_and_lock(atomic_t *atomic, spinlock_t *lock);<BR>#define atomic_dec_and_lock(atomic, lock) \<BR>&nbsp;&nbsp;__cond_lock(lock, _atomic_dec_and_lock(atomic, lock))</P>
<P>#endif /* __LINUX_SPINLOCK_H */