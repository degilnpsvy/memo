3. The Design 
<P></P>
<P>In order to ease the asynchronous execution of functions a new<BR>abstraction, the work item, is introduced.</P>
<P>A work item is a simple struct that holds a pointer to the function<BR>that is to be executed asynchronously.&nbsp; Whenever a driver or subsystem<BR>wants a function to be executed asynchronously it has to set up a work<BR>item pointing to that function and queue that work item on a<BR>workqueue.</P>
<P>Special purpose threads, called worker threads, execute the functions<BR>off of the queue, one after the other.&nbsp; If no work is queued, the<BR>worker threads become idle.&nbsp; These worker threads are managed in so<BR>called worker-pools.</P>
<P><FONT class=extract>The cmwq design differentiates between the user-facing workqueues that<BR>subsystems and drivers queue work items on and the backend mechanism<BR>which manages worker-pools and processes the queued work items.</FONT></P>
<P><FONT class=extract>There are two worker-pools, one for normal work items and the other<BR>for high priority ones, for each possible CPU and some extra<BR>worker-pools to serve work items queued on unbound workqueues - the<BR>number of these backing pools is dynamic.</FONT></P>
<P>Subsystems and drivers can create and queue work items through special<BR>workqueue API functions as they see fit. They can influence some<BR>aspects of the way the work items are executed by setting flags on the<BR>workqueue they are putting the work item on. These flags include<BR>things like CPU locality, concurrency limits, priority and more.&nbsp; To<BR>get a detailed overview refer to the API description of<BR>alloc_workqueue() below.</P>
<P><FONT class=extract>When a work item is queued to a workqueue, the target worker-pool is<BR>determined according to the queue parameters and workqueue attributes<BR>and appended on the shared worklist of the worker-pool.&nbsp; For example,<BR>unless specifically overridden, a work item of a bound workqueue will<BR>be queued on the worklist of either normal or highpri worker-pool that<BR>is associated to the CPU the issuer is running on.</FONT></P>
<P><FONT class=extract>For any worker pool implementation, managing the concurrency level<BR>(how many execution contexts are active) is an important issue.&nbsp; cmwq<BR>tries to keep the concurrency at a minimal but sufficient level.<BR>Minimal to save resources and sufficient in that the system is used at<BR>its full capacity.</FONT></P>
<P><FONT class=extract>Each worker-pool bound to an actual CPU implements concurrency<BR>management by hooking into the scheduler.&nbsp; The worker-pool is notified<BR>whenever an active worker wakes up or sleeps and keeps track of the<BR>number of the currently runnable workers.&nbsp; Generally, work items are<BR>not expected to hog a CPU and consume many cycles.&nbsp; That means<BR>maintaining just enough concurrency to prevent work processing from<BR>stalling should be optimal.&nbsp; As long as there are one or more runnable<BR>workers on the CPU, the worker-pool doesn't start execution of a new<BR>work, but, when the last running worker goes to sleep, it immediately<BR>schedules a new worker so that the CPU doesn't sit idle while there<BR>are pending work items.&nbsp; This allows using a minimal number of workers<BR>without losing execution bandwidth.</FONT></P>
<P><FONT class=extract>Keeping idle workers around doesn't cost other than the memory space<BR>for kthreads, so cmwq holds onto idle ones for a while before killing<BR>them.</FONT></P>
<P><FONT class=extract>For unbound workqueues, the number of backing pools is dynamic.<BR>Unbound workqueue can be assigned custom attributes using<BR>apply_workqueue_attrs() and workqueue will automatically create<BR>backing worker pools matching the attributes.&nbsp; The responsibility of<BR>regulating concurrency level is on the users.&nbsp; There is also a flag to<BR>mark a bound wq to ignore the concurrency management.&nbsp; Please refer to<BR>the API section for details.</FONT></P>
<P><FONT class=extract>Forward progress guarantee relies on that workers can be created when<BR>more execution contexts are necessary, which in turn is guaranteed<BR>through the use of rescue workers.&nbsp; All work items which might be used<BR>on code paths that handle memory reclaim are required to be queued on<BR>wq's that have a rescue-worker reserved for execution under memory<BR>pressure.&nbsp; Else it is possible that the worker-pool deadlocks waiting<BR>for execution contexts to free up.</FONT>