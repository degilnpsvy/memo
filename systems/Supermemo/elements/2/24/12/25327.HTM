# Documentation/workqueue.txt 
<P></P>
<P>Concurrency Managed Workqueue (cmwq)</P>
<P></P>
<P>September, 2010&nbsp;&nbsp;Tejun Heo &lt;<A href="mailto:tj@kernel.org">tj@kernel.org</A>&gt;<BR>&nbsp;&nbsp;&nbsp;Florian Mickler &lt;<A href="mailto:florian@mickler.org">florian@mickler.org</A>&gt;</P>
<P>CONTENTS</P>
<P>1. Introduction<BR>2. Why cmwq?<BR>3. The Design<BR>4. Application Programming Interface (API)<BR>5. Example Execution Scenarios<BR>6. Guidelines<BR>7. Debugging</P>
<P><BR><FONT class=extract>1. Introduction</FONT></P>
<P><FONT class=extract>There are many cases where an asynchronous process execution context<BR>is needed and the workqueue (wq) API is the most commonly used<BR>mechanism for such cases.</FONT></P>
<P><FONT class=extract>When such an asynchronous execution context is needed, a work item<BR>describing which function to execute is put on a queue.&nbsp; An<BR>independent thread serves as the asynchronous execution context.&nbsp; The<BR>queue is called workqueue and the thread is called worker.</FONT></P>
<P><FONT class=extract>While there are work items on the workqueue the worker executes the<BR>functions associated with the work items one after the other.&nbsp; When<BR>there is no work item left on the workqueue the worker becomes idle.<BR>When a new work item gets queued, the worker begins executing again.</FONT></P>
<P><BR>2. Why cmwq?</P>
<P>In the original wq implementation, a multi threaded (MT) wq had one<BR>worker thread per CPU and a single threaded (ST) wq had one worker<BR>thread system-wide.&nbsp; A single MT wq needed to keep around the same<BR>number of workers as the number of CPUs.&nbsp; The kernel grew a lot of MT<BR>wq users over the years and with the number of CPU cores continuously<BR>rising, some systems saturated the default 32k PID space just booting<BR>up.</P>
<P>Although MT wq wasted a lot of resource, the level of concurrency<BR>provided was unsatisfactory.&nbsp; The limitation was common to both ST and<BR>MT wq albeit less severe on MT.&nbsp; Each wq maintained its own separate<BR>worker pool.&nbsp; A MT wq could provide only one execution context per CPU<BR>while a ST wq one for the whole system.&nbsp; Work items had to compete for<BR>those very limited execution contexts leading to various problems<BR>including proneness to deadlocks around the single execution context.</P>
<P>The tension between the provided level of concurrency and resource<BR>usage also forced its users to make unnecessary tradeoffs like libata<BR>choosing to use ST wq for polling PIOs and accepting an unnecessary<BR>limitation that no two polling PIOs can progress at the same time.&nbsp; As<BR>MT wq don't provide much better concurrency, users which require<BR>higher level of concurrency, like async or fscache, had to implement<BR>their own thread pool.</P>
<P><FONT class=extract>Concurrency Managed Workqueue (cmwq) is a reimplementation of wq with<BR>focus on the following goals.</FONT></P>
<P><FONT class=extract>* Maintain compatibility with the original workqueue API.</FONT></P>
<P><FONT class=extract>* Use per-CPU unified worker pools shared by all wq to provide<BR>&nbsp; flexible level of concurrency on demand without wasting a lot of<BR>&nbsp; resource.</FONT></P>
<P><FONT class=extract>* Automatically regulate worker pool and level of concurrency so that<BR>&nbsp; the API users don't need to worry about such details.</FONT></P>
<P><BR><FONT class=extract>3. The Design</FONT></P>
<P><FONT class=extract>In order to ease the asynchronous execution of functions a new<BR>abstraction, the work item, is introduced.</FONT></P>
<P><FONT class=extract>A work item is a simple struct that holds a pointer to the function<BR>that is to be executed asynchronously.&nbsp; Whenever a driver or subsystem<BR>wants a function to be executed asynchronously it has to set up a work<BR>item pointing to that function and queue that work item on a<BR>workqueue.</FONT></P>
<P><FONT class=extract>Special purpose threads, called worker threads, execute the functions<BR>off of the queue, one after the other.&nbsp; If no work is queued, the<BR>worker threads become idle.&nbsp; These worker threads are managed in so<BR>called worker-pools.</FONT></P>
<P><FONT class=extract>The cmwq design differentiates between the user-facing workqueues that<BR>subsystems and drivers queue work items on and the backend mechanism<BR>which manages worker-pools and processes the queued work items.</FONT></P>
<P><FONT class=extract>There are two worker-pools, one for normal work items and the other<BR>for high priority ones, for each possible CPU and some extra<BR>worker-pools to serve work items queued on unbound workqueues - the<BR>number of these backing pools is dynamic.</FONT></P>
<P><FONT class=extract>Subsystems and drivers can create and queue work items through special<BR>workqueue API functions as they see fit. They can influence some<BR>aspects of the way the work items are executed by setting flags on the<BR>workqueue they are putting the work item on. These flags include<BR>things like CPU locality, concurrency limits, priority and more.&nbsp; To<BR>get a detailed overview refer to the API description of<BR>alloc_workqueue() below.</FONT></P>
<P><FONT class=extract>When a work item is queued to a workqueue, the target worker-pool is<BR>determined according to the queue parameters and workqueue attributes<BR>and appended on the shared worklist of the worker-pool.&nbsp; For example,<BR>unless specifically overridden, a work item of a bound workqueue will<BR>be queued on the worklist of either normal or highpri worker-pool that<BR>is associated to the CPU the issuer is running on.</FONT></P>
<P><FONT class=extract>For any worker pool implementation, managing the concurrency level<BR>(how many execution contexts are active) is an important issue.&nbsp; cmwq<BR>tries to keep the concurrency at a minimal but sufficient level.<BR>Minimal to save resources and sufficient in that the system is used at<BR>its full capacity.</FONT></P>
<P><FONT class=extract>Each worker-pool bound to an actual CPU implements concurrency<BR>management by hooking into the scheduler.&nbsp; The worker-pool is notified<BR>whenever an active worker wakes up or sleeps and keeps track of the<BR>number of the currently runnable workers.&nbsp; Generally, work items are<BR>not expected to hog a CPU and consume many cycles.&nbsp; That means<BR>maintaining just enough concurrency to prevent work processing from<BR>stalling should be optimal.&nbsp; As long as there are one or more runnable<BR>workers on the CPU, the worker-pool doesn't start execution of a new<BR>work, but, when the last running worker goes to sleep, it immediately<BR>schedules a new worker so that the CPU doesn't sit idle while there<BR>are pending work items.&nbsp; This allows using a minimal number of workers<BR>without losing execution bandwidth.</FONT></P>
<P><FONT class=extract>Keeping idle workers around doesn't cost other than the memory space<BR>for kthreads, so cmwq holds onto idle ones for a while before killing<BR>them.</FONT></P>
<P><FONT class=extract>For unbound workqueues, the number of backing pools is dynamic.<BR>Unbound workqueue can be assigned custom attributes using<BR>apply_workqueue_attrs() and workqueue will automatically create<BR>backing worker pools matching the attributes.&nbsp; The responsibility of<BR>regulating concurrency level is on the users.&nbsp; There is also a flag to<BR>mark a bound wq to ignore the concurrency management.&nbsp; Please refer to<BR>the API section for details.</FONT></P>
<P><FONT class=extract>Forward progress guarantee relies on that workers can be created when<BR>more execution contexts are necessary, which in turn is guaranteed<BR>through the use of rescue workers.&nbsp; All work items which might be used<BR>on code paths that handle memory reclaim are required to be queued on<BR>wq's that have a rescue-worker reserved for execution under memory<BR>pressure.&nbsp; Else it is possible that the worker-pool deadlocks waiting<BR>for execution contexts to free up.</FONT></P>
<P><BR><FONT class=extract>4. Application Programming Interface (API)</FONT></P>
<P><FONT class=extract>alloc_workqueue() allocates a wq.&nbsp; The original create_*workqueue()<BR>functions are deprecated and scheduled for removal.&nbsp; alloc_workqueue()<BR>takes three arguments - @name, @flags and @max_active.&nbsp; @name is the<BR>name of the wq and also used as the name of the rescuer thread if<BR>there is one.</FONT></P>
<P><FONT class=extract>A wq no longer manages execution resources but serves as a domain for<BR>forward progress guarantee, flush and work item attributes.&nbsp; @flags<BR>and @max_active control how work items are assigned execution<BR>resources, scheduled and executed.</FONT></P>
<P><FONT class=extract>@flags:</FONT></P>
<P><FONT class=extract>&nbsp; WQ_UNBOUND</FONT></P>
<P><FONT class=extract>&nbsp;Work items queued to an unbound wq are served by the special<BR>&nbsp;woker-pools which host workers which are not bound to any<BR>&nbsp;specific CPU.&nbsp; This makes the wq behave as a simple execution<BR>&nbsp;context provider without concurrency management.&nbsp; The unbound<BR>&nbsp;worker-pools try to start execution of work items as soon as<BR>&nbsp;possible.&nbsp; Unbound wq sacrifices locality but is useful for<BR>&nbsp;the following cases.</FONT></P>
<P><FONT class=extract>&nbsp;* Wide fluctuation in the concurrency level requirement is<BR>&nbsp;&nbsp; expected and using bound wq may end up creating large number<BR>&nbsp;&nbsp; of mostly unused workers across different CPUs as the issuer<BR>&nbsp;&nbsp; hops through different CPUs.</FONT></P>
<P><FONT class=extract>&nbsp;* Long running CPU intensive workloads which can be better<BR>&nbsp;&nbsp; managed by the system scheduler.</FONT></P>
<P><FONT class=extract>&nbsp; WQ_FREEZABLE</FONT></P>
<P><FONT class=extract>&nbsp;A freezable wq participates in the freeze phase of the system<BR>&nbsp;suspend operations.&nbsp; Work items on the wq are drained and no<BR>&nbsp;new work item starts execution until thawed.</FONT></P>
<P><FONT class=extract>&nbsp; WQ_MEM_RECLAIM</FONT></P>
<P><FONT class=extract>&nbsp;All wq which might be used in the memory reclaim paths _MUST_<BR>&nbsp;have this flag set.&nbsp; The wq is guaranteed to have at least one<BR>&nbsp;execution context regardless of memory pressure.</FONT></P>
<P><FONT class=extract>&nbsp; WQ_HIGHPRI</FONT></P>
<P><FONT class=extract>&nbsp;Work items of a highpri wq are queued to the highpri<BR>&nbsp;worker-pool of the target cpu.&nbsp; Highpri worker-pools are<BR>&nbsp;served by worker threads with elevated nice level.</FONT></P>
<P><FONT class=extract>&nbsp;Note that normal and highpri worker-pools don't interact with<BR>&nbsp;each other.&nbsp; Each maintain its separate pool of workers and<BR>&nbsp;implements concurrency management among its workers.</FONT></P>
<P><FONT class=extract>&nbsp; WQ_CPU_INTENSIVE</FONT></P>
<P><FONT class=extract>&nbsp;Work items of a CPU intensive wq do not contribute to the<BR>&nbsp;concurrency level.&nbsp; In other words, runnable CPU intensive<BR>&nbsp;work items will not prevent other work items in the same<BR>&nbsp;worker-pool from starting execution.&nbsp; This is useful for bound<BR>&nbsp;work items which are expected to hog CPU cycles so that their<BR>&nbsp;execution is regulated by the system scheduler.</FONT></P>
<P><FONT class=extract>&nbsp;Although CPU intensive work items don't contribute to the<BR>&nbsp;concurrency level, start of their executions is still<BR>&nbsp;regulated by the concurrency management and runnable<BR>&nbsp;non-CPU-intensive work items can delay execution of CPU<BR>&nbsp;intensive work items.</FONT></P>
<P><FONT class=extract>&nbsp;This flag is meaningless for unbound wq.</FONT></P>
<P><FONT class=extract>Note that the flag WQ_NON_REENTRANT no longer exists as all workqueues<BR>are now non-reentrant - any work item is guaranteed to be executed by<BR>at most one worker system-wide at any given time.</FONT></P>
<P><FONT class=extract>@max_active:</FONT></P>
<P><FONT class=extract>@max_active determines the maximum number of execution contexts per<BR>CPU which can be assigned to the work items of a wq.&nbsp; For example,<BR>with @max_active of 16, at most 16 work items of the wq can be<BR>executing at the same time per CPU.</FONT></P>
<P><FONT class=extract>Currently, for a bound wq, the maximum limit for @max_active is 512<BR>and the default value used when 0 is specified is 256.&nbsp; For an unbound<BR>wq, the limit is higher of 512 and 4 * num_possible_cpus().&nbsp; These<BR>values are chosen sufficiently high such that they are not the<BR>limiting factor while providing protection in runaway cases.</FONT></P>
<P><FONT class=extract>The number of active work items of a wq is usually regulated by the<BR>users of the wq, more specifically, by how many work items the users<BR>may queue at the same time.&nbsp; Unless there is a specific need for<BR>throttling the number of active work items, specifying '0' is<BR>recommended.</FONT></P>
<P><FONT class=extract>Some users depend on the strict execution ordering of ST wq.&nbsp; The<BR>combination of @max_active of 1 and WQ_UNBOUND is used to achieve this<BR>behavior.&nbsp; Work items on such wq are always queued to the unbound<BR>worker-pools and only one work item can be active at any given time thus<BR>achieving the same ordering property as ST wq.</FONT></P>
<P><BR>5. Example Execution Scenarios</P>
<P>The following example execution scenarios try to illustrate how cmwq<BR>behave under different configurations.</P>
<P>&nbsp;Work items w0, w1, w2 are queued to a bound wq q0 on the same CPU.<BR>&nbsp;w0 burns CPU for 5ms then sleeps for 10ms then burns CPU for 5ms<BR>&nbsp;again before finishing.&nbsp; w1 and w2 burn CPU for 5ms then sleep for<BR>&nbsp;10ms.</P>
<P>Ignoring all other tasks, works and processing overhead, and assuming<BR>simple FIFO scheduling, the following is one highly simplified version<BR>of possible sequences of events with the original wq.</P>
<P>&nbsp;TIME IN MSECS&nbsp;EVENT<BR>&nbsp;0&nbsp;&nbsp;w0 starts and burns CPU<BR>&nbsp;5&nbsp;&nbsp;w0 sleeps<BR>&nbsp;15&nbsp;&nbsp;w0 wakes up and burns CPU<BR>&nbsp;20&nbsp;&nbsp;w0 finishes<BR>&nbsp;20&nbsp;&nbsp;w1 starts and burns CPU<BR>&nbsp;25&nbsp;&nbsp;w1 sleeps<BR>&nbsp;35&nbsp;&nbsp;w1 wakes up and finishes<BR>&nbsp;35&nbsp;&nbsp;w2 starts and burns CPU<BR>&nbsp;40&nbsp;&nbsp;w2 sleeps<BR>&nbsp;50&nbsp;&nbsp;w2 wakes up and finishes</P>
<P>And with cmwq with @max_active &gt;= 3,</P>
<P>&nbsp;TIME IN MSECS&nbsp;EVENT<BR>&nbsp;0&nbsp;&nbsp;w0 starts and burns CPU<BR>&nbsp;5&nbsp;&nbsp;w0 sleeps<BR>&nbsp;5&nbsp;&nbsp;w1 starts and burns CPU<BR>&nbsp;10&nbsp;&nbsp;w1 sleeps<BR>&nbsp;10&nbsp;&nbsp;w2 starts and burns CPU<BR>&nbsp;15&nbsp;&nbsp;w2 sleeps<BR>&nbsp;15&nbsp;&nbsp;w0 wakes up and burns CPU<BR>&nbsp;20&nbsp;&nbsp;w0 finishes<BR>&nbsp;20&nbsp;&nbsp;w1 wakes up and finishes<BR>&nbsp;25&nbsp;&nbsp;w2 wakes up and finishes</P>
<P>If @max_active == 2,</P>
<P>&nbsp;TIME IN MSECS&nbsp;EVENT<BR>&nbsp;0&nbsp;&nbsp;w0 starts and burns CPU<BR>&nbsp;5&nbsp;&nbsp;w0 sleeps<BR>&nbsp;5&nbsp;&nbsp;w1 starts and burns CPU<BR>&nbsp;10&nbsp;&nbsp;w1 sleeps<BR>&nbsp;15&nbsp;&nbsp;w0 wakes up and burns CPU<BR>&nbsp;20&nbsp;&nbsp;w0 finishes<BR>&nbsp;20&nbsp;&nbsp;w1 wakes up and finishes<BR>&nbsp;20&nbsp;&nbsp;w2 starts and burns CPU<BR>&nbsp;25&nbsp;&nbsp;w2 sleeps<BR>&nbsp;35&nbsp;&nbsp;w2 wakes up and finishes</P>
<P>Now, let's assume w1 and w2 are queued to a different wq q1 which has<BR>WQ_CPU_INTENSIVE set,</P>
<P>&nbsp;TIME IN MSECS&nbsp;EVENT<BR>&nbsp;0&nbsp;&nbsp;w0 starts and burns CPU<BR>&nbsp;5&nbsp;&nbsp;w0 sleeps<BR>&nbsp;5&nbsp;&nbsp;w1 and w2 start and burn CPU<BR>&nbsp;10&nbsp;&nbsp;w1 sleeps<BR>&nbsp;15&nbsp;&nbsp;w2 sleeps<BR>&nbsp;15&nbsp;&nbsp;w0 wakes up and burns CPU<BR>&nbsp;20&nbsp;&nbsp;w0 finishes<BR>&nbsp;20&nbsp;&nbsp;w1 wakes up and finishes<BR>&nbsp;25&nbsp;&nbsp;w2 wakes up and finishes</P>
<P><BR>6. Guidelines</P>
<P>* Do not forget to use WQ_MEM_RECLAIM if a wq may process work items<BR>&nbsp; which are used during memory reclaim.&nbsp; Each wq with WQ_MEM_RECLAIM<BR>&nbsp; set has an execution context reserved for it.&nbsp; If there is<BR>&nbsp; dependency among multiple work items used during memory reclaim,<BR>&nbsp; they should be queued to separate wq each with WQ_MEM_RECLAIM.</P>
<P>* Unless strict ordering is required, there is no need to use ST wq.</P>
<P>* Unless there is a specific need, using 0 for @max_active is<BR>&nbsp; recommended.&nbsp; In most use cases, concurrency level usually stays<BR>&nbsp; well under the default limit.</P>
<P>* A wq serves as a domain for forward progress guarantee<BR>&nbsp; (WQ_MEM_RECLAIM, flush and work item attributes.&nbsp; Work items which<BR>&nbsp; are not involved in memory reclaim and don't need to be flushed as a<BR>&nbsp; part of a group of work items, and don't require any special<BR>&nbsp; attribute, can use one of the system wq.&nbsp; There is no difference in<BR>&nbsp; execution characteristics between using a dedicated wq and a system<BR>&nbsp; wq.</P>
<P>* Unless work items are expected to consume a huge amount of CPU<BR>&nbsp; cycles, using a bound wq is usually beneficial due to the increased<BR>&nbsp; level of locality in wq operations and work item execution.</P>
<P><BR>7. Debugging</P>
<P>Because the work functions are executed by generic worker threads<BR>there are a few tricks needed to shed some light on misbehaving<BR>workqueue users.</P>
<P>Worker threads show up in the process list as:</P>
<P>root&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5671&nbsp; 0.0&nbsp; 0.0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp;&nbsp; 0 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; S&nbsp;&nbsp;&nbsp; 12:07&nbsp;&nbsp; 0:00 [kworker/0:1]<BR>root&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5672&nbsp; 0.0&nbsp; 0.0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp;&nbsp; 0 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; S&nbsp;&nbsp;&nbsp; 12:07&nbsp;&nbsp; 0:00 [kworker/1:2]<BR>root&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5673&nbsp; 0.0&nbsp; 0.0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp;&nbsp; 0 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; S&nbsp;&nbsp;&nbsp; 12:12&nbsp;&nbsp; 0:00 [kworker/0:0]<BR>root&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5674&nbsp; 0.0&nbsp; 0.0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp;&nbsp; 0 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; S&nbsp;&nbsp;&nbsp; 12:13&nbsp;&nbsp; 0:00 [kworker/1:0]</P>
<P>If kworkers are going crazy (using too much cpu), there are two types<BR>of possible problems:</P>
<P>&nbsp;1. Something being scheduled in rapid succession<BR>&nbsp;2. A single work item that consumes lots of cpu cycles</P>
<P>The first one can be tracked using tracing:</P>
<P>&nbsp;$ echo workqueue:workqueue_queue_work &gt; /sys/kernel/debug/tracing/set_event<BR>&nbsp;$ cat /sys/kernel/debug/tracing/trace_pipe &gt; out.txt<BR>&nbsp;(wait a few secs)<BR>&nbsp;^C</P>
<P>If something is busy looping on work queueing, it would be dominating<BR>the output and the offender can be determined with the work item<BR>function.</P>
<P>For the second type of problems it should be possible to just check<BR>the stack trace of the offending worker thread.</P>
<P>&nbsp;$ cat /proc/THE_OFFENDING_KWORKER/stack</P>
<P>The work item's function should be trivially visible in the stack<BR>trace.