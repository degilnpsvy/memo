include/linux/seqlock.h 
<P></P>
<P>#ifndef __LINUX_SEQLOCK_H<BR>#define __LINUX_SEQLOCK_H<BR><FONT class=extract>/*<BR>&nbsp;* Reader/writer consistent mechanism without starving writers. This type of<BR>&nbsp;* lock for data where the reader wants a consistent set of information<BR>&nbsp;* and is willing to retry if the information changes. There are two types<BR>&nbsp;* of readers:<BR>&nbsp;* 1. Sequence readers which never block a writer but they may have to retry<BR>&nbsp;*&nbsp;&nbsp;&nbsp; if a writer is in progress by detecting change in sequence number.<BR>&nbsp;*&nbsp;&nbsp;&nbsp; Writers do not wait for a sequence reader.<BR>&nbsp;* 2. Locking readers which will wait if a writer or another locking reader<BR>&nbsp;*&nbsp;&nbsp;&nbsp; is in progress. A locking reader in progress will also block a writer<BR>&nbsp;*&nbsp;&nbsp;&nbsp; from going forward. Unlike the regular rwlock, the read lock here is<BR>&nbsp;*&nbsp;&nbsp;&nbsp; exclusive so that only one locking reader can get it.<BR>&nbsp;*<BR>&nbsp;* This is not as cache friendly as brlock. Also, this may not work well<BR>&nbsp;* for data that contains pointers, because any writer could<BR>&nbsp;* invalidate a pointer that a reader was following.<BR>&nbsp;*<BR>&nbsp;* Expected non-blocking reader usage:<BR>&nbsp;* &nbsp;do {<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp; seq = read_seqbegin(&amp;foo);<BR>&nbsp;* &nbsp;...<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; } while (read_seqretry(&amp;foo, seq));<BR>&nbsp;*<BR>&nbsp;*<BR>&nbsp;* On non-SMP the spin locks disappear but the writer still needs<BR>&nbsp;* to increment the sequence variables because an interrupt routine could<BR>&nbsp;* change the state of the data.<BR>&nbsp;*<BR>&nbsp;* Based on x86_64 vsyscall gettimeofday <BR>&nbsp;* by Keith Owens and Andrea Arcangeli<BR>&nbsp;*/</FONT></P>
<P></P>
<P>#include &lt;linux/spinlock.h&gt;<BR>#include &lt;linux/preempt.h&gt;<BR>#include &lt;linux/lockdep.h&gt;<BR>#include &lt;linux/compiler.h&gt;<BR>#include &lt;asm/processor.h&gt;</P>
<P>/*<BR>&nbsp;* Version using sequence counter only.<BR>&nbsp;* This can be used when code has its own mutex protecting the<BR>&nbsp;* updating starting before the write_seqcountbeqin() and ending<BR>&nbsp;* after the write_seqcount_end().<BR>&nbsp;*/<BR>typedef struct seqcount {<BR>&nbsp;unsigned sequence;<BR>#ifdef CONFIG_DEBUG_LOCK_ALLOC<BR>&nbsp;struct lockdep_map dep_map;<BR>#endif<BR>} seqcount_t;</P>
<P>static inline void __seqcount_init(seqcount_t *s, const char *name,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; struct lock_class_key *key)<BR>{<BR>&nbsp;/*<BR>&nbsp; * Make sure we are not reinitializing a held lock:<BR>&nbsp; */<BR>&nbsp;lockdep_init_map(&amp;s-&gt;dep_map, name, key, 0);<BR>&nbsp;s-&gt;sequence = 0;<BR>}</P>
<P>#ifdef CONFIG_DEBUG_LOCK_ALLOC<BR># define SEQCOUNT_DEP_MAP_INIT(lockname) \<BR>&nbsp;&nbsp;.dep_map = { .name = #lockname } \</P>
<P># define seqcount_init(s)&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;do {&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;&nbsp;static struct lock_class_key __key;&nbsp;\<BR>&nbsp;&nbsp;__seqcount_init((s), #s, &amp;__key);&nbsp;\<BR>&nbsp;} while (0)</P>
<P>static inline void seqcount_lockdep_reader_access(const seqcount_t *s)<BR>{<BR>&nbsp;seqcount_t *l = (seqcount_t *)s;<BR>&nbsp;unsigned long flags;</P>
<P>&nbsp;local_irq_save(flags);<BR>&nbsp;seqcount_acquire_read(&amp;l-&gt;dep_map, 0, 0, _RET_IP_);<BR>&nbsp;seqcount_release(&amp;l-&gt;dep_map, 1, _RET_IP_);<BR>&nbsp;local_irq_restore(flags);<BR>}</P>
<P>#else<BR># define SEQCOUNT_DEP_MAP_INIT(lockname)<BR># define seqcount_init(s) __seqcount_init(s, NULL, NULL)<BR># define seqcount_lockdep_reader_access(x)<BR>#endif</P>
<P>#define SEQCNT_ZERO(lockname) { .sequence = 0, SEQCOUNT_DEP_MAP_INIT(lockname)}</P>
<P><BR>/**<BR>&nbsp;* __read_seqcount_begin - begin a seq-read critical section (without barrier)<BR>&nbsp;* @s: pointer to seqcount_t<BR>&nbsp;* Returns: count to be passed to read_seqcount_retry<BR>&nbsp;*<BR>&nbsp;* __read_seqcount_begin is like read_seqcount_begin, but has no smp_rmb()<BR>&nbsp;* barrier. Callers should ensure that smp_rmb() or equivalent ordering is<BR>&nbsp;* provided before actually loading any of the variables that are to be<BR>&nbsp;* protected in this critical section.<BR>&nbsp;*<BR>&nbsp;* Use carefully, only in critical code, and comment how the barrier is<BR>&nbsp;* provided.<BR>&nbsp;*/<BR>static inline unsigned __read_seqcount_begin(const seqcount_t *s)<BR>{<BR>&nbsp;unsigned ret;</P>
<P>repeat:<BR>&nbsp;ret = READ_ONCE(s-&gt;sequence);<BR>&nbsp;if (unlikely(ret &amp; 1)) {<BR>&nbsp;&nbsp;cpu_relax();<BR>&nbsp;&nbsp;goto repeat;<BR>&nbsp;}<BR>&nbsp;return ret;<BR>}</P>
<P>/**<BR>&nbsp;* raw_read_seqcount - Read the raw seqcount<BR>&nbsp;* @s: pointer to seqcount_t<BR>&nbsp;* Returns: count to be passed to read_seqcount_retry<BR>&nbsp;*<BR>&nbsp;* raw_read_seqcount opens a read critical section of the given<BR>&nbsp;* seqcount without any lockdep checking and without checking or<BR>&nbsp;* masking the LSB. Calling code is responsible for handling that.<BR>&nbsp;*/<BR>static inline unsigned raw_read_seqcount(const seqcount_t *s)<BR>{<BR>&nbsp;unsigned ret = READ_ONCE(s-&gt;sequence);<BR>&nbsp;smp_rmb();<BR>&nbsp;return ret;<BR>}</P>
<P>/**<BR>&nbsp;* raw_read_seqcount_begin - start seq-read critical section w/o lockdep<BR>&nbsp;* @s: pointer to seqcount_t<BR>&nbsp;* Returns: count to be passed to read_seqcount_retry<BR>&nbsp;*<BR>&nbsp;* raw_read_seqcount_begin opens a read critical section of the given<BR>&nbsp;* seqcount, but without any lockdep checking. Validity of the critical<BR>&nbsp;* section is tested by checking read_seqcount_retry function.<BR>&nbsp;*/<BR>static inline unsigned raw_read_seqcount_begin(const seqcount_t *s)<BR>{<BR>&nbsp;unsigned ret = __read_seqcount_begin(s);<BR>&nbsp;smp_rmb();<BR>&nbsp;return ret;<BR>}</P>
<P>/**<BR>&nbsp;* read_seqcount_begin - begin a seq-read critical section<BR>&nbsp;* @s: pointer to seqcount_t<BR>&nbsp;* Returns: count to be passed to read_seqcount_retry<BR>&nbsp;*<BR>&nbsp;* read_seqcount_begin opens a read critical section of the given seqcount.<BR>&nbsp;* Validity of the critical section is tested by checking read_seqcount_retry<BR>&nbsp;* function.<BR>&nbsp;*/<BR>static inline unsigned read_seqcount_begin(const seqcount_t *s)<BR>{<BR>&nbsp;seqcount_lockdep_reader_access(s);<BR>&nbsp;return raw_read_seqcount_begin(s);<BR>}</P>
<P>/**<BR>&nbsp;* raw_seqcount_begin - begin a seq-read critical section<BR>&nbsp;* @s: pointer to seqcount_t<BR>&nbsp;* Returns: count to be passed to read_seqcount_retry<BR>&nbsp;*<BR>&nbsp;* raw_seqcount_begin opens a read critical section of the given seqcount.<BR>&nbsp;* Validity of the critical section is tested by checking read_seqcount_retry<BR>&nbsp;* function.<BR>&nbsp;*<BR>&nbsp;* Unlike read_seqcount_begin(), this function will not wait for the count<BR>&nbsp;* to stabilize. If a writer is active when we begin, we will fail the<BR>&nbsp;* read_seqcount_retry() instead of stabilizing at the beginning of the<BR>&nbsp;* critical section.<BR>&nbsp;*/<BR>static inline unsigned raw_seqcount_begin(const seqcount_t *s)<BR>{<BR>&nbsp;unsigned ret = READ_ONCE(s-&gt;sequence);<BR>&nbsp;smp_rmb();<BR>&nbsp;return ret &amp; ~1;<BR>}</P>
<P>/**<BR>&nbsp;* __read_seqcount_retry - end a seq-read critical section (without barrier)<BR>&nbsp;* @s: pointer to seqcount_t<BR>&nbsp;* @start: count, from read_seqcount_begin<BR>&nbsp;* Returns: 1 if retry is required, else 0<BR>&nbsp;*<BR>&nbsp;* __read_seqcount_retry is like read_seqcount_retry, but has no smp_rmb()<BR>&nbsp;* barrier. Callers should ensure that smp_rmb() or equivalent ordering is<BR>&nbsp;* provided before actually loading any of the variables that are to be<BR>&nbsp;* protected in this critical section.<BR>&nbsp;*<BR>&nbsp;* Use carefully, only in critical code, and comment how the barrier is<BR>&nbsp;* provided.<BR>&nbsp;*/<BR>static inline int __read_seqcount_retry(const seqcount_t *s, unsigned start)<BR>{<BR>&nbsp;return unlikely(s-&gt;sequence != start);<BR>}</P>
<P>/**<BR>&nbsp;* read_seqcount_retry - end a seq-read critical section<BR>&nbsp;* @s: pointer to seqcount_t<BR>&nbsp;* @start: count, from read_seqcount_begin<BR>&nbsp;* Returns: 1 if retry is required, else 0<BR>&nbsp;*<BR>&nbsp;* read_seqcount_retry closes a read critical section of the given seqcount.<BR>&nbsp;* If the critical section was invalid, it must be ignored (and typically<BR>&nbsp;* retried).<BR>&nbsp;*/<BR>static inline int read_seqcount_retry(const seqcount_t *s, unsigned start)<BR>{<BR>&nbsp;smp_rmb();<BR>&nbsp;return __read_seqcount_retry(s, start);<BR>}</P>
<P>&nbsp;</P>
<P>static inline void raw_write_seqcount_begin(seqcount_t *s)<BR>{<BR>&nbsp;s-&gt;sequence++;<BR>&nbsp;smp_wmb();<BR>}</P>
<P>static inline void raw_write_seqcount_end(seqcount_t *s)<BR>{<BR>&nbsp;smp_wmb();<BR>&nbsp;s-&gt;sequence++;<BR>}</P>
<P>/**<BR>&nbsp;* raw_write_seqcount_barrier - do a seq write barrier<BR>&nbsp;* @s: pointer to seqcount_t<BR>&nbsp;*<BR>&nbsp;* This can be used to provide an ordering guarantee instead of the<BR>&nbsp;* usual consistency guarantee. It is one wmb cheaper, because we can<BR>&nbsp;* collapse the two back-to-back wmb()s.<BR>&nbsp;*<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; seqcount_t seq;<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; bool X = true, Y = false;<BR>&nbsp;*<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; void read(void)<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; {<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; bool x, y;<BR>&nbsp;*<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; do {<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; int s = read_seqcount_begin(&amp;seq);<BR>&nbsp;*<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x = X; y = Y;<BR>&nbsp;*<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; } while (read_seqcount_retry(&amp;seq, s));<BR>&nbsp;*<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; BUG_ON(!x &amp;&amp; !y);<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<BR>&nbsp;*<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; void write(void)<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; {<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Y = true;<BR>&nbsp;*<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; raw_write_seqcount_barrier(seq);<BR>&nbsp;*<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; X = false;<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<BR>&nbsp;*/<BR>static inline void raw_write_seqcount_barrier(seqcount_t *s)<BR>{<BR>&nbsp;s-&gt;sequence++;<BR>&nbsp;smp_wmb();<BR>&nbsp;s-&gt;sequence++;<BR>}</P>
<P>static inline int raw_read_seqcount_latch(seqcount_t *s)<BR>{<BR>&nbsp;return lockless_dereference(s-&gt;sequence);<BR>}</P>
<P>/**<BR>&nbsp;* raw_write_seqcount_latch - redirect readers to even/odd copy<BR>&nbsp;* @s: pointer to seqcount_t<BR>&nbsp;*<BR>&nbsp;* The latch technique is a multiversion concurrency control method that allows<BR>&nbsp;* queries during non-atomic modifications. If you can guarantee queries never<BR>&nbsp;* interrupt the modification -- e.g. the concurrency is strictly between CPUs<BR>&nbsp;* -- you most likely do not need this.<BR>&nbsp;*<BR>&nbsp;* Where the traditional RCU/lockless data structures rely on atomic<BR>&nbsp;* modifications to ensure queries observe either the old or the new state the<BR>&nbsp;* latch allows the same for non-atomic updates. The trade-off is doubling the<BR>&nbsp;* cost of storage; we have to maintain two copies of the entire data<BR>&nbsp;* structure.<BR>&nbsp;*<BR>&nbsp;* Very simply put: we first modify one copy and then the other. This ensures<BR>&nbsp;* there is always one copy in a stable state, ready to give us an answer.<BR>&nbsp;*<BR>&nbsp;* The basic form is a data structure like:<BR>&nbsp;*<BR>&nbsp;* struct latch_struct {<BR>&nbsp;*&nbsp;seqcount_t&nbsp;&nbsp;seq;<BR>&nbsp;*&nbsp;struct data_struct&nbsp;data[2];<BR>&nbsp;* };<BR>&nbsp;*<BR>&nbsp;* Where a modification, which is assumed to be externally serialized, does the<BR>&nbsp;* following:<BR>&nbsp;*<BR>&nbsp;* void latch_modify(struct latch_struct *latch, ...)<BR>&nbsp;* {<BR>&nbsp;*&nbsp;smp_wmb();&nbsp;&lt;- Ensure that the last data[1] update is visible<BR>&nbsp;*&nbsp;latch-&gt;seq++;<BR>&nbsp;*&nbsp;smp_wmb();&nbsp;&lt;- Ensure that the seqcount update is visible<BR>&nbsp;*<BR>&nbsp;*&nbsp;modify(latch-&gt;data[0], ...);<BR>&nbsp;*<BR>&nbsp;*&nbsp;smp_wmb();&nbsp;&lt;- Ensure that the data[0] update is visible<BR>&nbsp;*&nbsp;latch-&gt;seq++;<BR>&nbsp;*&nbsp;smp_wmb();&nbsp;&lt;- Ensure that the seqcount update is visible<BR>&nbsp;*<BR>&nbsp;*&nbsp;modify(latch-&gt;data[1], ...);<BR>&nbsp;* }<BR>&nbsp;*<BR>&nbsp;* The query will have a form like:<BR>&nbsp;*<BR>&nbsp;* struct entry *latch_query(struct latch_struct *latch, ...)<BR>&nbsp;* {<BR>&nbsp;*&nbsp;struct entry *entry;<BR>&nbsp;*&nbsp;unsigned seq, idx;<BR>&nbsp;*<BR>&nbsp;*&nbsp;do {<BR>&nbsp;*&nbsp;&nbsp;seq = lockless_dereference(latch-&gt;seq);<BR>&nbsp;*<BR>&nbsp;*&nbsp;&nbsp;idx = seq &amp; 0x01;<BR>&nbsp;*&nbsp;&nbsp;entry = data_query(latch-&gt;data[idx], ...);<BR>&nbsp;*<BR>&nbsp;*&nbsp;&nbsp;smp_rmb();<BR>&nbsp;*&nbsp;} while (seq != latch-&gt;seq);<BR>&nbsp;*<BR>&nbsp;*&nbsp;return entry;<BR>&nbsp;* }<BR>&nbsp;*<BR>&nbsp;* So during the modification, queries are first redirected to data[1]. Then we<BR>&nbsp;* modify data[0]. When that is complete, we redirect queries back to data[0]<BR>&nbsp;* and we can modify data[1].<BR>&nbsp;*<BR>&nbsp;* NOTE: The non-requirement for atomic modifications does _NOT_ include<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; the publishing of new entries in the case where data is a dynamic<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; data structure.<BR>&nbsp;*<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; An iteration might start in data[0] and get suspended long enough<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; to miss an entire modification sequence, once it resumes it might<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; observe the new entry.<BR>&nbsp;*<BR>&nbsp;* NOTE: When data is a dynamic data structure; one should use regular RCU<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; patterns to manage the lifetimes of the objects within.<BR>&nbsp;*/<BR>static inline void raw_write_seqcount_latch(seqcount_t *s)<BR>{<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; smp_wmb();&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; /* prior stores before incrementing "sequence" */<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; s-&gt;sequence++;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; smp_wmb();&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; /* increment "sequence" before following stores */<BR>}</P>
<P>/*<BR>&nbsp;* Sequence counter only version assumes that callers are using their<BR>&nbsp;* own mutexing.<BR>&nbsp;*/<BR>static inline void write_seqcount_begin_nested(seqcount_t *s, int subclass)<BR>{<BR>&nbsp;raw_write_seqcount_begin(s);<BR>&nbsp;seqcount_acquire(&amp;s-&gt;dep_map, subclass, 0, _RET_IP_);<BR>}</P>
<P>static inline void write_seqcount_begin(seqcount_t *s)<BR>{<BR>&nbsp;write_seqcount_begin_nested(s, 0);<BR>}</P>
<P>static inline void write_seqcount_end(seqcount_t *s)<BR>{<BR>&nbsp;seqcount_release(&amp;s-&gt;dep_map, 1, _RET_IP_);<BR>&nbsp;raw_write_seqcount_end(s);<BR>}</P>
<P>/**<BR>&nbsp;* write_seqcount_invalidate - invalidate in-progress read-side seq operations<BR>&nbsp;* @s: pointer to seqcount_t<BR>&nbsp;*<BR>&nbsp;* After write_seqcount_invalidate, no read-side seq operations will complete<BR>&nbsp;* successfully and see data older than this.<BR>&nbsp;*/<BR>static inline void write_seqcount_invalidate(seqcount_t *s)<BR>{<BR>&nbsp;smp_wmb();<BR>&nbsp;s-&gt;sequence+=2;<BR>}</P>
<P>typedef struct {<BR>&nbsp;struct seqcount seqcount;<BR>&nbsp;spinlock_t lock;<BR>} seqlock_t;</P>
<P>/*<BR>&nbsp;* These macros triggered gcc-3.x compile-time problems.&nbsp; We think these are<BR>&nbsp;* OK now.&nbsp; Be cautious.<BR>&nbsp;*/<BR>#define __SEQLOCK_UNLOCKED(lockname)&nbsp;&nbsp;&nbsp;\<BR>&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;&nbsp;.seqcount = SEQCNT_ZERO(lockname),&nbsp;\<BR>&nbsp;&nbsp;.lock =&nbsp;__SPIN_LOCK_UNLOCKED(lockname)&nbsp;\<BR>&nbsp;}</P>
<P>#define seqlock_init(x)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;do {&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;&nbsp;seqcount_init(&amp;(x)-&gt;seqcount);&nbsp;&nbsp;\<BR>&nbsp;&nbsp;spin_lock_init(&amp;(x)-&gt;lock);&nbsp;&nbsp;\<BR>&nbsp;} while (0)</P>
<P>#define DEFINE_SEQLOCK(x) \<BR>&nbsp;&nbsp;seqlock_t x = __SEQLOCK_UNLOCKED(x)</P>
<P>/*<BR>&nbsp;* Read side functions for starting and finalizing a read side section.<BR>&nbsp;*/<BR>static inline unsigned read_seqbegin(const seqlock_t *sl)<BR>{<BR>&nbsp;return read_seqcount_begin(&amp;sl-&gt;seqcount);<BR>}</P>
<P>static inline unsigned read_seqretry(const seqlock_t *sl, unsigned start)<BR>{<BR>&nbsp;return read_seqcount_retry(&amp;sl-&gt;seqcount, start);<BR>}</P>
<P>/*<BR>&nbsp;* Lock out other writers and update the count.<BR>&nbsp;* Acts like a normal spin_lock/unlock.<BR>&nbsp;* Don't need preempt_disable() because that is in the spin_lock already.<BR>&nbsp;*/<BR>static inline void write_seqlock(seqlock_t *sl)<BR>{<BR>&nbsp;spin_lock(&amp;sl-&gt;lock);<BR>&nbsp;write_seqcount_begin(&amp;sl-&gt;seqcount);<BR>}</P>
<P>static inline void write_sequnlock(seqlock_t *sl)<BR>{<BR>&nbsp;write_seqcount_end(&amp;sl-&gt;seqcount);<BR>&nbsp;spin_unlock(&amp;sl-&gt;lock);<BR>}</P>
<P>static inline void write_seqlock_bh(seqlock_t *sl)<BR>{<BR>&nbsp;spin_lock_bh(&amp;sl-&gt;lock);<BR>&nbsp;write_seqcount_begin(&amp;sl-&gt;seqcount);<BR>}</P>
<P>static inline void write_sequnlock_bh(seqlock_t *sl)<BR>{<BR>&nbsp;write_seqcount_end(&amp;sl-&gt;seqcount);<BR>&nbsp;spin_unlock_bh(&amp;sl-&gt;lock);<BR>}</P>
<P>static inline void write_seqlock_irq(seqlock_t *sl)<BR>{<BR>&nbsp;spin_lock_irq(&amp;sl-&gt;lock);<BR>&nbsp;write_seqcount_begin(&amp;sl-&gt;seqcount);<BR>}</P>
<P>static inline void write_sequnlock_irq(seqlock_t *sl)<BR>{<BR>&nbsp;write_seqcount_end(&amp;sl-&gt;seqcount);<BR>&nbsp;spin_unlock_irq(&amp;sl-&gt;lock);<BR>}</P>
<P>static inline unsigned long __write_seqlock_irqsave(seqlock_t *sl)<BR>{<BR>&nbsp;unsigned long flags;</P>
<P>&nbsp;spin_lock_irqsave(&amp;sl-&gt;lock, flags);<BR>&nbsp;write_seqcount_begin(&amp;sl-&gt;seqcount);<BR>&nbsp;return flags;<BR>}</P>
<P>#define write_seqlock_irqsave(lock, flags)&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;do { flags = __write_seqlock_irqsave(lock); } while (0)</P>
<P>static inline void<BR>write_sequnlock_irqrestore(seqlock_t *sl, unsigned long flags)<BR>{<BR>&nbsp;write_seqcount_end(&amp;sl-&gt;seqcount);<BR>&nbsp;spin_unlock_irqrestore(&amp;sl-&gt;lock, flags);<BR>}</P>
<P>/*<BR>&nbsp;* A locking reader exclusively locks out other writers and locking readers,<BR>&nbsp;* but doesn't update the sequence number. Acts like a normal spin_lock/unlock.<BR>&nbsp;* Don't need preempt_disable() because that is in the spin_lock already.<BR>&nbsp;*/<BR>static inline void read_seqlock_excl(seqlock_t *sl)<BR>{<BR>&nbsp;spin_lock(&amp;sl-&gt;lock);<BR>}</P>
<P>static inline void read_sequnlock_excl(seqlock_t *sl)<BR>{<BR>&nbsp;spin_unlock(&amp;sl-&gt;lock);<BR>}</P>
<P>/**<BR>&nbsp;* read_seqbegin_or_lock - begin a sequence number check or locking block<BR>&nbsp;* @lock: sequence lock<BR>&nbsp;* @seq : sequence number to be checked<BR>&nbsp;*<BR>&nbsp;* First try it once optimistically without taking the lock. If that fails,<BR>&nbsp;* take the lock. The sequence number is also used as a marker for deciding<BR>&nbsp;* whether to be a reader (even) or writer (odd).<BR>&nbsp;* N.B. seq must be initialized to an even number to begin with.<BR>&nbsp;*/<BR>static inline void read_seqbegin_or_lock(seqlock_t *lock, int *seq)<BR>{<BR>&nbsp;if (!(*seq &amp; 1))&nbsp;/* Even */<BR>&nbsp;&nbsp;*seq = read_seqbegin(lock);<BR>&nbsp;else&nbsp;&nbsp;&nbsp;/* Odd */<BR>&nbsp;&nbsp;read_seqlock_excl(lock);<BR>}</P>
<P>static inline int need_seqretry(seqlock_t *lock, int seq)<BR>{<BR>&nbsp;return !(seq &amp; 1) &amp;&amp; read_seqretry(lock, seq);<BR>}</P>
<P>static inline void done_seqretry(seqlock_t *lock, int seq)<BR>{<BR>&nbsp;if (seq &amp; 1)<BR>&nbsp;&nbsp;read_sequnlock_excl(lock);<BR>}</P>
<P>static inline void read_seqlock_excl_bh(seqlock_t *sl)<BR>{<BR>&nbsp;spin_lock_bh(&amp;sl-&gt;lock);<BR>}</P>
<P>static inline void read_sequnlock_excl_bh(seqlock_t *sl)<BR>{<BR>&nbsp;spin_unlock_bh(&amp;sl-&gt;lock);<BR>}</P>
<P>static inline void read_seqlock_excl_irq(seqlock_t *sl)<BR>{<BR>&nbsp;spin_lock_irq(&amp;sl-&gt;lock);<BR>}</P>
<P>static inline void read_sequnlock_excl_irq(seqlock_t *sl)<BR>{<BR>&nbsp;spin_unlock_irq(&amp;sl-&gt;lock);<BR>}</P>
<P>static inline unsigned long __read_seqlock_excl_irqsave(seqlock_t *sl)<BR>{<BR>&nbsp;unsigned long flags;</P>
<P>&nbsp;spin_lock_irqsave(&amp;sl-&gt;lock, flags);<BR>&nbsp;return flags;<BR>}</P>
<P>#define read_seqlock_excl_irqsave(lock, flags)&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;do { flags = __read_seqlock_excl_irqsave(lock); } while (0)</P>
<P>static inline void<BR>read_sequnlock_excl_irqrestore(seqlock_t *sl, unsigned long flags)<BR>{<BR>&nbsp;spin_unlock_irqrestore(&amp;sl-&gt;lock, flags);<BR>}</P>
<P>static inline unsigned long<BR>read_seqbegin_or_lock_irqsave(seqlock_t *lock, int *seq)<BR>{<BR>&nbsp;unsigned long flags = 0;</P>
<P>&nbsp;if (!(*seq &amp; 1))&nbsp;/* Even */<BR>&nbsp;&nbsp;*seq = read_seqbegin(lock);<BR>&nbsp;else&nbsp;&nbsp;&nbsp;/* Odd */<BR>&nbsp;&nbsp;read_seqlock_excl_irqsave(lock, flags);</P>
<P>&nbsp;return flags;<BR>}</P>
<P>static inline void<BR>done_seqretry_irqrestore(seqlock_t *lock, int seq, unsigned long flags)<BR>{<BR>&nbsp;if (seq &amp; 1)<BR>&nbsp;&nbsp;read_sequnlock_excl_irqrestore(lock, flags);<BR>}<BR>#endif /* __LINUX_SEQLOCK_H */