include/linux/workqueue.h 
<P></P>
<P>/*<BR>&nbsp;* workqueue.h --- work queue handling for Linux.<BR>&nbsp;*/</P>
<P></P>
<P>#ifndef _LINUX_WORKQUEUE_H<BR>#define _LINUX_WORKQUEUE_H</P>
<P>#include &lt;linux/timer.h&gt;<BR>#include &lt;linux/linkage.h&gt;<BR>#include &lt;linux/bitops.h&gt;<BR>#include &lt;linux/lockdep.h&gt;<BR>#include &lt;linux/threads.h&gt;<BR>#include &lt;linux/atomic.h&gt;<BR>#include &lt;linux/cpumask.h&gt;</P>
<P>struct workqueue_struct;</P>
<P>struct work_struct;<BR><FONT class=extract>typedef void (*work_func_t)(struct work_struct *work);<BR>void delayed_work_timer_fn(unsigned long __data)<FONT class=extract>;</FONT></FONT></P>
<P><FONT class=extract>/*<BR>&nbsp;* The first word is the work queue pointer and the flags rolled into<BR>&nbsp;* one<BR>&nbsp;*/<BR>#define work_data_bits(work) ((unsigned long *)(&amp;(work)-&gt;data))</FONT></P>
<P><FONT class=extract>enum {<BR>&nbsp;WORK_STRUCT_PENDING_BIT&nbsp;= 0,&nbsp;/* work item is pending execution */<BR>&nbsp;WORK_STRUCT_DELAYED_BIT&nbsp;= 1,&nbsp;/* work item is delayed */<BR>&nbsp;WORK_STRUCT_PWQ_BIT&nbsp;= 2,&nbsp;/* data points to pwq */<BR>&nbsp;WORK_STRUCT_LINKED_BIT&nbsp;= 3,&nbsp;/* next work is linked to this one */<BR>#ifdef CONFIG_DEBUG_OBJECTS_WORK<BR>&nbsp;WORK_STRUCT_STATIC_BIT&nbsp;= 4,&nbsp;/* static initializer (debugobjects) */<BR>&nbsp;WORK_STRUCT_COLOR_SHIFT&nbsp;= 5,&nbsp;/* color for workqueue flushing */<BR>#else<BR>&nbsp;WORK_STRUCT_COLOR_SHIFT&nbsp;= 4,&nbsp;/* color for workqueue flushing */<BR>#endif</FONT></P>
<P><FONT class=extract>&nbsp;WORK_STRUCT_COLOR_BITS&nbsp;= 4,</FONT></P>
<P><FONT class=extract>&nbsp;WORK_STRUCT_PENDING&nbsp;= 1 &lt;&lt; WORK_STRUCT_PENDING_BIT,<BR>&nbsp;WORK_STRUCT_DELAYED&nbsp;= 1 &lt;&lt; WORK_STRUCT_DELAYED_BIT,<BR>&nbsp;WORK_STRUCT_PWQ&nbsp;&nbsp;= 1 &lt;&lt; WORK_STRUCT_PWQ_BIT,<BR>&nbsp;WORK_STRUCT_LINKED&nbsp;= 1 &lt;&lt; WORK_STRUCT_LINKED_BIT,<BR>#ifdef CONFIG_DEBUG_OBJECTS_WORK<BR>&nbsp;WORK_STRUCT_STATIC&nbsp;= 1 &lt;&lt; WORK_STRUCT_STATIC_BIT,<BR>#else<BR>&nbsp;WORK_STRUCT_STATIC&nbsp;= 0,<BR>#endif</FONT></P>
<P><FONT class=extract>&nbsp;/*<BR>&nbsp; * The last color is no color used for works which don't<BR>&nbsp; * participate in workqueue flushing.<BR>&nbsp; */<BR>&nbsp;WORK_NR_COLORS&nbsp;&nbsp;= (1 &lt;&lt; WORK_STRUCT_COLOR_BITS) - 1,<BR>&nbsp;WORK_NO_COLOR&nbsp;&nbsp;= WORK_NR_COLORS,</FONT></P>
<P><FONT class=extract>&nbsp;/* not bound to any CPU, prefer the local CPU */<BR>&nbsp;WORK_CPU_UNBOUND&nbsp;= NR_CPUS,</FONT></P>
<P><FONT class=extract>&nbsp;/*<BR>&nbsp; * Reserve 7 bits off of pwq pointer w/ debugobjects turned off.<BR>&nbsp; * This makes pwqs aligned to 256 bytes and allows 15 workqueue<BR>&nbsp; * flush colors.<BR>&nbsp; */<BR>&nbsp;WORK_STRUCT_FLAG_BITS&nbsp;= WORK_STRUCT_COLOR_SHIFT +<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; WORK_STRUCT_COLOR_BITS,</FONT></P>
<P><FONT class=extract>&nbsp;/* data contains off-queue information when !WORK_STRUCT_PWQ */<BR>&nbsp;WORK_OFFQ_FLAG_BASE&nbsp;= WORK_STRUCT_COLOR_SHIFT,</FONT></P>
<P><FONT class=extract>&nbsp;__WORK_OFFQ_CANCELING&nbsp;= WORK_OFFQ_FLAG_BASE,<BR>&nbsp;WORK_OFFQ_CANCELING&nbsp;= (1 &lt;&lt; __WORK_OFFQ_CANCELING),</FONT></P>
<P><FONT class=extract>&nbsp;/*<BR>&nbsp; * When a work item is off queue, its high bits point to the last<BR>&nbsp; * pool it was on.&nbsp; Cap at 31 bits and use the highest number to<BR>&nbsp; * indicate that no pool is associated.<BR>&nbsp; */<BR>&nbsp;WORK_OFFQ_FLAG_BITS&nbsp;= 1,<BR>&nbsp;WORK_OFFQ_POOL_SHIFT&nbsp;= WORK_OFFQ_FLAG_BASE + WORK_OFFQ_FLAG_BITS,<BR>&nbsp;WORK_OFFQ_LEFT&nbsp;&nbsp;= BITS_PER_LONG - WORK_OFFQ_POOL_SHIFT,<BR>&nbsp;WORK_OFFQ_POOL_BITS&nbsp;= WORK_OFFQ_LEFT &lt;= 31 ? WORK_OFFQ_LEFT : 31,<BR>&nbsp;WORK_OFFQ_POOL_NONE&nbsp;= (1LU &lt;&lt; WORK_OFFQ_POOL_BITS) - 1,</FONT></P>
<P><FONT class=extract>&nbsp;/* convenience constants */<BR>&nbsp;WORK_STRUCT_FLAG_MASK&nbsp;= (1UL &lt;&lt; WORK_STRUCT_FLAG_BITS) - 1,<BR>&nbsp;WORK_STRUCT_WQ_DATA_MASK = ~WORK_STRUCT_FLAG_MASK,<BR>&nbsp;WORK_STRUCT_NO_POOL&nbsp;= (unsigned long)WORK_OFFQ_POOL_NONE &lt;&lt; WORK_OFFQ_POOL_SHIFT,</FONT></P>
<P><FONT class=extract>&nbsp;/* bit mask for work_busy() return values */<BR>&nbsp;WORK_BUSY_PENDING&nbsp;= 1 &lt;&lt; 0,<BR>&nbsp;WORK_BUSY_RUNNING&nbsp;= 1 &lt;&lt; 1,</FONT></P>
<P><FONT class=extract>&nbsp;/* maximum string length for set_worker_desc() */<BR>&nbsp;WORKER_DESC_LEN&nbsp;&nbsp;= 24,<BR>};</FONT></P>
<P><FONT class=extract>struct work_struct {<BR>&nbsp;atomic_long_t data;<BR>&nbsp;struct list_head entry;<BR>&nbsp;work_func_t func;<BR>#ifdef CONFIG_LOCKDEP<BR>&nbsp;struct lockdep_map lockdep_map;<BR>#endif<BR>};</FONT></P>
<P><FONT class=extract>#define WORK_DATA_INIT()&nbsp;ATOMIC_LONG_INIT(WORK_STRUCT_NO_POOL)<BR>#define WORK_DATA_STATIC_INIT()&nbsp;\<BR>&nbsp;ATOMIC_LONG_INIT(WORK_STRUCT_NO_POOL | WORK_STRUCT_STATIC)</FONT></P>
<P><FONT class=extract>struct delayed_work {<BR>&nbsp;struct work_struct work;<BR>&nbsp;struct timer_list timer;</FONT></P>
<P><FONT class=extract>&nbsp;/* target workqueue and CPU -&gt;timer uses to queue -&gt;work */<BR>&nbsp;struct workqueue_struct *wq;<BR>&nbsp;int cpu;<BR>};</FONT></P>
<P><FONT class=extract>/*<BR>&nbsp;* A struct for workqueue attributes.&nbsp; This can be used to change<BR>&nbsp;* attributes of an unbound workqueue.<BR>&nbsp;*<BR>&nbsp;* Unlike other fields, -&gt;no_numa isn't a property of a worker_pool.&nbsp; It<BR>&nbsp;* only modifies how apply_workqueue_attrs() select pools and thus doesn't<BR>&nbsp;* participate in pool hash calculations or equality comparisons.<BR>&nbsp;*/<BR>struct workqueue_attrs {<BR>&nbsp;int&nbsp;&nbsp;&nbsp;nice;&nbsp;&nbsp;/* nice level */<BR>&nbsp;cpumask_var_t&nbsp;&nbsp;cpumask;&nbsp;/* allowed CPUs */<BR>&nbsp;bool&nbsp;&nbsp;&nbsp;no_numa;&nbsp;/* disable NUMA affinity */<BR>};</FONT></P>
<P><FONT class=extract>static inline struct delayed_work *to_delayed_work(struct work_struct *work)<BR>{<BR>&nbsp;return container_of(work, struct delayed_work, work);<BR>}</FONT></P>
<P><FONT class=extract>struct execute_work {<BR>&nbsp;struct work_struct work;<BR>};</FONT></P>
<P><FONT class=extract>#ifdef CONFIG_LOCKDEP<BR>/*<BR>&nbsp;* NB: because we have to copy the lockdep_map, setting _key<BR>&nbsp;* here is required, otherwise it could get initialised to the<BR>&nbsp;* copy of the lockdep_map!<BR>&nbsp;*/<BR>#define __WORK_INIT_LOCKDEP_MAP(n, k) \<BR>&nbsp;.lockdep_map = STATIC_LOCKDEP_MAP_INIT(n, k),<BR>#else<BR>#define __WORK_INIT_LOCKDEP_MAP(n, k)<BR>#endif</FONT></P>
<P><FONT class=extract>#define __WORK_INITIALIZER(n, f) {&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;.data = WORK_DATA_STATIC_INIT(),&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;.entry&nbsp;= { &amp;(n).entry, &amp;(n).entry },&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;.func = (f),&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;__WORK_INIT_LOCKDEP_MAP(#n, &amp;(n))&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;}</FONT></P>
<P><FONT class=extract>#define __DELAYED_WORK_INITIALIZER(n, f, tflags) {&nbsp;&nbsp;&nbsp;\<BR>&nbsp;.work = __WORK_INITIALIZER((n).work, (f)),&nbsp;&nbsp;&nbsp;\<BR>&nbsp;.timer = __TIMER_INITIALIZER(delayed_work_timer_fn,&nbsp;&nbsp;\<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0, (unsigned long)&amp;(n),&nbsp;&nbsp;\<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (tflags) | TIMER_IRQSAFE),&nbsp;&nbsp;\<BR>&nbsp;}</FONT></P>
<P><FONT class=extract>#define DECLARE_WORK(n, f)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;struct work_struct n = __WORK_INITIALIZER(n, f)</FONT></P>
<P><FONT class=extract>#define DECLARE_DELAYED_WORK(n, f)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;struct delayed_work n = __DELAYED_WORK_INITIALIZER(n, f, 0)</FONT></P>
<P><FONT class=extract>#define DECLARE_DEFERRABLE_WORK(n, f)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;struct delayed_work n = __DELAYED_WORK_INITIALIZER(n, f, TIMER_DEFERRABLE)</FONT></P>
<P><FONT class=extract>#ifdef CONFIG_DEBUG_OBJECTS_WORK<BR>extern void __init_work(struct work_struct *work, int onstack);<BR>extern void destroy_work_on_stack(struct work_struct *work);<BR>extern void destroy_delayed_work_on_stack(struct delayed_work *work);<BR>static inline unsigned int work_static(struct work_struct *work)<BR>{<BR>&nbsp;return *work_data_bits(work) &amp; WORK_STRUCT_STATIC;<BR>}<BR>#else<BR>static inline void __init_work(struct work_struct *work, int onstack) { }<BR>static inline void destroy_work_on_stack(struct work_struct *work) { }<BR>static inline void destroy_delayed_work_on_stack(struct delayed_work *work) { }<BR>static inline unsigned int work_static(struct work_struct *work) { return 0; }<BR>#endif</FONT></P>
<P><FONT class=extract>/*<BR>&nbsp;* initialize all of a work item in one go<BR>&nbsp;*<BR>&nbsp;* NOTE! No point in using "atomic_long_set()": using a direct<BR>&nbsp;* assignment of the work data initializer allows the compiler<BR>&nbsp;* to generate better code.<BR>&nbsp;*/<BR>#ifdef CONFIG_LOCKDEP<BR>#define __INIT_WORK(_work, _func, _onstack)&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;do {&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;&nbsp;static struct lock_class_key __key;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;&nbsp;__init_work((_work), _onstack);&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;&nbsp;(_work)-&gt;data = (atomic_long_t) WORK_DATA_INIT();&nbsp;\<BR>&nbsp;&nbsp;lockdep_init_map(&amp;(_work)-&gt;lockdep_map, #_work, &amp;__key, 0); \<BR>&nbsp;&nbsp;INIT_LIST_HEAD(&amp;(_work)-&gt;entry);&nbsp;&nbsp;&nbsp;\<BR>&nbsp;&nbsp;(_work)-&gt;func = (_func);&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;} while (0)<BR>#else<BR>#define __INIT_WORK(_work, _func, _onstack)&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;do {&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;&nbsp;__init_work((_work), _onstack);&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;&nbsp;(_work)-&gt;data = (atomic_long_t) WORK_DATA_INIT();&nbsp;\<BR>&nbsp;&nbsp;INIT_LIST_HEAD(&amp;(_work)-&gt;entry);&nbsp;&nbsp;&nbsp;\<BR>&nbsp;&nbsp;(_work)-&gt;func = (_func);&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;} while (0)<BR>#endif</FONT></P>
<P><FONT class=extract>#define INIT_WORK(_work, _func)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;__INIT_WORK((_work), (_func), 0)</FONT></P>
<P><FONT class=extract>#define INIT_WORK_ONSTACK(_work, _func)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;__INIT_WORK((_work), (_func), 1)</FONT></P>
<P><FONT class=extract>#define __INIT_DELAYED_WORK(_work, _func, _tflags)&nbsp;&nbsp;&nbsp;\<BR>&nbsp;do {&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;&nbsp;INIT_WORK(&amp;(_work)-&gt;work, (_func));&nbsp;&nbsp;&nbsp;\<BR>&nbsp;&nbsp;__setup_timer(&amp;(_work)-&gt;timer, delayed_work_timer_fn,&nbsp;\<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (unsigned long)(_work),&nbsp;&nbsp;&nbsp;\<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (_tflags) | TIMER_IRQSAFE);&nbsp;&nbsp;\<BR>&nbsp;} while (0)</FONT></P>
<P><FONT class=extract>#define __INIT_DELAYED_WORK_ONSTACK(_work, _func, _tflags)&nbsp;&nbsp;\<BR>&nbsp;do {&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;&nbsp;INIT_WORK_ONSTACK(&amp;(_work)-&gt;work, (_func));&nbsp;&nbsp;\<BR>&nbsp;&nbsp;__setup_timer_on_stack(&amp;(_work)-&gt;timer,&nbsp;&nbsp;&nbsp;\<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; delayed_work_timer_fn,&nbsp;&nbsp;\<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (unsigned long)(_work),&nbsp;&nbsp;\<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (_tflags) | TIMER_IRQSAFE);&nbsp;\<BR>&nbsp;} while (0)</FONT></P>
<P><FONT class=extract>#define INIT_DELAYED_WORK(_work, _func)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;__INIT_DELAYED_WORK(_work, _func, 0)</FONT></P>
<P><FONT class=extract>#define INIT_DELAYED_WORK_ONSTACK(_work, _func)&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;__INIT_DELAYED_WORK_ONSTACK(_work, _func, 0)</FONT></P>
<P><FONT class=extract>#define INIT_DEFERRABLE_WORK(_work, _func)&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;__INIT_DELAYED_WORK(_work, _func, TIMER_DEFERRABLE)</FONT></P>
<P><FONT class=extract>#define INIT_DEFERRABLE_WORK_ONSTACK(_work, _func)&nbsp;&nbsp;&nbsp;\<BR>&nbsp;__INIT_DELAYED_WORK_ONSTACK(_work, _func, TIMER_DEFERRABLE)</FONT></P>
<P><FONT class=extract>/**<BR>&nbsp;* work_pending - Find out whether a work item is currently pending<BR>&nbsp;* @work: The work item in question<BR>&nbsp;*/<BR>#define work_pending(work) \<BR>&nbsp;test_bit(WORK_STRUCT_PENDING_BIT, work_data_bits(work))</FONT></P>
<P><FONT class=extract>/**<BR>&nbsp;* delayed_work_pending - Find out whether a delayable work item is currently<BR>&nbsp;* pending<BR>&nbsp;* @work: The work item in question<BR>&nbsp;*/<BR>#define delayed_work_pending(w) \<BR>&nbsp;work_pending(&amp;(w)-&gt;work)</FONT></P>
<P><FONT class=extract>/*<BR>&nbsp;* Workqueue flags and constants.&nbsp; For details, please refer to<BR>&nbsp;* Documentation/workqueue.txt.<BR>&nbsp;*/<BR>enum {<BR>&nbsp;WQ_UNBOUND&nbsp;&nbsp;= 1 &lt;&lt; 1, /* not bound to any cpu */<BR>&nbsp;WQ_FREEZABLE&nbsp;&nbsp;= 1 &lt;&lt; 2, /* freeze during suspend */<BR>&nbsp;WQ_MEM_RECLAIM&nbsp;&nbsp;= 1 &lt;&lt; 3, /* may be used for memory reclaim */<BR>&nbsp;WQ_HIGHPRI&nbsp;&nbsp;= 1 &lt;&lt; 4, /* high priority */<BR>&nbsp;WQ_CPU_INTENSIVE&nbsp;= 1 &lt;&lt; 5, /* cpu intensive workqueue */<BR>&nbsp;WQ_SYSFS&nbsp;&nbsp;= 1 &lt;&lt; 6, /* visible in sysfs, see wq_sysfs_register() */</FONT></P>
<P><FONT class=extract>&nbsp;/*<BR>&nbsp; * Per-cpu workqueues are generally preferred because they tend to<BR>&nbsp; * show better performance thanks to cache locality.&nbsp; Per-cpu<BR>&nbsp; * workqueues exclude the scheduler from choosing the CPU to<BR>&nbsp; * execute the worker threads, which has an unfortunate side effect<BR>&nbsp; * of increasing power consumption.<BR>&nbsp; *<BR>&nbsp; * The scheduler considers a CPU idle if it doesn't have any task<BR>&nbsp; * to execute and tries to keep idle cores idle to conserve power;<BR>&nbsp; * however, for example, a per-cpu work item scheduled from an<BR>&nbsp; * interrupt handler on an idle CPU will force the scheduler to<BR>&nbsp; * excute the work item on that CPU breaking the idleness, which in<BR>&nbsp; * turn may lead to more scheduling choices which are sub-optimal<BR>&nbsp; * in terms of power consumption.<BR>&nbsp; *<BR>&nbsp; * Workqueues marked with WQ_POWER_EFFICIENT are per-cpu by default<BR>&nbsp; * but become unbound if workqueue.power_efficient kernel param is<BR>&nbsp; * specified.&nbsp; Per-cpu workqueues which are identified to<BR>&nbsp; * contribute significantly to power-consumption are identified and<BR>&nbsp; * marked with this flag and enabling the power_efficient mode<BR>&nbsp; * leads to noticeable power saving at the cost of small<BR>&nbsp; * performance disadvantage.<BR>&nbsp; *<BR>&nbsp; * </FONT><A href="http://thread.gmane.org/gmane.linux.kernel/1480396"><FONT class=extract>http://thread.gmane.org/gmane.linux.kernel/1480396</FONT></A><BR><FONT class=extract>&nbsp; */<BR>&nbsp;WQ_POWER_EFFICIENT&nbsp;= 1 &lt;&lt; 7,</FONT></P>
<P><FONT class=extract>&nbsp;__WQ_DRAINING&nbsp;&nbsp;= 1 &lt;&lt; 16, /* internal: workqueue is draining */<BR>&nbsp;__WQ_ORDERED&nbsp;&nbsp;= 1 &lt;&lt; 17, /* internal: workqueue is ordered */</FONT></P>
<P><FONT class=extract>&nbsp;WQ_MAX_ACTIVE&nbsp;&nbsp;= 512,&nbsp;&nbsp; /* I like 512, better ideas? */<BR>&nbsp;WQ_MAX_UNBOUND_PER_CPU&nbsp;= 4,&nbsp;&nbsp; /* 4 * #cpus for unbound wq */<BR>&nbsp;WQ_DFL_ACTIVE&nbsp;&nbsp;= WQ_MAX_ACTIVE / 2,<BR>};</FONT></P>
<P><FONT class=extract>/* unbound wq's aren't per-cpu, scale max_active according to #cpus */<BR>#define WQ_UNBOUND_MAX_ACTIVE&nbsp;\<BR>&nbsp;max_t(int, WQ_MAX_ACTIVE, num_possible_cpus() * WQ_MAX_UNBOUND_PER_CPU)</FONT></P>
<P><FONT class=extract>/*<BR>&nbsp;* System-wide workqueues which are always present.<BR>&nbsp;*<BR>&nbsp;* system_wq is the one used by schedule[_delayed]_work[_on]().<BR>&nbsp;* Multi-CPU multi-threaded.&nbsp; There are users which expect relatively<BR>&nbsp;* short queue flush time.&nbsp; Don't queue works which can run for too<BR>&nbsp;* long.<BR>&nbsp;*<BR>&nbsp;* system_highpri_wq is similar to system_wq but for work items which<BR>&nbsp;* require WQ_HIGHPRI.<BR>&nbsp;*<BR>&nbsp;* system_long_wq is similar to system_wq but may host long running<BR>&nbsp;* works.&nbsp; Queue flushing might take relatively long.<BR>&nbsp;*<BR>&nbsp;* system_unbound_wq is unbound workqueue.&nbsp; Workers are not bound to<BR>&nbsp;* any specific CPU, not concurrency managed, and all queued works are<BR>&nbsp;* executed immediately as long as max_active limit is not reached and<BR>&nbsp;* resources are available.<BR>&nbsp;*<BR>&nbsp;* system_freezable_wq is equivalent to system_wq except that it's<BR>&nbsp;* freezable.<BR>&nbsp;*<BR>&nbsp;* *_power_efficient_wq are inclined towards saving power and converted<BR>&nbsp;* into WQ_UNBOUND variants if 'wq_power_efficient' is enabled; otherwise,<BR>&nbsp;* they are same as their non-power-efficient counterparts - e.g.<BR>&nbsp;* system_power_efficient_wq is identical to system_wq if<BR>&nbsp;* 'wq_power_efficient' is disabled.&nbsp; See WQ_POWER_EFFICIENT for more info.<BR>&nbsp;*/<BR>extern struct workqueue_struct *system_wq;<BR>extern struct workqueue_struct *system_highpri_wq;<BR>extern struct workqueue_struct *system_long_wq;<BR>extern struct workqueue_struct *system_unbound_wq;<BR>extern struct workqueue_struct *system_freezable_wq;<BR>extern struct workqueue_struct *system_power_efficient_wq;<BR>extern struct workqueue_struct *system_freezable_power_efficient_wq;</FONT></P>
<P><FONT class=extract>extern struct workqueue_struct *<BR>__alloc_workqueue_key(const char *fmt, unsigned int flags, int max_active,<BR>&nbsp;struct lock_class_key *key, const char *lock_name, ...) __printf(1, 6);</FONT></P>
<P><FONT class=extract>/**<BR>&nbsp;* alloc_workqueue - allocate a workqueue<BR>&nbsp;* @fmt: printf format for the name of the workqueue<BR>&nbsp;* @flags: WQ_* flags<BR>&nbsp;* @max_active: max in-flight work items, 0 for default<BR>&nbsp;* @args: args for @fmt<BR>&nbsp;*<BR>&nbsp;* Allocate a workqueue with the specified parameters.&nbsp; For detailed<BR>&nbsp;* information on WQ_* flags, please refer to Documentation/workqueue.txt.<BR>&nbsp;*<BR>&nbsp;* The __lock_name macro dance is to guarantee that single lock_class_key<BR>&nbsp;* doesn't end up with different namesm, which isn't allowed by lockdep.<BR>&nbsp;*<BR>&nbsp;* RETURNS:<BR>&nbsp;* Pointer to the allocated workqueue on success, %NULL on failure.<BR>&nbsp;*/<BR>#ifdef CONFIG_LOCKDEP<BR>#define alloc_workqueue(fmt, flags, max_active, args...)&nbsp;&nbsp;\<BR>({&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;static struct lock_class_key __key;&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;const char *__lock_name;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;__lock_name = #fmt#args;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;__alloc_workqueue_key((fmt), (flags), (max_active),&nbsp;&nbsp;\<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;__key, __lock_name, ##args);&nbsp;&nbsp;\<BR>})<BR>#else<BR>#define alloc_workqueue(fmt, flags, max_active, args...)&nbsp;&nbsp;\<BR>&nbsp;__alloc_workqueue_key((fmt), (flags), (max_active),&nbsp;&nbsp;\<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; NULL, NULL, ##args)<BR>#endif</FONT></P>
<P><FONT class=extract>/**<BR>&nbsp;* alloc_ordered_workqueue - allocate an ordered workqueue<BR>&nbsp;* @fmt: printf format for the name of the workqueue<BR>&nbsp;* @flags: WQ_* flags (only WQ_FREEZABLE and WQ_MEM_RECLAIM are meaningful)<BR>&nbsp;* @args: args for @fmt<BR>&nbsp;*<BR>&nbsp;* Allocate an ordered workqueue.&nbsp; An ordered workqueue executes at<BR>&nbsp;* most one work item at any given time in the queued order.&nbsp; They are<BR>&nbsp;* implemented as unbound workqueues with @max_active of one.<BR>&nbsp;*<BR>&nbsp;* RETURNS:<BR>&nbsp;* Pointer to the allocated workqueue on success, %NULL on failure.<BR>&nbsp;*/<BR>#define alloc_ordered_workqueue(fmt, flags, args...)&nbsp;&nbsp;&nbsp;\<BR>&nbsp;alloc_workqueue(fmt, WQ_UNBOUND | __WQ_ORDERED | (flags), 1, ##args)</FONT></P>
<P><FONT class=extract>#define create_workqueue(name)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;alloc_workqueue("%s", WQ_MEM_RECLAIM, 1, (name))<BR>#define create_freezable_workqueue(name)&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;alloc_workqueue("%s", WQ_FREEZABLE | WQ_UNBOUND | WQ_MEM_RECLAIM, \<BR>&nbsp;&nbsp;&nbsp;1, (name))<BR>#define create_singlethread_workqueue(name)&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;alloc_ordered_workqueue("%s", WQ_MEM_RECLAIM, name)</FONT></P>
<P><FONT class=extract>extern void destroy_workqueue(struct workqueue_struct *wq);</FONT></P>
<P><FONT class=extract>struct workqueue_attrs *alloc_workqueue_attrs(gfp_t gfp_mask);<BR>void free_workqueue_attrs(struct workqueue_attrs *attrs);<BR>int apply_workqueue_attrs(struct workqueue_struct *wq,<BR>&nbsp;&nbsp;&nbsp;&nbsp; const struct workqueue_attrs *attrs);<BR>int workqueue_set_unbound_cpumask(cpumask_var_t cpumask);</FONT></P>
<P><FONT class=extract>extern bool queue_work_on(int cpu, struct workqueue_struct *wq,<BR>&nbsp;&nbsp;&nbsp;struct work_struct *work);<BR>extern bool queue_delayed_work_on(int cpu, struct workqueue_struct *wq,<BR>&nbsp;&nbsp;&nbsp;struct delayed_work *work, unsigned long delay);<BR>extern bool mod_delayed_work_on(int cpu, struct workqueue_struct *wq,<BR>&nbsp;&nbsp;&nbsp;struct delayed_work *dwork, unsigned long delay);</FONT></P>
<P><FONT class=extract>extern void flush_workqueue(struct workqueue_struct *wq);<BR>extern void drain_workqueue(struct workqueue_struct *wq);</FONT></P>
<P><FONT class=extract>extern int schedule_on_each_cpu(work_func_t func);</FONT></P>
<P><FONT class=extract>int execute_in_process_context(work_func_t fn, struct execute_work *);</FONT></P>
<P><FONT class=extract>extern bool flush_work(struct work_struct *work);<BR>extern bool cancel_work_sync(struct work_struct *work);</FONT></P>
<P><FONT class=extract>extern bool flush_delayed_work(struct delayed_work *dwork);<BR>extern bool cancel_delayed_work(struct delayed_work *dwork);<BR>extern bool cancel_delayed_work_sync(struct delayed_work *dwork);</FONT></P>
<P><FONT class=extract>extern void workqueue_set_max_active(struct workqueue_struct *wq,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; int max_active);<BR>extern bool current_is_workqueue_rescuer(void);<BR>extern bool workqueue_congested(int cpu, struct workqueue_struct *wq);<BR>extern unsigned int work_busy(struct work_struct *work);<BR>extern __printf(1, 2) void set_worker_desc(const char *fmt, ...);<BR>extern void print_worker_info(const char *log_lvl, struct task_struct *task);<BR>extern void show_workqueue_state(void);</FONT></P>
<P><FONT class=extract>/**<BR>&nbsp;* queue_work - queue work on a workqueue<BR>&nbsp;* @wq: workqueue to use<BR>&nbsp;* @work: work to queue<BR>&nbsp;*<BR>&nbsp;* Returns %false if @work was already on a queue, %true otherwise.<BR>&nbsp;*<BR>&nbsp;* We queue the work to the CPU on which it was submitted, but if the CPU dies<BR>&nbsp;* it can be processed by another CPU.<BR>&nbsp;*/<BR>static inline bool queue_work(struct workqueue_struct *wq,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; struct work_struct *work)<BR>{<BR>&nbsp;return queue_work_on(WORK_CPU_UNBOUND, wq, work);<BR>}</FONT></P>
<P><FONT class=extract>/**<BR>&nbsp;* queue_delayed_work - queue work on a workqueue after delay<BR>&nbsp;* @wq: workqueue to use<BR>&nbsp;* @dwork: delayable work to queue<BR>&nbsp;* @delay: number of jiffies to wait before queueing<BR>&nbsp;*<BR>&nbsp;* Equivalent to queue_delayed_work_on() but tries to use the local CPU.<BR>&nbsp;*/<BR>static inline bool queue_delayed_work(struct workqueue_struct *wq,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; struct delayed_work *dwork,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; unsigned long delay)<BR>{<BR>&nbsp;return queue_delayed_work_on(WORK_CPU_UNBOUND, wq, dwork, delay);<BR>}</FONT></P>
<P><FONT class=extract>/**<BR>&nbsp;* mod_delayed_work - modify delay of or queue a delayed work<BR>&nbsp;* @wq: workqueue to use<BR>&nbsp;* @dwork: work to queue<BR>&nbsp;* @delay: number of jiffies to wait before queueing<BR>&nbsp;*<BR>&nbsp;* mod_delayed_work_on() on local CPU.<BR>&nbsp;*/<BR>static inline bool mod_delayed_work(struct workqueue_struct *wq,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; struct delayed_work *dwork,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; unsigned long delay)<BR>{<BR>&nbsp;return mod_delayed_work_on(WORK_CPU_UNBOUND, wq, dwork, delay);<BR>}</FONT></P>
<P><FONT class=extract>/**<BR>&nbsp;* schedule_work_on - put work task on a specific cpu<BR>&nbsp;* @cpu: cpu to put the work task on<BR>&nbsp;* @work: job to be done<BR>&nbsp;*<BR>&nbsp;* This puts a job on a specific cpu<BR>&nbsp;*/<BR>static inline bool schedule_work_on(int cpu, struct work_struct *work)<BR>{<BR>&nbsp;return queue_work_on(cpu, system_wq, work);<BR>}</FONT></P>
<P><FONT class=extract>/**<BR>&nbsp;* schedule_work - put work task in global workqueue<BR>&nbsp;* @work: job to be done<BR>&nbsp;*<BR>&nbsp;* Returns %false if @work was already on the kernel-global workqueue and<BR>&nbsp;* %true otherwise.<BR>&nbsp;*<BR>&nbsp;* This puts a job in the kernel-global workqueue if it was not already<BR>&nbsp;* queued and leaves it in the same position on the kernel-global<BR>&nbsp;* workqueue otherwise.<BR>&nbsp;*/<BR>static inline bool schedule_work(struct work_struct *work)<BR>{<BR>&nbsp;return queue_work(system_wq, work);<BR>}</FONT></P>
<P><FONT class=extract>/**<BR>&nbsp;* flush_scheduled_work - ensure that any scheduled work has run to completion.<BR>&nbsp;*<BR>&nbsp;* Forces execution of the kernel-global workqueue and blocks until its<BR>&nbsp;* completion.<BR>&nbsp;*<BR>&nbsp;* Think twice before calling this function!&nbsp; It's very easy to get into<BR>&nbsp;* trouble if you don't take great care.&nbsp; Either of the following situations<BR>&nbsp;* will lead to deadlock:<BR>&nbsp;*<BR>&nbsp;*&nbsp;One of the work items currently on the workqueue needs to acquire<BR>&nbsp;*&nbsp;a lock held by your code or its caller.<BR>&nbsp;*<BR>&nbsp;*&nbsp;Your code is running in the context of a work routine.<BR>&nbsp;*<BR>&nbsp;* They will be detected by lockdep when they occur, but the first might not<BR>&nbsp;* occur very often.&nbsp; It depends on what work items are on the workqueue and<BR>&nbsp;* what locks they need, which you have no control over.<BR>&nbsp;*<BR>&nbsp;* In most situations flushing the entire workqueue is overkill; you merely<BR>&nbsp;* need to know that a particular work item isn't queued and isn't running.<BR>&nbsp;* In such cases you should use cancel_delayed_work_sync() or<BR>&nbsp;* cancel_work_sync() instead.<BR>&nbsp;*/<BR>static inline void flush_scheduled_work(void)<BR>{<BR>&nbsp;flush_workqueue(system_wq);<BR>}</FONT></P>
<P><FONT class=extract>/**<BR>&nbsp;* schedule_delayed_work_on - queue work in global workqueue on CPU after delay<BR>&nbsp;* @cpu: cpu to use<BR>&nbsp;* @dwork: job to be done<BR>&nbsp;* @delay: number of jiffies to wait<BR>&nbsp;*<BR>&nbsp;* After waiting for a given time this puts a job in the kernel-global<BR>&nbsp;* workqueue on the specified CPU.<BR>&nbsp;*/<BR>static inline bool schedule_delayed_work_on(int cpu, struct delayed_work *dwork,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; unsigned long delay)<BR>{<BR>&nbsp;return queue_delayed_work_on(cpu, system_wq, dwork, delay);<BR>}</FONT></P>
<P><FONT class=extract>/**<BR>&nbsp;* schedule_delayed_work - put work task in global workqueue after delay<BR>&nbsp;* @dwork: job to be done<BR>&nbsp;* @delay: number of jiffies to wait or 0 for immediate execution<BR>&nbsp;*<BR>&nbsp;* After waiting for a given time this puts a job in the kernel-global<BR>&nbsp;* workqueue.<BR>&nbsp;*/<BR>static inline bool schedule_delayed_work(struct delayed_work *dwork,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; unsigned long delay)<BR>{<BR>&nbsp;return queue_delayed_work(system_wq, dwork, delay);<BR>}</FONT></P>
<P><FONT class=extract>/**<BR>&nbsp;* keventd_up - is workqueue initialized yet?<BR>&nbsp;*/<BR>static inline bool keventd_up(void)<BR>{<BR>&nbsp;return system_wq != NULL;<BR>}</FONT></P>
<P><FONT class=extract>#ifndef CONFIG_SMP<BR>static inline long work_on_cpu(int cpu, long (*fn)(void *), void *arg)<BR>{<BR>&nbsp;return fn(arg);<BR>}<BR>#else<BR>long work_on_cpu(int cpu, long (*fn)(void *), void *arg);<BR>#endif /* CONFIG_SMP */</FONT></P>
<P><FONT class=extract>#ifdef CONFIG_FREEZER<BR>extern void freeze_workqueues_begin(void);<BR>extern bool freeze_workqueues_busy(void);<BR>extern void thaw_workqueues(void);<BR>#endif /* CONFIG_FREEZER */</FONT></P>
<P><FONT class=extract>#ifdef CONFIG_SYSFS<BR>int workqueue_sysfs_register(struct workqueue_struct *wq);<BR>#else&nbsp;/* CONFIG_SYSFS */<BR>static inline int workqueue_sysfs_register(struct workqueue_struct *wq)<BR>{ return 0; }<BR>#endif&nbsp;/* CONFIG_SYSFS */</FONT></P>
<P>#endif