Consider a typical block of kernel code: 
<P></P>
<P>&nbsp;&nbsp;&nbsp; spin_lock(&amp;the_lock);<BR>&nbsp;&nbsp;&nbsp; do_something_on(&amp;shared_data);<BR>&nbsp;&nbsp;&nbsp; do_something_else_with(&amp;shared_data);<BR>&nbsp;&nbsp;&nbsp; spin_unlock(&amp;the_lock);</P>
<P>If all the code follows the locking rules, the value of shared_data cannot<BR>change unexpectedly while the_lock is held.&nbsp; Any other code which might<BR>want to play with that data will be waiting on the lock.&nbsp; The spinlock<BR>primitives act as memory barriers - they are explicitly written to do so -<BR>meaning that data accesses will not be optimized across them.&nbsp; So the<BR>compiler might think it knows what will be in shared_data, but the<BR>spin_lock() call, since it acts as a memory barrier, will force it to<BR><SPAN class=cloze>[...]</SPAN>.&nbsp; There will be no optimization problems with<BR>accesses to that data.</P>
<P>If shared_data were declared volatile, the locking would still be<BR>necessary.&nbsp; But the compiler would also be prevented from optimizing access<BR>to shared_data _within_ the critical section, when we know that nobody else<BR>can be working with it.&nbsp; While the lock is held, shared_data is not<BR>volatile.&nbsp; When dealing with shared data, proper locking makes volatile<BR>unnecessary - and potentially harmful.