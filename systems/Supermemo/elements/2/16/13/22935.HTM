# Documentation/virtual/kvm/locking.txt
<P></P>
<P>KVM Lock Overview<BR>=================</P>
<P></P>
<P>1. Acquisition Orders<BR>---------------------</P>
<P>(to be written)</P>
<P>2: Exception<BR>------------</P>
<P>Fast page fault:</P>
<P>Fast page fault is the fast path which fixes the guest page fault out of<BR>the mmu-lock on x86. Currently, the page fault can be fast only if the<BR>shadow page table is present and it is caused by write-protect, that means<BR>we just need change the W bit of the spte.</P>
<P>What we use to avoid all the race is the SPTE_HOST_WRITEABLE bit and<BR>SPTE_MMU_WRITEABLE bit on the spte:<BR>- SPTE_HOST_WRITEABLE means the gfn is writable on host.<BR>- SPTE_MMU_WRITEABLE means the gfn is writable on mmu. The bit is set when<BR>&nbsp; the gfn is writable on guest mmu and it is not write-protected by shadow<BR>&nbsp; page write-protection.</P>
<P>On fast page fault path, we will use cmpxchg to atomically set the spte W<BR>bit if spte.SPTE_HOST_WRITEABLE = 1 and spte.SPTE_WRITE_PROTECT = 1, this<BR>is safe because whenever changing these bits can be detected by cmpxchg.</P>
<P>But we need carefully check these cases:<BR>1): The mapping from gfn to pfn<BR>The mapping from gfn to pfn may be changed since we can only ensure the pfn<BR>is not changed during cmpxchg. This is a ABA problem, for example, below case<BR>will happen:</P>
<P>At the beginning:<BR>gpte = gfn1<BR>gfn1 is mapped to pfn1 on host<BR>spte is the shadow page table entry corresponding with gpte and<BR>spte = pfn1</P>
<P>&nbsp;&nbsp; VCPU 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; VCPU0<BR>on fast page fault path:</P>
<P>&nbsp;&nbsp; old_spte = *spte;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; pfn1 is swapped out:<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; spte = 0;</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; pfn1 is re-alloced for gfn2.</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; gpte is changed to point to<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; gfn2 by the guest:<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; spte = pfn1;</P>
<P>&nbsp;&nbsp; if (cmpxchg(spte, old_spte, old_spte+W)<BR>&nbsp;mark_page_dirty(vcpu-&gt;kvm, gfn1)<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; OOPS!!!</P>
<P>We dirty-log for gfn1, that means gfn2 is lost in dirty-bitmap.</P>
<P>For direct sp, we can easily avoid it since the spte of direct sp is fixed<BR>to gfn. For indirect sp, before we do cmpxchg, we call gfn_to_pfn_atomic()<BR>to pin gfn to pfn, because after gfn_to_pfn_atomic():<BR>- We have held the refcount of pfn that means the pfn can not be freed and<BR>&nbsp; be reused for another gfn.<BR>- The pfn is writable that means it can not be shared between different gfns<BR>&nbsp; by KSM.</P>
<P>Then, we can ensure the dirty bitmaps is correctly set for a gfn.</P>
<P>Currently, to simplify the whole things, we disable fast page fault for<BR>indirect shadow page.</P>
<P>2): Dirty bit tracking<BR>In the origin code, the spte can be fast updated (non-atomically) if the<BR>spte is read-only and the Accessed bit has already been set since the<BR>Accessed bit and Dirty bit can not be lost.</P>
<P>But it is not true after fast page fault since the spte can be marked<BR>writable between reading spte and updating spte. Like below case:</P>
<P>At the beginning:<BR>spte.W = 0<BR>spte.Accessed = 1</P>
<P>&nbsp;&nbsp; VCPU 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; VCPU0<BR>In mmu_spte_clear_track_bits():</P>
<P>&nbsp;&nbsp; old_spte = *spte;</P>
<P>&nbsp;&nbsp; /* 'if' condition is satisfied. */<BR>&nbsp;&nbsp; if (old_spte.Accssed == 1 &amp;&amp;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; old_spte.W == 0)<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; spte = 0ull;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; on fast page fault path:<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; spte.W = 1<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; memory write on the spte:<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; spte.Dirty = 1</P>
<P><BR>&nbsp;&nbsp; else<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; old_spte = xchg(spte, 0ull)</P>
<P><BR>&nbsp;&nbsp; if (old_spte.Accssed == 1)<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; kvm_set_pfn_accessed(spte.pfn);<BR>&nbsp;&nbsp; if (old_spte.Dirty == 1)<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; kvm_set_pfn_dirty(spte.pfn);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; OOPS!!!</P>
<P>The Dirty bit is lost in this case.</P>
<P>In order to avoid this kind of issue, we always treat the spte as "volatile"<BR>if it can be updated out of mmu-lock, see spte_has_volatile_bits(), it means,<BR>the spte is always atomicly updated in this case.</P>
<P>3): flush tlbs due to spte updated<BR>If the spte is updated from writable to readonly, we should flush all TLBs,<BR>otherwise rmap_write_protect will find a read-only spte, even though the<BR>writable spte might be cached on a CPU's TLB.</P>
<P>As mentioned before, the spte can be updated to writable out of mmu-lock on<BR>fast page fault path, in order to easily audit the path, we see if TLBs need<BR>be flushed caused by this reason in mmu_spte_update() since this is a common<BR>function to update spte (present -&gt; present).</P>
<P>Since the spte is "volatile" if it can be updated out of mmu-lock, we always<BR>atomicly update the spte, the race caused by fast page fault can be avoided,<BR>See the comments in spte_has_volatile_bits() and mmu_spte_update().</P>
<P>3. Reference<BR>------------</P>
<P>Name:&nbsp;&nbsp;kvm_lock<BR>Type:&nbsp;&nbsp;spinlock_t<BR>Arch:&nbsp;&nbsp;any<BR>Protects:&nbsp;- vm_list</P>
<P>Name:&nbsp;&nbsp;kvm_count_lock<BR>Type:&nbsp;&nbsp;raw_spinlock_t<BR>Arch:&nbsp;&nbsp;any<BR>Protects:&nbsp;- hardware virtualization enable/disable<BR>Comment:&nbsp;'raw' because hardware enabling/disabling must be atomic /wrt<BR>&nbsp;&nbsp;migration.</P>
<P>Name:&nbsp;&nbsp;kvm_arch::tsc_write_lock<BR>Type:&nbsp;&nbsp;raw_spinlock<BR>Arch:&nbsp;&nbsp;x86<BR>Protects:&nbsp;- kvm_arch::{last_tsc_write,last_tsc_nsec,last_tsc_offset}<BR>&nbsp;&nbsp;- tsc offset in vmcb<BR>Comment:&nbsp;'raw' because updating the tsc offsets must not be preempted.</P>
<P>Name:&nbsp;&nbsp;kvm-&gt;mmu_lock<BR>Type:&nbsp;&nbsp;spinlock_t<BR>Arch:&nbsp;&nbsp;any<BR>Protects:&nbsp;-shadow page/shadow tlb entry<BR>Comment:&nbsp;it is a spinlock since it is used in mmu notifier.