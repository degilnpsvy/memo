<H3 id=-100000 class=docSection1Title>Direct Memory Access</H3>
<P class=docText><SPAN class=docEmphasis><A name="Direct Memory"></A>Direct Memory Access</SPAN><A name="capability to"></A> (DMA) is the capability to transfer data from a peripheral to main memory without the CPU's intervention. DMA boosts the performance of peripherals manyfold, because it doesn't burn CPU cycles to move data. PCI networking cards and IDE disk drives are common examples of peripherals relying on DMA for data transfer.</P>
<P class=docText><A name="is initiated"></A>DMA is initiated by a DMA master. The PC motherboard has a DMA controller on the South Bridge that can master the I/O bus and initiate DMA to or from a peripheral. This is usually the case for legacy ISA cards. However, buses such as PCI can master the bus and initiate DMA transfers. CardBus cards are similar to PCI and also support DMA mastering. PCMCIA devices, on the other hand, do not support DMA mastering, but the PCMCIA controller, which is usually wired to a PCI bus, might have DMA mastering capabilities.</P>
<P class=docText>The issue of <SPAN class=docEmphasis>cache coherency</SPAN><A name="so data"></A> is synonymous with DMA. For optimum performance, processors cache recently accessed bytes, so data passing between the CPU and main memory streams through the processor cache. During DMA, however, data travels directly between the DMA controller and main memory and, hence, bypasses the processor cache. This evasion has the potential to introduce inconsistencies because the <A name=iddle1137></A><A name=iddle1335></A><A name=iddle1818></A><A name=iddle1827></A><A name=iddle1830></A><A name=iddle1866></A><A name=iddle2932></A><A name=iddle2939></A><A name=iddle3415></A><A name=iddle3423></A><A name=iddle4190></A><A name="data living"></A>processor might work on stale data living in its cache. Some architectures automatically synchronize the cache with main memory using a technique called <SPAN class=docEmphasis>bus snooping.</SPAN><A name="learn how"></A> Many others rely on software to achieve coherency, however. We will learn how to perform coherent DMA operations after introducing a few more topics.</P>
<P class=docText>DMA can occur <SPAN class=docEmphasis>synchronously</SPAN> or <SPAN class=docEmphasis>asynchronously.</SPAN><A name="from a"></A> An example of the former is DMA from a system frame buffer to an LCD controller. A user application writes pixel data to a DMA-mapped frame buffer via <SPAN class=docEmphasis>/dev/fbX</SPAN><A name="synchronously at"></A>, while the LCD controller uses DMA to collect this data synchronously at timed intervals. We discuss more about this in <A class=docLink href="http://www.embeddedlinux.org.cn/EssentialLinuxDeviceDrivers/final/ch12.html#ch12">Chapter 12</A><A name="asynchronous DMA"></A>, "Video Drivers." An example of asynchronous DMA is the transmit and receive of data frames between the CPU and a network card discussed in <A class=docLink href="http://www.embeddedlinux.org.cn/EssentialLinuxDeviceDrivers/final/ch15.html#ch15">Chapter 15</A>, "Network Interface Cards."</P>
<P class=docText><A name="regions that"></A>System memory regions that are the source or destination of DMA transfers are called DMA buffers. If a bus interface has addressing limitations, that'll affect the memory range that can hold DMA buffers. So, DMA buffers suitable for a 24-bit bus such as ISA can live only in the bottom 16MB of system memory called <TT>ZONE_DMA</TT><A name="the section"></A> (see the section "<A class=docLink href="http://www.embeddedlinux.org.cn/EssentialLinuxDeviceDrivers/final/ch02lev1sec7.html#ch02lev1sec7">Allocating Memory</A>" in <A class=docLink href="http://www.embeddedlinux.org.cn/EssentialLinuxDeviceDrivers/final/ch02.html#ch02">Chapter 2</A><A name="the Kernel"></A>, "A Peek Inside the Kernel"). PCI buses are 32-bits wide by default, so you won't usually face such limitations on 32-bit platforms. To inform the kernel about any special needs of DMA-able buffers, use the following:</P>
<DIV class=docText><PRE>dma_set_mask(struct device *dev, u64 mask);</PRE></DIV><BR>
<P class=docText><A name="this function"></A>If this function returns success, you may DMA to any address that is <TT>mask</TT> bits in length. For example, the e1000 PCI-X Gigabit Ethernet driver (<SPAN class=docEmphasis>drivers/net/e1000/e1000_main.c</SPAN>) does the following:</P>
<DIV class=docText><PRE>if (!(err = pci_set_dma_mask(pdev, DMA_64BIT_MASK))) {
   /* System supports 64-bit DMA */
   pci_using_dac = 1;
} else {
  /* See if 32-bit DMA is supported */
  if ((err = pci_set_dma_mask(pdev, DMA_32BIT_MASK))) {
    /* No, let's abort */
    E1000_ERR("No usable DMA configuration, aborting\n");
    return err;
  }
  /* 32-bit DMA */
  pci_using_dac = 0;
}</PRE></DIV><BR>
<P class=docText><A name=iddle1052></A><A name=iddle1345></A><A name=iddle1348></A><A name=iddle1588></A><A name=iddle1819></A><A name=iddle1822></A><A name=iddle1825></A><A name=iddle2244></A><A name=iddle2456></A><A name=iddle2933></A><A name=iddle2935></A><A name=iddle2937></A><A name=iddle3416></A><A name=iddle3419></A><A name=iddle3421></A><A name=iddle3444></A><A name=iddle3877></A><A name="bus controller"></A>I/O devices view DMA buffers through the lens of the bus controller and any intervening <SPAN class=docEmphasis>I/O memory management unit</SPAN> (IOMMU). Because of this, I/O devices work with <SPAN class=docEmphasis>bus</SPAN><A name="virtual addresses"></A> addresses, rather than physical or kernel virtual addresses. So, when you inform a PCI card about the location of a DMA buffer, you have to let it know the buffer's bus address. DMA service routines map the kernel virtual address of DMA buffers to bus addresses so that both the device and the CPU can access the buffers. Bus addresses are of type <TT>dma_addr_t</TT>, defined in <SPAN class=docEmphasis>include/asm-your-arch/types.h</SPAN>.</P>
<P class=docText><A name="One is"></A>There are a couple more concepts worth knowing about DMA. One is the idea of <SPAN class=docEmphasis>bounce buffers.</SPAN><A name="used as"></A> Bounce buffers reside in DMA-able regions and are used as temporary memory when DMA is requested to/from non-DMA-able memory regions. An example is DMA to an address higher than 4GB from a 32-bit PCI peripheral when there is no intervening IOMMU. Data is first transferred to a bounce buffer and then copied to the final destination. The second concept is a flavor of DMA called <SPAN class=docEmphasis>scatter-gather.</SPAN> When data to be DMA'ed is spread over discontinuous regions, scatter<SPAN class=docEmphasis>-</SPAN><A name="scattered buffers"></A>gather capability enables the hardware to gather contents of the scattered buffers at one go. The reverse occurs when data is DMA'ed from the card to buffers scattered in memory. Scatter-gather capability boosts performance by eliminating the need to service multiple DMA requests.</P>
<P class=docText><A name="a healthy"></A>The kernel features a healthy API that masks many of the internal details of configuring DMA. This API gets simpler if you are writing a driver for a PCI card that supports bus mastering. (Most PCI cards do.) PCI DMA routines are essentially wrappers around the generic DMA service routines and are defined in <SPAN class=docEmphasis>include/asm-generic/pci-dma-compat.h.</SPAN><A name="we use"></A> In this chapter, we use only the PCI DMA API.</P>
<P class=docText><A name="The kernel"></A>The kernel provides two classes of DMA service routines to PCI drivers:</P>
<DIV style="FONT-WEIGHT: bold">
<OL class=docList type=1>
<LI>
<DIV style="FONT-WEIGHT: normal">
<P class=docList><SPAN class=docEmphasis>Consistent</SPAN><A name="These routines"></A> (or coherent) DMA access methods. These routines guarantee data coherency in the face of DMA activity. If both the PCI device and the CPU are likely to frequently operate on a DMA buffer, consistency is crucial, so use the consistent API. The trade-off is a degree of performance penalty. To obtain a consistent DMA buffer, call this service routine:</P>
<DIV class=docText><PRE>void *pci_alloc_consistent(struct pci_dev *pdev,
                           size_t size,
                           dma_addr_t *dma_handle);</PRE></DIV>
<P class=docList><A name="function allocates"></A>This function allocates a DMA buffer, generates its bus address, and returns the associated kernel virtual address. The first two arguments respectively hold the PCI device structure (which is discussed later) and the size of the requested <A name=iddle1826></A><A name=iddle2938></A><A name=iddle3422></A><A name=iddle3466></A><A name=iddle3468></A><A name=iddle4168></A>DMA buffer. The third argument, <TT>dma_handle</TT><A name="function call"></A>, is a pointer to the bus address that the function call generates. The following snippet allocates and frees a consistent DMA buffer:</P>
<DIV class=docText><PRE>/* Allocate */
void *vaddr = pci_alloc_consistent(pdev, size,
                                   &amp;dma_handle);
/* Use */
/* ... */
/* Free */
pci_free_consistent(pdev, size, vaddr, dma_handle);</PRE></DIV></DIV></LI>
<LI>
<DIV style="FONT-WEIGHT: normal">
<P class=docList><SPAN class=docEmphasis>Streaming</SPAN><A name="not guarantee"></A> DMA access methods. These routines do not guarantee consistency and are faster as a result. They are useful when there is not much need for shared access between the CPU and the I/O device. When a streamed buffer has been mapped for device access, the driver has to explicitly unmap (or sync) it before the CPU can reliably operate on it. There are two families of streaming access routines: <TT>pci_[map|unmap|dma_sync]_single()</TT> and <TT>pci_[map|unmap|dma_sync]_sg()</TT>.</P>
<P class=docList><A name="DMA buffer"></A>The first function family maps, unmaps, and synchronizes a single preallocated DMA buffer. <TT>pci_map_single()</TT> is prototyped as follows:</P>
<DIV class=docText><PRE>dma_addr_t pci_map_single(struct pci_dev *pdev, void *ptr,
                          size_t size, int direction);</PRE></DIV>
<P class=docList><A name="PCI device"></A>The first three arguments respectively hold the PCI device structure, the kernel virtual address of a preallocated DMA buffer, and the size of the supplied buffer. The fourth argument, <TT>direction</TT>, can be one of the following: <TT>PCI_DMA_BIDIRECTION</TT>, <TT>PCI_DMA_TODEVICE</TT>, <TT>PCI_DMA_FROMDEVICE</TT>, or <TT>PCI_DMA_NONE</TT><A name="last is"></A>. The names are self-explanatory, but the first option is expensive, and the last is for debugging. We discuss streamed DMA mapping further when we develop an example driver later.</P>
<P class=docList><A name="family maps"></A>The second function family maps, unmaps, and synchronizes a scatter-gather list of DMA buffers. <TT>pci_map_sg()</TT><A name="as follows"></A> is prototyped as follows:</P>
<DIV class=docText><PRE>int pci_map_sg(struct pci_dev *pdev,
               struct scatterlist *sgl,
               int num_entries, int direction);</PRE></DIV>
<P class=docList><A name="The scattered"></A>The scattered list is specified using the second argument, <TT>struct scatterlist</TT>, defined in <SPAN class=docEmphasis>include/asm-your-arch/scatterlist.h.</SPAN> <TT>num_entries</TT><A name="number of"></A> is the number of <A name=iddle2004></A><A name=iddle3424></A><A name="in the"></A>entries in the <TT>scatterlist</TT><A name="that described"></A>. The first and last arguments are the same as that described for <TT>pci_map_single()</TT><A name="of mapped"></A>. The function returns the number of mapped entries:</P>
<DIV class=docText><PRE>num_mapped = pci_map_sg(pdev, sgl, num_entries,
                        PCI_DMA_TODEVICE);
for (i=0; i<NUM_MAPPED; i++)="" {="" sg_dma_address(&sgl[i])="" bus="" address="" entry="" sg_dma_len(&sgl[i])="" returns="" length="" of="" this="" region="" *="" }<="" pre="" the=""></NUM_MAPPED;></PRE></DIV></DIV></LI></OL></DIV>
<P class=docText><A name="help you"></A>Let's summarize the characteristics of coherent and streaming DMA to help you decide their suitability for your usage scenario:</P>
<UL>
<LI>
<P class=docList><A name="expensive to"></A>Coherent mappings are simple to code but expensive to use. Streaming mappings have the reverse characteristic.</P></LI>
<LI>
<P class=docList><A name="device need"></A>Coherent mappings are preferred when both the CPU and the I/O device need to frequently manipulate the DMA buffer. This is usually the case for synchronous DMA. An example is the frame buffer driver mentioned previously, where each DMA operates on the same buffer. Because the CPU and the video controller are constantly accessing the frame buffer, it makes sense to use coherent mappings in this situation.</P></LI>
<LI>
<P class=docList><A name="when the"></A>Use streaming mappings when the I/O device owns the buffer for long durations. Streamed DMA is common for asynchronous operation when each DMA operates on a different buffer. An example is a network driver, where the buffers that carry transmit packets are mapped and unmapped on-the-fly.</P></LI>
<LI>
<P class=docList><A name="are good"></A>DMA descriptors are good candidates for coherent mapping. DMA descriptors contain metadata about DMA buffers such as their address and length and are frequently accessed by both the CPU and the device. Mapping descriptors on-the-fly is expensive because that entails frequent unmappings and remappings (or sync operations).</P></LI></UL>