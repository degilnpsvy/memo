# Documentation/oops-tracing.txt
<P></P>
<P>NOTE: ksymoops is useless on 2.6.&nbsp; Please use the Oops in its original format<BR>(from dmesg, etc).&nbsp; Ignore any references in this or other docs to "decoding<BR>the Oops" or "running it through ksymoops".&nbsp; If you post an Oops from 2.6 that<BR>has been run through ksymoops, people will just tell you to repost it.</P>
<P></P>
<P>Quick Summary<BR>-------------</P>
<P>Find the Oops and send it to the maintainer of the kernel area that seems to be<BR>involved with the problem.&nbsp; Don't worry too much about getting the wrong person.<BR>If you are unsure send it to the person responsible for the code relevant to<BR>what you were doing.&nbsp; If it occurs repeatably try and describe how to recreate<BR>it.&nbsp; That's worth even more than the oops.</P>
<P>If you are totally stumped as to whom to send the report, send it to <BR><A href="mailto:linux-kernel@vger.kernel.org">linux-kernel@vger.kernel.org</A>. Thanks for your help in making Linux as<BR>stable as humanly possible.</P>
<P>Where is the Oops?<BR>----------------------</P>
<P>Normally the Oops text is read from the kernel buffers by klogd and<BR>handed to syslogd which writes it to a syslog file, typically<BR>/var/log/messages (depends on /etc/syslog.conf).&nbsp; Sometimes klogd dies,<BR>in which case you can run dmesg &gt; file to read the data from the kernel<BR>buffers and save it.&nbsp; Or you can cat /proc/kmsg &gt; file, however you<BR>have to break in to stop the transfer, kmsg is a "never ending file".<BR>If the machine has crashed so badly that you cannot enter commands or<BR>the disk is not available then you have three options :-</P>
<P>(1) Hand copy the text from the screen and type it in after the machine<BR>&nbsp;&nbsp;&nbsp; has restarted.&nbsp; Messy but it is the only option if you have not<BR>&nbsp;&nbsp;&nbsp; planned for a crash. Alternatively, you can take a picture of<BR>&nbsp;&nbsp;&nbsp; the screen with a digital camera - not nice, but better than<BR>&nbsp;&nbsp;&nbsp; nothing.&nbsp; If the messages scroll off the top of the console, you<BR>&nbsp;&nbsp;&nbsp; may find that booting with a higher resolution (eg, vga=791)<BR>&nbsp;&nbsp;&nbsp; will allow you to read more of the text. (Caveat: This needs vesafb,<BR>&nbsp;&nbsp;&nbsp; so won't help for 'early' oopses)</P>
<P>(2) Boot with a serial console (see Documentation/serial-console.txt),<BR>&nbsp;&nbsp;&nbsp; run a null modem to a second machine and capture the output there<BR>&nbsp;&nbsp;&nbsp; using your favourite communication program.&nbsp; Minicom works well.</P>
<P>(3) Use Kdump (see Documentation/kdump/kdump.txt),<BR>&nbsp;&nbsp;&nbsp; extract the kernel ring buffer from old memory with using dmesg<BR>&nbsp;&nbsp;&nbsp; gdbmacro in Documentation/kdump/gdbmacros.txt.</P>
<P><BR>Full Information<BR>----------------</P>
<P>NOTE: the message from Linus below applies to 2.4 kernel.&nbsp; I have preserved it<BR>for historical reasons, and because some of the information in it still<BR>applies.&nbsp; Especially, please ignore any references to ksymoops. </P>
<P>From: Linus Torvalds &lt;<A href="mailto:torvalds@osdl.org">torvalds@osdl.org</A>&gt;</P>
<P>How to track down an Oops.. [originally a mail to linux-kernel]</P>
<P>The main trick is having 5 years of experience with those pesky oops <BR>messages ;-)</P>
<P>Actually, there are things you can do that make this easier. I have two <BR>separate approaches:</P>
<P>&nbsp;gdb /usr/src/linux/vmlinux<BR>&nbsp;gdb&gt; disassemble &lt;offending_function&gt;</P>
<P>That's the easy way to find the problem, at least if the bug-report is <BR>well made (like this one was - run through ksymoops to get the <BR>information of which function and the offset in the function that it <BR>happened in).</P>
<P>Oh, it helps if the report happens on a kernel that is compiled with the <BR>same compiler and similar setups.</P>
<P>The other thing to do is disassemble the "Code:" part of the bug report: <BR>ksymoops will do this too with the correct tools, but if you don't have<BR>the tools you can just do a silly program:</P>
<P>&nbsp;char str[] = "\xXX\xXX\xXX...";<BR>&nbsp;main(){}</P>
<P>and compile it with gcc -g and then do "disassemble str" (where the "XX" <BR>stuff are the values reported by the Oops - you can just cut-and-paste <BR>and do a replace of spaces to "\x" - that's what I do, as I'm too lazy <BR>to write a program to automate this all).</P>
<P>Alternatively, you can use the shell script in scripts/decodecode.<BR>Its usage is:&nbsp; decodecode &lt; oops.txt</P>
<P>The hex bytes that follow "Code:" may (in some architectures) have a series<BR>of bytes that precede the current instruction pointer as well as bytes at and<BR>following the current instruction pointer.&nbsp; In some cases, one instruction<BR>byte or word is surrounded by &lt;&gt; or (), as in "&lt;86&gt;" or "(f00d)".&nbsp; These<BR>&lt;&gt; or () markings indicate the current instruction pointer.&nbsp; Example from<BR>i386, split into multiple lines for readability:</P>
<P>Code: f9 0f 8d f9 00 00 00 8d 42 0c e8 dd 26 11 c7 a1 60 ea 2b f9 8b 50 08 a1<BR>64 ea 2b f9 8d 34 82 8b 1e 85 db 74 6d 8b 15 60 ea 2b f9 &lt;8b&gt; 43 04 39 42 54<BR>7e 04 40 89 42 54 8b 43 04 3b 05 00 f6 52 c0</P>
<P>Finally, if you want to see where the code comes from, you can do</P>
<P>&nbsp;cd /usr/src/linux<BR>&nbsp;make fs/buffer.s &nbsp;# or whatever file the bug happened in</P>
<P>and then you get a better idea of what happens than with the gdb <BR>disassembly.</P>
<P>Now, the trick is just then to combine all the data you have: the C <BR>sources (and general knowledge of what it _should_ do), the assembly <BR>listing and the code disassembly (and additionally the register dump you <BR>also get from the "oops" message - that can be useful to see _what_ the <BR>corrupted pointers were, and when you have the assembler listing you can <BR>also match the other registers to whatever C expressions they were used <BR>for).</P>
<P>Essentially, you just look at what doesn't match (in this case it was the <BR>"Code" disassembly that didn't match with what the compiler generated). <BR>Then you need to find out _why_ they don't match. Often it's simple - you <BR>see that the code uses a NULL pointer and then you look at the code and <BR>wonder how the NULL pointer got there, and if it's a valid thing to do <BR>you just check against it..</P>
<P>Now, if somebody gets the idea that this is time-consuming and requires <BR>some small amount of concentration, you're right. Which is why I will <BR>mostly just ignore any panic reports that don't have the symbol table <BR>info etc looked up: it simply gets too hard to look it up (I have some <BR>programs to search for specific patterns in the kernel code segment, and <BR>sometimes I have been able to look up those kinds of panics too, but <BR>that really requires pretty good knowledge of the kernel just to be able <BR>to pick out the right sequences etc..)</P>
<P>_Sometimes_ it happens that I just see the disassembled code sequence <BR>from the panic, and I know immediately where it's coming from. That's when <BR>I get worried that I've been doing this for too long ;-)</P>
<P>&nbsp;&nbsp;Linus</P>
<P><BR>---------------------------------------------------------------------------<BR>Notes on Oops tracing with klogd:</P>
<P>In order to help Linus and the other kernel developers there has been<BR>substantial support incorporated into klogd for processing protection<BR>faults.&nbsp; In order to have full support for address resolution at least<BR>version 1.3-pl3 of the sysklogd package should be used.</P>
<P>When a protection fault occurs the klogd daemon automatically<BR>translates important addresses in the kernel log messages to their<BR>symbolic equivalents.&nbsp; This translated kernel message is then<BR>forwarded through whatever reporting mechanism klogd is using.&nbsp; The<BR>protection fault message can be simply cut out of the message files<BR>and forwarded to the kernel developers.</P>
<P>Two types of address resolution are performed by klogd.&nbsp; The first is<BR>static translation and the second is dynamic translation.&nbsp; Static<BR>translation uses the System.map file in much the same manner that<BR>ksymoops does.&nbsp; In order to do static translation the klogd daemon<BR>must be able to find a system map file at daemon initialization time.<BR>See the klogd man page for information on how klogd searches for map<BR>files.</P>
<P>Dynamic address translation is important when kernel loadable modules<BR>are being used.&nbsp; Since memory for kernel modules is allocated from the<BR>kernel's dynamic memory pools there are no fixed locations for either<BR>the start of the module or for functions and symbols in the module.</P>
<P>The kernel supports system calls which allow a program to determine<BR>which modules are loaded and their location in memory.&nbsp; Using these<BR>system calls the klogd daemon builds a symbol table which can be used<BR>to debug a protection fault which occurs in a loadable kernel module.</P>
<P>At the very minimum klogd will provide the name of the module which<BR>generated the protection fault.&nbsp; There may be additional symbolic<BR>information available if the developer of the loadable module chose to<BR>export symbol information from the module.</P>
<P>Since the kernel module environment can be dynamic there must be a<BR>mechanism for notifying the klogd daemon when a change in module<BR>environment occurs.&nbsp; There are command line options available which<BR>allow klogd to signal the currently executing daemon that symbol<BR>information should be refreshed.&nbsp; See the klogd manual page for more<BR>information.</P>
<P>A patch is included with the sysklogd distribution which modifies the<BR>modules-2.0.0 package to automatically signal klogd whenever a module<BR>is loaded or unloaded.&nbsp; Applying this patch provides essentially<BR>seamless support for debugging protection faults which occur with<BR>kernel loadable modules.</P>
<P>The following is an example of a protection fault in a loadable module<BR>processed by klogd:<BR>---------------------------------------------------------------------------<BR>Aug 29 09:51:01 blizard kernel: Unable to handle kernel paging request at virtual address f15e97cc<BR>Aug 29 09:51:01 blizard kernel: current-&gt;tss.cr3 = 0062d000, %cr3 = 0062d000<BR>Aug 29 09:51:01 blizard kernel: *pde = 00000000<BR>Aug 29 09:51:01 blizard kernel: Oops: 0002<BR>Aug 29 09:51:01 blizard kernel: CPU:&nbsp;&nbsp;&nbsp; 0<BR>Aug 29 09:51:01 blizard kernel: EIP:&nbsp;&nbsp;&nbsp; 0010:[oops:_oops+16/3868]<BR>Aug 29 09:51:01 blizard kernel: EFLAGS: 00010212<BR>Aug 29 09:51:01 blizard kernel: eax: 315e97cc&nbsp;&nbsp; ebx: 003a6f80&nbsp;&nbsp; ecx: 001be77b&nbsp;&nbsp; edx: 00237c0c<BR>Aug 29 09:51:01 blizard kernel: esi: 00000000&nbsp;&nbsp; edi: bffffdb3&nbsp;&nbsp; ebp: 00589f90&nbsp;&nbsp; esp: 00589f8c<BR>Aug 29 09:51:01 blizard kernel: ds: 0018&nbsp;&nbsp; es: 0018&nbsp;&nbsp; fs: 002b&nbsp;&nbsp; gs: 002b&nbsp;&nbsp; ss: 0018<BR>Aug 29 09:51:01 blizard kernel: Process oops_test (pid: 3374, process nr: 21, stackpage=00589000)<BR>Aug 29 09:51:01 blizard kernel: Stack: 315e97cc 00589f98 0100b0b4 bffffed4 0012e38e 00240c64 003a6f80 00000001 <BR>Aug 29 09:51:01 blizard kernel:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 00000000 00237810 bfffff00 0010a7fa 00000003 00000001 00000000 bfffff00 <BR>Aug 29 09:51:01 blizard kernel:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; bffffdb3 bffffed4 ffffffda 0000002b 0007002b 0000002b 0000002b 00000036 <BR>Aug 29 09:51:01 blizard kernel: Call Trace: [oops:_oops_ioctl+48/80] [_sys_ioctl+254/272] [_system_call+82/128] <BR>Aug 29 09:51:01 blizard kernel: Code: c7 00 05 00 00 00 eb 08 90 90 90 90 90 90 90 90 89 ec 5d c3 <BR>---------------------------------------------------------------------------</P>
<P>Dr. G.W. Wettstein&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Oncology Research Div. Computing Facility<BR>Roger Maris Cancer Center&nbsp;&nbsp;&nbsp; INTERNET: <A href="mailto:greg@wind.rmcc.com">greg@wind.rmcc.com</A><BR>820 4th St. N.<BR>Fargo, ND&nbsp; 58122<BR>Phone: 701-234-7556</P>
<P><BR>---------------------------------------------------------------------------<BR>Tainted kernels:</P>
<P>Some oops reports contain the string 'Tainted: ' after the program<BR>counter. This indicates that the kernel has been tainted by some<BR>mechanism.&nbsp; The string is followed by a series of position-sensitive<BR>characters, each representing a particular tainted value.</P>
<P>&nbsp; 1: 'G' if all modules loaded have a GPL or compatible license, 'P' if<BR>&nbsp;&nbsp;&nbsp;&nbsp; any proprietary module has been loaded.&nbsp; Modules without a<BR>&nbsp;&nbsp;&nbsp;&nbsp; MODULE_LICENSE or with a MODULE_LICENSE that is not recognised by<BR>&nbsp;&nbsp;&nbsp;&nbsp; insmod as GPL compatible are assumed to be proprietary.</P>
<P>&nbsp; 2: 'F' if any module was force loaded by "insmod -f", ' ' if all<BR>&nbsp;&nbsp;&nbsp;&nbsp; modules were loaded normally.</P>
<P>&nbsp; 3: 'S' if the oops occurred on an SMP kernel running on hardware that<BR>&nbsp;&nbsp;&nbsp;&nbsp; hasn't been certified as safe to run multiprocessor.<BR>&nbsp;&nbsp;&nbsp;&nbsp; Currently this occurs only on various Athlons that are not<BR>&nbsp;&nbsp;&nbsp;&nbsp; SMP capable.</P>
<P>&nbsp; 4: 'R' if a module was force unloaded by "rmmod -f", ' ' if all<BR>&nbsp;&nbsp;&nbsp;&nbsp; modules were unloaded normally.</P>
<P>&nbsp; 5: 'M' if any processor has reported a Machine Check Exception,<BR>&nbsp;&nbsp;&nbsp;&nbsp; ' ' if no Machine Check Exceptions have occurred.</P>
<P>&nbsp; 6: 'B' if a page-release function has found a bad page reference or<BR>&nbsp;&nbsp;&nbsp;&nbsp; some unexpected page flags.</P>
<P>&nbsp; 7: 'U' if a user or user application specifically requested that the<BR>&nbsp;&nbsp;&nbsp;&nbsp; Tainted flag be set, ' ' otherwise.</P>
<P>&nbsp; 8: 'D' if the kernel has died recently, i.e. there was an OOPS or BUG.</P>
<P>&nbsp; 9: 'A' if the ACPI table has been overridden.</P>
<P>&nbsp;10: 'W' if a warning has previously been issued by the kernel.<BR>&nbsp;&nbsp;&nbsp;&nbsp; (Though some warnings may set more specific taint flags.)</P>
<P>&nbsp;11: 'C' if a staging driver has been loaded.</P>
<P>&nbsp;12: 'I' if the kernel is working around a severe bug in the platform<BR>&nbsp;&nbsp;&nbsp;&nbsp; firmware (BIOS or similar).</P>
<P>&nbsp;13: 'O' if an externally-built ("out-of-tree") module has been loaded.</P>
<P>The primary reason for the 'Tainted: ' string is to tell kernel<BR>debuggers if this is a clean kernel or if anything unusual has<BR>occurred.&nbsp; Tainting is permanent: even if an offending module is<BR>unloaded, the tainted value remains to indicate that the kernel is not<BR>trustworthy.