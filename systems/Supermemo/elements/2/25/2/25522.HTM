include/linux/mm.h</P>
<P>#ifndef _LINUX_MM_H<BR>#define _LINUX_MM_H</P>
<P></P>
<P>#include &lt;linux/errno.h&gt;</P>
<P>#ifdef __KERNEL__</P>
<P>#include &lt;linux/mmdebug.h&gt;<BR>#include &lt;linux/gfp.h&gt;<BR>#include &lt;linux/bug.h&gt;<BR>#include &lt;linux/list.h&gt;<BR>#include &lt;linux/mmzone.h&gt;<BR>#include &lt;linux/rbtree.h&gt;<BR>#include &lt;linux/atomic.h&gt;<BR>#include &lt;linux/debug_locks.h&gt;<BR>#include &lt;linux/mm_types.h&gt;<BR>#include &lt;linux/range.h&gt;<BR>#include &lt;linux/pfn.h&gt;<BR>#include &lt;linux/bit_spinlock.h&gt;<BR>#include &lt;linux/shrinker.h&gt;<BR>#include &lt;linux/resource.h&gt;<BR>#include &lt;linux/page_ext.h&gt;</P>
<P>struct mempolicy;<BR>struct anon_vma;<BR>struct anon_vma_chain;<BR>struct file_ra_state;<BR>struct user_struct;<BR>struct writeback_control;<BR>struct bdi_writeback;</P>
<P>#ifndef CONFIG_NEED_MULTIPLE_NODES&nbsp;/* Don't use mapnrs, do it properly */<BR>extern unsigned long max_mapnr;</P>
<P>static inline void set_max_mapnr(unsigned long limit)<BR>{<BR>&nbsp;max_mapnr = limit;<BR>}<BR>#else<BR>static inline void set_max_mapnr(unsigned long limit) { }<BR>#endif</P>
<P>extern unsigned long totalram_pages;<BR>extern void * high_memory;<BR>extern int page_cluster;</P>
<P>#ifdef CONFIG_SYSCTL<BR>extern int sysctl_legacy_va_layout;<BR>#else<BR>#define sysctl_legacy_va_layout 0<BR>#endif</P>
<P>#include &lt;asm/page.h&gt;<BR>#include &lt;asm/pgtable.h&gt;<BR>#include &lt;asm/processor.h&gt;</P>
<P>#ifndef __pa_symbol<BR>#define __pa_symbol(x)&nbsp; __pa(RELOC_HIDE((unsigned long)(x), 0))<BR>#endif</P>
<P>/*<BR>&nbsp;* To prevent common memory management code establishing<BR>&nbsp;* a zero page mapping on a read fault.<BR>&nbsp;* This macro should be defined within &lt;asm/pgtable.h&gt;.<BR>&nbsp;* s390 does this to prevent multiplexing of hardware bits<BR>&nbsp;* related to the physical page in case of virtualization.<BR>&nbsp;*/<BR>#ifndef mm_forbids_zeropage<BR>#define mm_forbids_zeropage(X)&nbsp;(0)<BR>#endif</P>
<P>extern unsigned long sysctl_user_reserve_kbytes;<BR>extern unsigned long sysctl_admin_reserve_kbytes;</P>
<P>extern int sysctl_overcommit_memory;<BR>extern int sysctl_overcommit_ratio;<BR>extern unsigned long sysctl_overcommit_kbytes;</P>
<P>extern int overcommit_ratio_handler(struct ctl_table *, int, void __user *,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; size_t *, loff_t *);<BR>extern int overcommit_kbytes_handler(struct ctl_table *, int, void __user *,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; size_t *, loff_t *);</P>
<P>#define nth_page(page,n) pfn_to_page(page_to_pfn((page)) + (n))</P>
<P>/* to align the pointer to the (next) page boundary */<BR>#define PAGE_ALIGN(addr) ALIGN(addr, PAGE_SIZE)</P>
<P>/* test whether an address (unsigned long or pointer) is aligned to PAGE_SIZE */<BR>#define PAGE_ALIGNED(addr)&nbsp;IS_ALIGNED((unsigned long)addr, PAGE_SIZE)</P>
<P>/*<BR>&nbsp;* Linux kernel virtual memory manager primitives.<BR>&nbsp;* The idea being to have a "virtual" mm in the same way<BR>&nbsp;* we have a virtual fs - giving a cleaner interface to the<BR>&nbsp;* mm details, and allowing different kinds of memory mappings<BR>&nbsp;* (from shared memory to executable loading to arbitrary<BR>&nbsp;* mmap() functions).<BR>&nbsp;*/</P>
<P>extern struct kmem_cache *vm_area_cachep;</P>
<P>#ifndef CONFIG_MMU<BR>extern struct rb_root nommu_region_tree;<BR>extern struct rw_semaphore nommu_region_sem;</P>
<P>extern unsigned int kobjsize(const void *objp);<BR>#endif</P>
<P>/*<BR>&nbsp;* vm_flags in vm_area_struct, see mm_types.h.<BR>&nbsp;*/<BR>#define VM_NONE&nbsp;&nbsp;0x00000000</P>
<P>#define VM_READ&nbsp;&nbsp;0x00000001&nbsp;/* currently active flags */<BR>#define VM_WRITE&nbsp;0x00000002<BR>#define VM_EXEC&nbsp;&nbsp;0x00000004<BR>#define VM_SHARED&nbsp;0x00000008</P>
<P>/* mprotect() hardcodes VM_MAYREAD &gt;&gt; 4 == VM_READ, and so for r/w/x bits. */<BR>#define VM_MAYREAD&nbsp;0x00000010&nbsp;/* limits for mprotect() etc */<BR>#define VM_MAYWRITE&nbsp;0x00000020<BR>#define VM_MAYEXEC&nbsp;0x00000040<BR>#define VM_MAYSHARE&nbsp;0x00000080</P>
<P>#define VM_GROWSDOWN&nbsp;0x00000100&nbsp;/* general info on the segment */<BR>#define VM_PFNMAP&nbsp;0x00000400&nbsp;/* Page-ranges managed without "struct page", just pure PFN */<BR>#define VM_DENYWRITE&nbsp;0x00000800&nbsp;/* ETXTBSY on write attempts.. */</P>
<P>#define VM_LOCKED&nbsp;0x00002000<BR>#define VM_IO&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0x00004000&nbsp;/* Memory mapped I/O or similar */</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;/* Used by sys_madvise() */<BR>#define VM_SEQ_READ&nbsp;0x00008000&nbsp;/* App will access data sequentially */<BR>#define VM_RAND_READ&nbsp;0x00010000&nbsp;/* App will not benefit from clustered reads */</P>
<P>#define VM_DONTCOPY&nbsp;0x00020000&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; /* Do not copy this vma on fork */<BR>#define VM_DONTEXPAND&nbsp;0x00040000&nbsp;/* Cannot expand with mremap() */<BR>#define VM_ACCOUNT&nbsp;0x00100000&nbsp;/* Is a VM accounted object */<BR>#define VM_NORESERVE&nbsp;0x00200000&nbsp;/* should the VM suppress accounting */<BR>#define VM_HUGETLB&nbsp;0x00400000&nbsp;/* Huge TLB Page VM */<BR>#define VM_ARCH_1&nbsp;0x01000000&nbsp;/* Architecture-specific flag */<BR>#define VM_ARCH_2&nbsp;0x02000000<BR>#define VM_DONTDUMP&nbsp;0x04000000&nbsp;/* Do not include in the core dump */</P>
<P>#ifdef CONFIG_MEM_SOFT_DIRTY<BR># define VM_SOFTDIRTY&nbsp;0x08000000&nbsp;/* Not soft dirty clean area */<BR>#else<BR># define VM_SOFTDIRTY&nbsp;0<BR>#endif</P>
<P>#define VM_MIXEDMAP&nbsp;0x10000000&nbsp;/* Can contain "struct page" and pure PFN pages */<BR>#define VM_HUGEPAGE&nbsp;0x20000000&nbsp;/* MADV_HUGEPAGE marked this vma */<BR>#define VM_NOHUGEPAGE&nbsp;0x40000000&nbsp;/* MADV_NOHUGEPAGE marked this vma */<BR>#define VM_MERGEABLE&nbsp;0x80000000&nbsp;/* KSM may merge identical pages */</P>
<P>#if defined(CONFIG_X86)<BR># define VM_PAT&nbsp;&nbsp;VM_ARCH_1&nbsp;/* PAT reserves whole VMA at once (x86) */<BR>#elif defined(CONFIG_PPC)<BR># define VM_SAO&nbsp;&nbsp;VM_ARCH_1&nbsp;/* Strong Access Ordering (powerpc) */<BR>#elif defined(CONFIG_PARISC)<BR># define VM_GROWSUP&nbsp;VM_ARCH_1<BR>#elif defined(CONFIG_METAG)<BR># define VM_GROWSUP&nbsp;VM_ARCH_1<BR>#elif defined(CONFIG_IA64)<BR># define VM_GROWSUP&nbsp;VM_ARCH_1<BR>#elif !defined(CONFIG_MMU)<BR># define VM_MAPPED_COPY&nbsp;VM_ARCH_1&nbsp;/* T if mapped copy of data (nommu mmap) */<BR>#endif</P>
<P>#if defined(CONFIG_X86)<BR>/* MPX specific bounds table or bounds directory */<BR># define VM_MPX&nbsp;&nbsp;VM_ARCH_2<BR>#endif</P>
<P>#ifndef VM_GROWSUP<BR># define VM_GROWSUP&nbsp;VM_NONE<BR>#endif</P>
<P>/* Bits set in the VMA until the stack is in its final location */<BR>#define VM_STACK_INCOMPLETE_SETUP&nbsp;(VM_RAND_READ | VM_SEQ_READ)</P>
<P>#ifndef VM_STACK_DEFAULT_FLAGS&nbsp;&nbsp;/* arch can override this */<BR>#define VM_STACK_DEFAULT_FLAGS VM_DATA_DEFAULT_FLAGS<BR>#endif</P>
<P>#ifdef CONFIG_STACK_GROWSUP<BR>#define VM_STACK_FLAGS&nbsp;(VM_GROWSUP | VM_STACK_DEFAULT_FLAGS | VM_ACCOUNT)<BR>#else<BR>#define VM_STACK_FLAGS&nbsp;(VM_GROWSDOWN | VM_STACK_DEFAULT_FLAGS | VM_ACCOUNT)<BR>#endif</P>
<P>/*<BR>&nbsp;* Special vmas that are non-mergable, non-mlock()able.<BR>&nbsp;* Note: mm/huge_memory.c VM_NO_THP depends on this definition.<BR>&nbsp;*/<BR>#define VM_SPECIAL (VM_IO | VM_DONTEXPAND | VM_PFNMAP | VM_MIXEDMAP)</P>
<P>/* This mask defines which mm-&gt;def_flags a process can inherit its parent */<BR>#define VM_INIT_DEF_MASK&nbsp;VM_NOHUGEPAGE</P>
<P>/*<BR>&nbsp;* mapping from the currently active vm_flags protection bits (the<BR>&nbsp;* low four bits) to a page protection mask..<BR>&nbsp;*/<BR>extern pgprot_t protection_map[16];</P>
<P>#define FAULT_FLAG_WRITE&nbsp;0x01&nbsp;/* Fault was a write access */<BR>#define FAULT_FLAG_MKWRITE&nbsp;0x02&nbsp;/* Fault was mkwrite of existing pte */<BR>#define FAULT_FLAG_ALLOW_RETRY&nbsp;0x04&nbsp;/* Retry fault if blocking */<BR>#define FAULT_FLAG_RETRY_NOWAIT&nbsp;0x08&nbsp;/* Don't drop mmap_sem and wait when retrying */<BR>#define FAULT_FLAG_KILLABLE&nbsp;0x10&nbsp;/* The fault task is in SIGKILL killable region */<BR>#define FAULT_FLAG_TRIED&nbsp;0x20&nbsp;/* Second try */<BR>#define FAULT_FLAG_USER&nbsp;&nbsp;0x40&nbsp;/* The fault originated in userspace */</P>
<P>/*<BR>&nbsp;* vm_fault is filled by the the pagefault handler and passed to the vma's<BR>&nbsp;* -&gt;fault function. The vma's -&gt;fault is responsible for returning a bitmask<BR>&nbsp;* of VM_FAULT_xxx flags that give details about how the fault was handled.<BR>&nbsp;*<BR>&nbsp;* pgoff should be used in favour of virtual_address, if possible.<BR>&nbsp;*/<BR>struct vm_fault {<BR>&nbsp;unsigned int flags;&nbsp;&nbsp;/* FAULT_FLAG_xxx flags */<BR>&nbsp;pgoff_t pgoff;&nbsp;&nbsp;&nbsp;/* Logical page offset based on vma */<BR>&nbsp;void __user *virtual_address;&nbsp;/* Faulting virtual address */</P>
<P>&nbsp;struct page *cow_page;&nbsp;&nbsp;/* Handler may choose to COW */<BR>&nbsp;struct page *page;&nbsp;&nbsp;/* -&gt;fault handlers should return a<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; * page here, unless VM_FAULT_NOPAGE<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; * is set (which is also implied by<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; * VM_FAULT_ERROR).<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; */<BR>&nbsp;/* for -&gt;map_pages() only */<BR>&nbsp;pgoff_t max_pgoff;&nbsp;&nbsp;/* map pages for offset from pgoff till<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; * max_pgoff inclusive */<BR>&nbsp;pte_t *pte;&nbsp;&nbsp;&nbsp;/* pte entry associated with -&gt;pgoff */<BR>};</P>
<P>/*<BR>&nbsp;* These are the virtual MM functions - opening of an area, closing and<BR>&nbsp;* unmapping it (needed to keep files on disk up-to-date etc), pointer<BR>&nbsp;* to the functions called when a no-page or a wp-page exception occurs. <BR>&nbsp;*/<BR>struct vm_operations_struct {<BR>&nbsp;void (*open)(struct vm_area_struct * area);<BR>&nbsp;void (*close)(struct vm_area_struct * area);<BR>&nbsp;int (*fault)(struct vm_area_struct *vma, struct vm_fault *vmf);<BR>&nbsp;void (*map_pages)(struct vm_area_struct *vma, struct vm_fault *vmf);</P>
<P>&nbsp;/* notification that a previously read-only page is about to become<BR>&nbsp; * writable, if an error is returned it will cause a SIGBUS */<BR>&nbsp;int (*page_mkwrite)(struct vm_area_struct *vma, struct vm_fault *vmf);</P>
<P>&nbsp;/* same as page_mkwrite when using VM_PFNMAP|VM_MIXEDMAP */<BR>&nbsp;int (*pfn_mkwrite)(struct vm_area_struct *vma, struct vm_fault *vmf);</P>
<P>&nbsp;/* called by access_process_vm when get_user_pages() fails, typically<BR>&nbsp; * for use by special VMAs that can switch between memory and hardware<BR>&nbsp; */<BR>&nbsp;int (*access)(struct vm_area_struct *vma, unsigned long addr,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; void *buf, int len, int write);</P>
<P>&nbsp;/* Called by the /proc/PID/maps code to ask the vma whether it<BR>&nbsp; * has a special name.&nbsp; Returning non-NULL will also cause this<BR>&nbsp; * vma to be dumped unconditionally. */<BR>&nbsp;const char *(*name)(struct vm_area_struct *vma);</P>
<P>#ifdef CONFIG_NUMA<BR>&nbsp;/*<BR>&nbsp; * set_policy() op must add a reference to any non-NULL @new mempolicy<BR>&nbsp; * to hold the policy upon return.&nbsp; Caller should pass NULL @new to<BR>&nbsp; * remove a policy and fall back to surrounding context--i.e. do not<BR>&nbsp; * install a MPOL_DEFAULT policy, nor the task or system default<BR>&nbsp; * mempolicy.<BR>&nbsp; */<BR>&nbsp;int (*set_policy)(struct vm_area_struct *vma, struct mempolicy *new);</P>
<P>&nbsp;/*<BR>&nbsp; * get_policy() op must add reference [mpol_get()] to any policy at<BR>&nbsp; * (vma,addr) marked as MPOL_SHARED.&nbsp; The shared policy infrastructure<BR>&nbsp; * in mm/mempolicy.c will do this automatically.<BR>&nbsp; * get_policy() must NOT add a ref if the policy at (vma,addr) is not<BR>&nbsp; * marked as MPOL_SHARED. vma policies are protected by the mmap_sem.<BR>&nbsp; * If no [shared/vma] mempolicy exists at the addr, get_policy() op<BR>&nbsp; * must return NULL--i.e., do not "fallback" to task or system default<BR>&nbsp; * policy.<BR>&nbsp; */<BR>&nbsp;struct mempolicy *(*get_policy)(struct vm_area_struct *vma,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;unsigned long addr);<BR>#endif<BR>&nbsp;/*<BR>&nbsp; * Called by vm_normal_page() for special PTEs to find the<BR>&nbsp; * page for @addr.&nbsp; This is useful if the default behavior<BR>&nbsp; * (using pte_page()) would not find the correct page.<BR>&nbsp; */<BR>&nbsp;struct page *(*find_special_page)(struct vm_area_struct *vma,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; unsigned long addr);<BR>};</P>
<P>struct mmu_gather;<BR>struct inode;</P>
<P>#define page_private(page)&nbsp;&nbsp;((page)-&gt;private)<BR>#define set_page_private(page, v)&nbsp;((page)-&gt;private = (v))</P>
<P>/* It's valid only if the page is free path or free_list */<BR>static inline void set_freepage_migratetype(struct page *page, int migratetype)<BR>{<BR>&nbsp;page-&gt;index = migratetype;<BR>}</P>
<P>/* It's valid only if the page is free path or free_list */<BR>static inline int get_freepage_migratetype(struct page *page)<BR>{<BR>&nbsp;return page-&gt;index;<BR>}</P>
<P>/*<BR>&nbsp;* FIXME: take this include out, include page-flags.h in<BR>&nbsp;* files which need it (119 of them)<BR>&nbsp;*/<BR>#include &lt;linux/page-flags.h&gt;<BR>#include &lt;linux/huge_mm.h&gt;</P>
<P>/*<BR>&nbsp;* Methods to modify the page usage count.<BR>&nbsp;*<BR>&nbsp;* What counts for a page usage:<BR>&nbsp;* - cache mapping&nbsp;&nbsp; (page-&gt;mapping)<BR>&nbsp;* - private data&nbsp;&nbsp;&nbsp; (page-&gt;private)<BR>&nbsp;* - page mapped in a task's page tables, each mapping<BR>&nbsp;*&nbsp;&nbsp; is counted separately<BR>&nbsp;*<BR>&nbsp;* Also, many kernel routines increase the page count before a critical<BR>&nbsp;* routine so they can be sure the page doesn't go away from under them.<BR>&nbsp;*/</P>
<P>/*<BR>&nbsp;* Drop a ref, return true if the refcount fell to zero (the page has no users)<BR>&nbsp;*/<BR>static inline int put_page_testzero(struct page *page)<BR>{<BR>&nbsp;VM_BUG_ON_PAGE(atomic_read(&amp;page-&gt;_count) == 0, page);<BR>&nbsp;return atomic_dec_and_test(&amp;page-&gt;_count);<BR>}</P>
<P>/*<BR>&nbsp;* Try to grab a ref unless the page has a refcount of zero, return false if<BR>&nbsp;* that is the case.<BR>&nbsp;* This can be called when MMU is off so it must not access<BR>&nbsp;* any of the virtual mappings.<BR>&nbsp;*/<BR>static inline int get_page_unless_zero(struct page *page)<BR>{<BR>&nbsp;return atomic_inc_not_zero(&amp;page-&gt;_count);<BR>}</P>
<P>/*<BR>&nbsp;* Try to drop a ref unless the page has a refcount of one, return false if<BR>&nbsp;* that is the case.<BR>&nbsp;* This is to make sure that the refcount won't become zero after this drop.<BR>&nbsp;* This can be called when MMU is off so it must not access<BR>&nbsp;* any of the virtual mappings.<BR>&nbsp;*/<BR>static inline int put_page_unless_one(struct page *page)<BR>{<BR>&nbsp;return atomic_add_unless(&amp;page-&gt;_count, -1, 1);<BR>}</P>
<P>extern int page_is_ram(unsigned long pfn);<BR>extern int region_is_ram(resource_size_t phys_addr, unsigned long size);</P>
<P>/* Support for virtually mapped pages */<BR>struct page *vmalloc_to_page(const void *addr);<BR>unsigned long vmalloc_to_pfn(const void *addr);</P>
<P>/*<BR>&nbsp;* Determine if an address is within the vmalloc range<BR>&nbsp;*<BR>&nbsp;* On nommu, vmalloc/vfree wrap through kmalloc/kfree directly, so there<BR>&nbsp;* is no special casing required.<BR>&nbsp;*/<BR>static inline int is_vmalloc_addr(const void *x)<BR>{<BR>#ifdef CONFIG_MMU<BR>&nbsp;unsigned long addr = (unsigned long)x;</P>
<P>&nbsp;return addr &gt;= VMALLOC_START &amp;&amp; addr &lt; VMALLOC_END;<BR>#else<BR>&nbsp;return 0;<BR>#endif<BR>}<BR>#ifdef CONFIG_MMU<BR>extern int is_vmalloc_or_module_addr(const void *x);<BR>#else<BR>static inline int is_vmalloc_or_module_addr(const void *x)<BR>{<BR>&nbsp;return 0;<BR>}<BR>#endif</P>
<P>extern void kvfree(const void *addr);</P>
<P>static inline void compound_lock(struct page *page)<BR>{<BR>#ifdef CONFIG_TRANSPARENT_HUGEPAGE<BR>&nbsp;VM_BUG_ON_PAGE(PageSlab(page), page);<BR>&nbsp;bit_spin_lock(PG_compound_lock, &amp;page-&gt;flags);<BR>#endif<BR>}</P>
<P>static inline void compound_unlock(struct page *page)<BR>{<BR>#ifdef CONFIG_TRANSPARENT_HUGEPAGE<BR>&nbsp;VM_BUG_ON_PAGE(PageSlab(page), page);<BR>&nbsp;bit_spin_unlock(PG_compound_lock, &amp;page-&gt;flags);<BR>#endif<BR>}</P>
<P>static inline unsigned long compound_lock_irqsave(struct page *page)<BR>{<BR>&nbsp;unsigned long uninitialized_var(flags);<BR>#ifdef CONFIG_TRANSPARENT_HUGEPAGE<BR>&nbsp;local_irq_save(flags);<BR>&nbsp;compound_lock(page);<BR>#endif<BR>&nbsp;return flags;<BR>}</P>
<P>static inline void compound_unlock_irqrestore(struct page *page,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; unsigned long flags)<BR>{<BR>#ifdef CONFIG_TRANSPARENT_HUGEPAGE<BR>&nbsp;compound_unlock(page);<BR>&nbsp;local_irq_restore(flags);<BR>#endif<BR>}</P>
<P>static inline struct page *compound_head_by_tail(struct page *tail)<BR>{<BR>&nbsp;struct page *head = tail-&gt;first_page;</P>
<P>&nbsp;/*<BR>&nbsp; * page-&gt;first_page may be a dangling pointer to an old<BR>&nbsp; * compound page, so recheck that it is still a tail<BR>&nbsp; * page before returning.<BR>&nbsp; */<BR>&nbsp;smp_rmb();<BR>&nbsp;if (likely(PageTail(tail)))<BR>&nbsp;&nbsp;return head;<BR>&nbsp;return tail;<BR>}</P>
<P>/*<BR>&nbsp;* Since either compound page could be dismantled asynchronously in THP<BR>&nbsp;* or we access asynchronously arbitrary positioned struct page, there<BR>&nbsp;* would be tail flag race. To handle this race, we should call<BR>&nbsp;* smp_rmb() before checking tail flag. compound_head_by_tail() did it.<BR>&nbsp;*/<BR>static inline struct page *compound_head(struct page *page)<BR>{<BR>&nbsp;if (unlikely(PageTail(page)))<BR>&nbsp;&nbsp;return compound_head_by_tail(page);<BR>&nbsp;return page;<BR>}</P>
<P>/*<BR>&nbsp;* If we access compound page synchronously such as access to<BR>&nbsp;* allocated page, there is no need to handle tail flag race, so we can<BR>&nbsp;* check tail flag directly without any synchronization primitive.<BR>&nbsp;*/<BR>static inline struct page *compound_head_fast(struct page *page)<BR>{<BR>&nbsp;if (unlikely(PageTail(page)))<BR>&nbsp;&nbsp;return page-&gt;first_page;<BR>&nbsp;return page;<BR>}</P>
<P>/*<BR>&nbsp;* The atomic page-&gt;_mapcount, starts from -1: so that transitions<BR>&nbsp;* both from it and to it can be tracked, using atomic_inc_and_test<BR>&nbsp;* and atomic_add_negative(-1).<BR>&nbsp;*/<BR>static inline void page_mapcount_reset(struct page *page)<BR>{<BR>&nbsp;atomic_set(&amp;(page)-&gt;_mapcount, -1);<BR>}</P>
<P>static inline int page_mapcount(struct page *page)<BR>{<BR>&nbsp;VM_BUG_ON_PAGE(PageSlab(page), page);<BR>&nbsp;return atomic_read(&amp;page-&gt;_mapcount) + 1;<BR>}</P>
<P>static inline int page_count(struct page *page)<BR>{<BR>&nbsp;return atomic_read(&amp;compound_head(page)-&gt;_count);<BR>}</P>
<P>static inline bool __compound_tail_refcounted(struct page *page)<BR>{<BR>&nbsp;return PageAnon(page) &amp;&amp; !PageSlab(page) &amp;&amp; !PageHeadHuge(page);<BR>}</P>
<P>/*<BR>&nbsp;* This takes a head page as parameter and tells if the<BR>&nbsp;* tail page reference counting can be skipped.<BR>&nbsp;*<BR>&nbsp;* For this to be safe, PageSlab and PageHeadHuge must remain true on<BR>&nbsp;* any given page where they return true here, until all tail pins<BR>&nbsp;* have been released.<BR>&nbsp;*/<BR>static inline bool compound_tail_refcounted(struct page *page)<BR>{<BR>&nbsp;VM_BUG_ON_PAGE(!PageHead(page), page);<BR>&nbsp;return __compound_tail_refcounted(page);<BR>}</P>
<P>static inline void get_huge_page_tail(struct page *page)<BR>{<BR>&nbsp;/*<BR>&nbsp; * __split_huge_page_refcount() cannot run from under us.<BR>&nbsp; */<BR>&nbsp;VM_BUG_ON_PAGE(!PageTail(page), page);<BR>&nbsp;VM_BUG_ON_PAGE(page_mapcount(page) &lt; 0, page);<BR>&nbsp;VM_BUG_ON_PAGE(atomic_read(&amp;page-&gt;_count) != 0, page);<BR>&nbsp;if (compound_tail_refcounted(page-&gt;first_page))<BR>&nbsp;&nbsp;atomic_inc(&amp;page-&gt;_mapcount);<BR>}</P>
<P>extern bool __get_page_tail(struct page *page);</P>
<P>static inline void get_page(struct page *page)<BR>{<BR>&nbsp;if (unlikely(PageTail(page)))<BR>&nbsp;&nbsp;if (likely(__get_page_tail(page)))<BR>&nbsp;&nbsp;&nbsp;return;<BR>&nbsp;/*<BR>&nbsp; * Getting a normal page or the head of a compound page<BR>&nbsp; * requires to already have an elevated page-&gt;_count.<BR>&nbsp; */<BR>&nbsp;VM_BUG_ON_PAGE(atomic_read(&amp;page-&gt;_count) &lt;= 0, page);<BR>&nbsp;atomic_inc(&amp;page-&gt;_count);<BR>}</P>
<P>static inline struct page *virt_to_head_page(const void *x)<BR>{<BR>&nbsp;struct page *page = virt_to_page(x);</P>
<P>&nbsp;/*<BR>&nbsp; * We don't need to worry about synchronization of tail flag<BR>&nbsp; * when we call virt_to_head_page() since it is only called for<BR>&nbsp; * already allocated page and this page won't be freed until<BR>&nbsp; * this virt_to_head_page() is finished. So use _fast variant.<BR>&nbsp; */<BR>&nbsp;return compound_head_fast(page);<BR>}</P>
<P>/*<BR>&nbsp;* Setup the page count before being freed into the page allocator for<BR>&nbsp;* the first time (boot or memory hotplug)<BR>&nbsp;*/<BR>static inline void init_page_count(struct page *page)<BR>{<BR>&nbsp;atomic_set(&amp;page-&gt;_count, 1);<BR>}</P>
<P>void put_page(struct page *page);<BR>void put_pages_list(struct list_head *pages);</P>
<P>void split_page(struct page *page, unsigned int order);<BR>int split_free_page(struct page *page);</P>
<P>/*<BR>&nbsp;* Compound pages have a destructor function.&nbsp; Provide a<BR>&nbsp;* prototype for that function and accessor functions.<BR>&nbsp;* These are _only_ valid on the head of a PG_compound page.<BR>&nbsp;*/</P>
<P>static inline void set_compound_page_dtor(struct page *page,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;compound_page_dtor *dtor)<BR>{<BR>&nbsp;page[1].compound_dtor = dtor;<BR>}</P>
<P>static inline compound_page_dtor *get_compound_page_dtor(struct page *page)<BR>{<BR>&nbsp;return page[1].compound_dtor;<BR>}</P>
<P>static inline int compound_order(struct page *page)<BR>{<BR>&nbsp;if (!PageHead(page))<BR>&nbsp;&nbsp;return 0;<BR>&nbsp;return page[1].compound_order;<BR>}</P>
<P>static inline void set_compound_order(struct page *page, unsigned long order)<BR>{<BR>&nbsp;page[1].compound_order = order;<BR>}</P>
<P>#ifdef CONFIG_MMU<BR>/*<BR>&nbsp;* Do pte_mkwrite, but only if the vma says VM_WRITE.&nbsp; We do this when<BR>&nbsp;* servicing faults for write access.&nbsp; In the normal case, do always want<BR>&nbsp;* pte_mkwrite.&nbsp; But get_user_pages can cause write faults for mappings<BR>&nbsp;* that do not have writing enabled, when used by access_process_vm.<BR>&nbsp;*/<BR>static inline pte_t maybe_mkwrite(pte_t pte, struct vm_area_struct *vma)<BR>{<BR>&nbsp;if (likely(vma-&gt;vm_flags &amp; VM_WRITE))<BR>&nbsp;&nbsp;pte = pte_mkwrite(pte);<BR>&nbsp;return pte;<BR>}</P>
<P>void do_set_pte(struct vm_area_struct *vma, unsigned long address,<BR>&nbsp;&nbsp;struct page *page, pte_t *pte, bool write, bool anon);<BR>#endif</P>
<P>/*<BR>&nbsp;* Multiple processes may "see" the same page. E.g. for untouched<BR>&nbsp;* mappings of /dev/null, all processes see the same page full of<BR>&nbsp;* zeroes, and text pages of executables and shared libraries have<BR>&nbsp;* only one copy in memory, at most, normally.<BR>&nbsp;*<BR>&nbsp;* For the non-reserved pages, page_count(page) denotes a reference count.<BR>&nbsp;*&nbsp;&nbsp; page_count() == 0 means the page is free. page-&gt;lru is then used for<BR>&nbsp;*&nbsp;&nbsp; freelist management in the buddy allocator.<BR>&nbsp;*&nbsp;&nbsp; page_count() &gt; 0&nbsp; means the page has been allocated.<BR>&nbsp;*<BR>&nbsp;* Pages are allocated by the slab allocator in order to provide memory<BR>&nbsp;* to kmalloc and kmem_cache_alloc. In this case, the management of the<BR>&nbsp;* page, and the fields in 'struct page' are the responsibility of mm/slab.c<BR>&nbsp;* unless a particular usage is carefully commented. (the responsibility of<BR>&nbsp;* freeing the kmalloc memory is the caller's, of course).<BR>&nbsp;*<BR>&nbsp;* A page may be used by anyone else who does a __get_free_page().<BR>&nbsp;* In this case, page_count still tracks the references, and should only<BR>&nbsp;* be used through the normal accessor functions. The top bits of page-&gt;flags<BR>&nbsp;* and page-&gt;virtual store page management information, but all other fields<BR>&nbsp;* are unused and could be used privately, carefully. The management of this<BR>&nbsp;* page is the responsibility of the one who allocated it, and those who have<BR>&nbsp;* subsequently been given references to it.<BR>&nbsp;*<BR>&nbsp;* The other pages (we may call them "pagecache pages") are completely<BR>&nbsp;* managed by the Linux memory manager: I/O, buffers, swapping etc.<BR>&nbsp;* The following discussion applies only to them.<BR>&nbsp;*<BR>&nbsp;* A pagecache page contains an opaque `private' member, which belongs to the<BR>&nbsp;* page's address_space. Usually, this is the address of a circular list of<BR>&nbsp;* the page's disk buffers. PG_private must be set to tell the VM to call<BR>&nbsp;* into the filesystem to release these pages.<BR>&nbsp;*<BR>&nbsp;* A page may belong to an inode's memory mapping. In this case, page-&gt;mapping<BR>&nbsp;* is the pointer to the inode, and page-&gt;index is the file offset of the page,<BR>&nbsp;* in units of PAGE_CACHE_SIZE.<BR>&nbsp;*<BR>&nbsp;* If pagecache pages are not associated with an inode, they are said to be<BR>&nbsp;* anonymous pages. These may become associated with the swapcache, and in that<BR>&nbsp;* case PG_swapcache is set, and page-&gt;private is an offset into the swapcache.<BR>&nbsp;*<BR>&nbsp;* In either case (swapcache or inode backed), the pagecache itself holds one<BR>&nbsp;* reference to the page. Setting PG_private should also increment the<BR>&nbsp;* refcount. The each user mapping also has a reference to the page.<BR>&nbsp;*<BR>&nbsp;* The pagecache pages are stored in a per-mapping radix tree, which is<BR>&nbsp;* rooted at mapping-&gt;page_tree, and indexed by offset.<BR>&nbsp;* Where 2.4 and early 2.6 kernels kept dirty/clean pages in per-address_space<BR>&nbsp;* lists, we instead now tag pages as dirty/writeback in the radix tree.<BR>&nbsp;*<BR>&nbsp;* All pagecache pages may be subject to I/O:<BR>&nbsp;* - inode pages may need to be read from disk,<BR>&nbsp;* - inode pages which have been modified and are MAP_SHARED may need<BR>&nbsp;*&nbsp;&nbsp; to be written back to the inode on disk,<BR>&nbsp;* - anonymous pages (including MAP_PRIVATE file mappings) which have been<BR>&nbsp;*&nbsp;&nbsp; modified may need to be swapped out to swap space and (later) to be read<BR>&nbsp;*&nbsp;&nbsp; back into memory.<BR>&nbsp;*/</P>
<P>/*<BR>&nbsp;* The zone field is never updated after free_area_init_core()<BR>&nbsp;* sets it, so none of the operations on it need to be atomic.<BR>&nbsp;*/</P>
<P>/* Page flags: | [SECTION] | [NODE] | ZONE | [LAST_CPUPID] | ... | FLAGS | */<BR>#define SECTIONS_PGOFF&nbsp;&nbsp;((sizeof(unsigned long)*8) - SECTIONS_WIDTH)<BR>#define NODES_PGOFF&nbsp;&nbsp;(SECTIONS_PGOFF - NODES_WIDTH)<BR>#define ZONES_PGOFF&nbsp;&nbsp;(NODES_PGOFF - ZONES_WIDTH)<BR>#define LAST_CPUPID_PGOFF&nbsp;(ZONES_PGOFF - LAST_CPUPID_WIDTH)</P>
<P>/*<BR>&nbsp;* Define the bit shifts to access each section.&nbsp; For non-existent<BR>&nbsp;* sections we define the shift as 0; that plus a 0 mask ensures<BR>&nbsp;* the compiler will optimise away reference to them.<BR>&nbsp;*/<BR>#define SECTIONS_PGSHIFT&nbsp;(SECTIONS_PGOFF * (SECTIONS_WIDTH != 0))<BR>#define NODES_PGSHIFT&nbsp;&nbsp;(NODES_PGOFF * (NODES_WIDTH != 0))<BR>#define ZONES_PGSHIFT&nbsp;&nbsp;(ZONES_PGOFF * (ZONES_WIDTH != 0))<BR>#define LAST_CPUPID_PGSHIFT&nbsp;(LAST_CPUPID_PGOFF * (LAST_CPUPID_WIDTH != 0))</P>
<P>/* NODE:ZONE or SECTION:ZONE is used to ID a zone for the buddy allocator */<BR>#ifdef NODE_NOT_IN_PAGE_FLAGS<BR>#define ZONEID_SHIFT&nbsp;&nbsp;(SECTIONS_SHIFT + ZONES_SHIFT)<BR>#define ZONEID_PGOFF&nbsp;&nbsp;((SECTIONS_PGOFF &lt; ZONES_PGOFF)? \<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;SECTIONS_PGOFF : ZONES_PGOFF)<BR>#else<BR>#define ZONEID_SHIFT&nbsp;&nbsp;(NODES_SHIFT + ZONES_SHIFT)<BR>#define ZONEID_PGOFF&nbsp;&nbsp;((NODES_PGOFF &lt; ZONES_PGOFF)? \<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;NODES_PGOFF : ZONES_PGOFF)<BR>#endif</P>
<P>#define ZONEID_PGSHIFT&nbsp;&nbsp;(ZONEID_PGOFF * (ZONEID_SHIFT != 0))</P>
<P>#if SECTIONS_WIDTH+NODES_WIDTH+ZONES_WIDTH &gt; BITS_PER_LONG - NR_PAGEFLAGS<BR>#error SECTIONS_WIDTH+NODES_WIDTH+ZONES_WIDTH &gt; BITS_PER_LONG - NR_PAGEFLAGS<BR>#endif</P>
<P>#define ZONES_MASK&nbsp;&nbsp;((1UL &lt;&lt; ZONES_WIDTH) - 1)<BR>#define NODES_MASK&nbsp;&nbsp;((1UL &lt;&lt; NODES_WIDTH) - 1)<BR>#define SECTIONS_MASK&nbsp;&nbsp;((1UL &lt;&lt; SECTIONS_WIDTH) - 1)<BR>#define LAST_CPUPID_MASK&nbsp;((1UL &lt;&lt; LAST_CPUPID_SHIFT) - 1)<BR>#define ZONEID_MASK&nbsp;&nbsp;((1UL &lt;&lt; ZONEID_SHIFT) - 1)</P>
<P>static inline enum zone_type page_zonenum(const struct page *page)<BR>{<BR>&nbsp;return (page-&gt;flags &gt;&gt; ZONES_PGSHIFT) &amp; ZONES_MASK;<BR>}</P>
<P>#if defined(CONFIG_SPARSEMEM) &amp;&amp; !defined(CONFIG_SPARSEMEM_VMEMMAP)<BR>#define SECTION_IN_PAGE_FLAGS<BR>#endif</P>
<P>/*<BR>&nbsp;* The identification function is mainly used by the buddy allocator for<BR>&nbsp;* determining if two pages could be buddies. We are not really identifying<BR>&nbsp;* the zone since we could be using the section number id if we do not have<BR>&nbsp;* node id available in page flags.<BR>&nbsp;* We only guarantee that it will return the same value for two combinable<BR>&nbsp;* pages in a zone.<BR>&nbsp;*/<BR>static inline int page_zone_id(struct page *page)<BR>{<BR>&nbsp;return (page-&gt;flags &gt;&gt; ZONEID_PGSHIFT) &amp; ZONEID_MASK;<BR>}</P>
<P>static inline int zone_to_nid(struct zone *zone)<BR>{<BR>#ifdef CONFIG_NUMA<BR>&nbsp;return zone-&gt;node;<BR>#else<BR>&nbsp;return 0;<BR>#endif<BR>}</P>
<P>#ifdef NODE_NOT_IN_PAGE_FLAGS<BR>extern int page_to_nid(const struct page *page);<BR>#else<BR>static inline int page_to_nid(const struct page *page)<BR>{<BR>&nbsp;return (page-&gt;flags &gt;&gt; NODES_PGSHIFT) &amp; NODES_MASK;<BR>}<BR>#endif</P>
<P>#ifdef CONFIG_NUMA_BALANCING<BR>static inline int cpu_pid_to_cpupid(int cpu, int pid)<BR>{<BR>&nbsp;return ((cpu &amp; LAST__CPU_MASK) &lt;&lt; LAST__PID_SHIFT) | (pid &amp; LAST__PID_MASK);<BR>}</P>
<P>static inline int cpupid_to_pid(int cpupid)<BR>{<BR>&nbsp;return cpupid &amp; LAST__PID_MASK;<BR>}</P>
<P>static inline int cpupid_to_cpu(int cpupid)<BR>{<BR>&nbsp;return (cpupid &gt;&gt; LAST__PID_SHIFT) &amp; LAST__CPU_MASK;<BR>}</P>
<P>static inline int cpupid_to_nid(int cpupid)<BR>{<BR>&nbsp;return cpu_to_node(cpupid_to_cpu(cpupid));<BR>}</P>
<P>static inline bool cpupid_pid_unset(int cpupid)<BR>{<BR>&nbsp;return cpupid_to_pid(cpupid) == (-1 &amp; LAST__PID_MASK);<BR>}</P>
<P>static inline bool cpupid_cpu_unset(int cpupid)<BR>{<BR>&nbsp;return cpupid_to_cpu(cpupid) == (-1 &amp; LAST__CPU_MASK);<BR>}</P>
<P>static inline bool __cpupid_match_pid(pid_t task_pid, int cpupid)<BR>{<BR>&nbsp;return (task_pid &amp; LAST__PID_MASK) == cpupid_to_pid(cpupid);<BR>}</P>
<P>#define cpupid_match_pid(task, cpupid) __cpupid_match_pid(task-&gt;pid, cpupid)<BR>#ifdef LAST_CPUPID_NOT_IN_PAGE_FLAGS<BR>static inline int page_cpupid_xchg_last(struct page *page, int cpupid)<BR>{<BR>&nbsp;return xchg(&amp;page-&gt;_last_cpupid, cpupid &amp; LAST_CPUPID_MASK);<BR>}</P>
<P>static inline int page_cpupid_last(struct page *page)<BR>{<BR>&nbsp;return page-&gt;_last_cpupid;<BR>}<BR>static inline void page_cpupid_reset_last(struct page *page)<BR>{<BR>&nbsp;page-&gt;_last_cpupid = -1 &amp; LAST_CPUPID_MASK;<BR>}<BR>#else<BR>static inline int page_cpupid_last(struct page *page)<BR>{<BR>&nbsp;return (page-&gt;flags &gt;&gt; LAST_CPUPID_PGSHIFT) &amp; LAST_CPUPID_MASK;<BR>}</P>
<P>extern int page_cpupid_xchg_last(struct page *page, int cpupid);</P>
<P>static inline void page_cpupid_reset_last(struct page *page)<BR>{<BR>&nbsp;int cpupid = (1 &lt;&lt; LAST_CPUPID_SHIFT) - 1;</P>
<P>&nbsp;page-&gt;flags &amp;= ~(LAST_CPUPID_MASK &lt;&lt; LAST_CPUPID_PGSHIFT);<BR>&nbsp;page-&gt;flags |= (cpupid &amp; LAST_CPUPID_MASK) &lt;&lt; LAST_CPUPID_PGSHIFT;<BR>}<BR>#endif /* LAST_CPUPID_NOT_IN_PAGE_FLAGS */<BR>#else /* !CONFIG_NUMA_BALANCING */<BR>static inline int page_cpupid_xchg_last(struct page *page, int cpupid)<BR>{<BR>&nbsp;return page_to_nid(page); /* XXX */<BR>}</P>
<P>static inline int page_cpupid_last(struct page *page)<BR>{<BR>&nbsp;return page_to_nid(page); /* XXX */<BR>}</P>
<P>static inline int cpupid_to_nid(int cpupid)<BR>{<BR>&nbsp;return -1;<BR>}</P>
<P>static inline int cpupid_to_pid(int cpupid)<BR>{<BR>&nbsp;return -1;<BR>}</P>
<P>static inline int cpupid_to_cpu(int cpupid)<BR>{<BR>&nbsp;return -1;<BR>}</P>
<P>static inline int cpu_pid_to_cpupid(int nid, int pid)<BR>{<BR>&nbsp;return -1;<BR>}</P>
<P>static inline bool cpupid_pid_unset(int cpupid)<BR>{<BR>&nbsp;return 1;<BR>}</P>
<P>static inline void page_cpupid_reset_last(struct page *page)<BR>{<BR>}</P>
<P>static inline bool cpupid_match_pid(struct task_struct *task, int cpupid)<BR>{<BR>&nbsp;return false;<BR>}<BR>#endif /* CONFIG_NUMA_BALANCING */</P>
<P>static inline struct zone *page_zone(const struct page *page)<BR>{<BR>&nbsp;return &amp;NODE_DATA(page_to_nid(page))-&gt;node_zones[page_zonenum(page)];<BR>}</P>
<P>#ifdef SECTION_IN_PAGE_FLAGS<BR>static inline void set_page_section(struct page *page, unsigned long section)<BR>{<BR>&nbsp;page-&gt;flags &amp;= ~(SECTIONS_MASK &lt;&lt; SECTIONS_PGSHIFT);<BR>&nbsp;page-&gt;flags |= (section &amp; SECTIONS_MASK) &lt;&lt; SECTIONS_PGSHIFT;<BR>}</P>
<P>static inline unsigned long page_to_section(const struct page *page)<BR>{<BR>&nbsp;return (page-&gt;flags &gt;&gt; SECTIONS_PGSHIFT) &amp; SECTIONS_MASK;<BR>}<BR>#endif</P>
<P>static inline void set_page_zone(struct page *page, enum zone_type zone)<BR>{<BR>&nbsp;page-&gt;flags &amp;= ~(ZONES_MASK &lt;&lt; ZONES_PGSHIFT);<BR>&nbsp;page-&gt;flags |= (zone &amp; ZONES_MASK) &lt;&lt; ZONES_PGSHIFT;<BR>}</P>
<P>static inline void set_page_node(struct page *page, unsigned long node)<BR>{<BR>&nbsp;page-&gt;flags &amp;= ~(NODES_MASK &lt;&lt; NODES_PGSHIFT);<BR>&nbsp;page-&gt;flags |= (node &amp; NODES_MASK) &lt;&lt; NODES_PGSHIFT;<BR>}</P>
<P>static inline void set_page_links(struct page *page, enum zone_type zone,<BR>&nbsp;unsigned long node, unsigned long pfn)<BR>{<BR>&nbsp;set_page_zone(page, zone);<BR>&nbsp;set_page_node(page, node);<BR>#ifdef SECTION_IN_PAGE_FLAGS<BR>&nbsp;set_page_section(page, pfn_to_section_nr(pfn));<BR>#endif<BR>}</P>
<P>/*<BR>&nbsp;* Some inline functions in vmstat.h depend on page_zone()<BR>&nbsp;*/<BR>#include &lt;linux/vmstat.h&gt;</P>
<P>static __always_inline void *lowmem_page_address(const struct page *page)<BR>{<BR>&nbsp;return __va(PFN_PHYS(page_to_pfn(page)));<BR>}</P>
<P>#if defined(CONFIG_HIGHMEM) &amp;&amp; !defined(WANT_PAGE_VIRTUAL)<BR>#define HASHED_PAGE_VIRTUAL<BR>#endif</P>
<P>#if defined(WANT_PAGE_VIRTUAL)<BR>static inline void *page_address(const struct page *page)<BR>{<BR>&nbsp;return page-&gt;virtual;<BR>}<BR>static inline void set_page_address(struct page *page, void *address)<BR>{<BR>&nbsp;page-&gt;virtual = address;<BR>}<BR>#define page_address_init()&nbsp; do { } while(0)<BR>#endif</P>
<P>#if defined(HASHED_PAGE_VIRTUAL)<BR>void *page_address(const struct page *page);<BR>void set_page_address(struct page *page, void *virtual);<BR>void page_address_init(void);<BR>#endif</P>
<P>#if !defined(HASHED_PAGE_VIRTUAL) &amp;&amp; !defined(WANT_PAGE_VIRTUAL)<BR>#define page_address(page) lowmem_page_address(page)<BR>#define set_page_address(page, address)&nbsp; do { } while(0)<BR>#define page_address_init()&nbsp; do { } while(0)<BR>#endif</P>
<P>extern void *page_rmapping(struct page *page);<BR>extern struct anon_vma *page_anon_vma(struct page *page);<BR>extern struct address_space *page_mapping(struct page *page);</P>
<P>extern struct address_space *__page_file_mapping(struct page *);</P>
<P>static inline<BR>struct address_space *page_file_mapping(struct page *page)<BR>{<BR>&nbsp;if (unlikely(PageSwapCache(page)))<BR>&nbsp;&nbsp;return __page_file_mapping(page);</P>
<P>&nbsp;return page-&gt;mapping;<BR>}</P>
<P>/*<BR>&nbsp;* Return the pagecache index of the passed page.&nbsp; Regular pagecache pages<BR>&nbsp;* use -&gt;index whereas swapcache pages use -&gt;private<BR>&nbsp;*/<BR>static inline pgoff_t page_index(struct page *page)<BR>{<BR>&nbsp;if (unlikely(PageSwapCache(page)))<BR>&nbsp;&nbsp;return page_private(page);<BR>&nbsp;return page-&gt;index;<BR>}</P>
<P>extern pgoff_t __page_file_index(struct page *page);</P>
<P>/*<BR>&nbsp;* Return the file index of the page. Regular pagecache pages use -&gt;index<BR>&nbsp;* whereas swapcache pages use swp_offset(-&gt;private)<BR>&nbsp;*/<BR>static inline pgoff_t page_file_index(struct page *page)<BR>{<BR>&nbsp;if (unlikely(PageSwapCache(page)))<BR>&nbsp;&nbsp;return __page_file_index(page);</P>
<P>&nbsp;return page-&gt;index;<BR>}</P>
<P>/*<BR>&nbsp;* Return true if this page is mapped into pagetables.<BR>&nbsp;*/<BR>static inline int page_mapped(struct page *page)<BR>{<BR>&nbsp;return atomic_read(&amp;(page)-&gt;_mapcount) &gt;= 0;<BR>}</P>
<P>/*<BR>&nbsp;* Return true only if the page has been allocated with<BR>&nbsp;* ALLOC_NO_WATERMARKS and the low watermark was not<BR>&nbsp;* met implying that the system is under some pressure.<BR>&nbsp;*/<BR>static inline bool page_is_pfmemalloc(struct page *page)<BR>{<BR>&nbsp;/*<BR>&nbsp; * Page index cannot be this large so this must be<BR>&nbsp; * a pfmemalloc page.<BR>&nbsp; */<BR>&nbsp;return page-&gt;index == -1UL;<BR>}</P>
<P>/*<BR>&nbsp;* Only to be called by the page allocator on a freshly allocated<BR>&nbsp;* page.<BR>&nbsp;*/<BR>static inline void set_page_pfmemalloc(struct page *page)<BR>{<BR>&nbsp;page-&gt;index = -1UL;<BR>}</P>
<P>static inline void clear_page_pfmemalloc(struct page *page)<BR>{<BR>&nbsp;page-&gt;index = 0;<BR>}</P>
<P>/*<BR>&nbsp;* Different kinds of faults, as returned by handle_mm_fault().<BR>&nbsp;* Used to decide whether a process gets delivered SIGBUS or<BR>&nbsp;* just gets major/minor fault counters bumped up.<BR>&nbsp;*/</P>
<P>#define VM_FAULT_MINOR&nbsp;0 /* For backwards compat. Remove me quickly. */</P>
<P>#define VM_FAULT_OOM&nbsp;0x0001<BR>#define VM_FAULT_SIGBUS&nbsp;0x0002<BR>#define VM_FAULT_MAJOR&nbsp;0x0004<BR>#define VM_FAULT_WRITE&nbsp;0x0008&nbsp;/* Special case for get_user_pages */<BR>#define VM_FAULT_HWPOISON 0x0010&nbsp;/* Hit poisoned small page */<BR>#define VM_FAULT_HWPOISON_LARGE 0x0020&nbsp; /* Hit poisoned large page. Index encoded in upper bits */<BR>#define VM_FAULT_SIGSEGV 0x0040</P>
<P>#define VM_FAULT_NOPAGE&nbsp;0x0100&nbsp;/* -&gt;fault installed the pte, not return page */<BR>#define VM_FAULT_LOCKED&nbsp;0x0200&nbsp;/* -&gt;fault locked the returned page */<BR>#define VM_FAULT_RETRY&nbsp;0x0400&nbsp;/* -&gt;fault blocked, must retry */<BR>#define VM_FAULT_FALLBACK 0x0800&nbsp;/* huge page fault failed, fall back to small */</P>
<P>#define VM_FAULT_HWPOISON_LARGE_MASK 0xf000 /* encodes hpage index for large hwpoison */</P>
<P>#define VM_FAULT_ERROR&nbsp;(VM_FAULT_OOM | VM_FAULT_SIGBUS | VM_FAULT_SIGSEGV | \<BR>&nbsp;&nbsp;&nbsp; VM_FAULT_HWPOISON | VM_FAULT_HWPOISON_LARGE | \<BR>&nbsp;&nbsp;&nbsp; VM_FAULT_FALLBACK)</P>
<P>/* Encode hstate index for a hwpoisoned large page */<BR>#define VM_FAULT_SET_HINDEX(x) ((x) &lt;&lt; 12)<BR>#define VM_FAULT_GET_HINDEX(x) (((x) &gt;&gt; 12) &amp; 0xf)</P>
<P>/*<BR>&nbsp;* Can be called by the pagefault handler when it gets a VM_FAULT_OOM.<BR>&nbsp;*/<BR>extern void pagefault_out_of_memory(void);</P>
<P>#define offset_in_page(p)&nbsp;((unsigned long)(p) &amp; ~PAGE_MASK)</P>
<P>/*<BR>&nbsp;* Flags passed to show_mem() and show_free_areas() to suppress output in<BR>&nbsp;* various contexts.<BR>&nbsp;*/<BR>#define SHOW_MEM_FILTER_NODES&nbsp;&nbsp;(0x0001u)&nbsp;/* disallowed nodes */</P>
<P>extern void show_free_areas(unsigned int flags);<BR>extern bool skip_free_areas_node(unsigned int flags, int nid);</P>
<P>int shmem_zero_setup(struct vm_area_struct *);<BR>#ifdef CONFIG_SHMEM<BR>bool shmem_mapping(struct address_space *mapping);<BR>#else<BR>static inline bool shmem_mapping(struct address_space *mapping)<BR>{<BR>&nbsp;return false;<BR>}<BR>#endif</P>
<P>extern int can_do_mlock(void);<BR>extern int user_shm_lock(size_t, struct user_struct *);<BR>extern void user_shm_unlock(size_t, struct user_struct *);</P>
<P>/*<BR>&nbsp;* Parameter block passed down to zap_pte_range in exceptional cases.<BR>&nbsp;*/<BR>struct zap_details {<BR>&nbsp;struct address_space *check_mapping;&nbsp;/* Check page-&gt;mapping if set */<BR>&nbsp;pgoff_t&nbsp;first_index;&nbsp;&nbsp;&nbsp;/* Lowest page-&gt;index to unmap */<BR>&nbsp;pgoff_t last_index;&nbsp;&nbsp;&nbsp;/* Highest page-&gt;index to unmap */<BR>};</P>
<P>struct page *vm_normal_page(struct vm_area_struct *vma, unsigned long addr,<BR>&nbsp;&nbsp;pte_t pte);</P>
<P>int zap_vma_ptes(struct vm_area_struct *vma, unsigned long address,<BR>&nbsp;&nbsp;unsigned long size);<BR>void zap_page_range(struct vm_area_struct *vma, unsigned long address,<BR>&nbsp;&nbsp;unsigned long size, struct zap_details *);<BR>void unmap_vmas(struct mmu_gather *tlb, struct vm_area_struct *start_vma,<BR>&nbsp;&nbsp;unsigned long start, unsigned long end);</P>
<P>/**<BR>&nbsp;* mm_walk - callbacks for walk_page_range<BR>&nbsp;* @pmd_entry: if set, called for each non-empty PMD (3rd-level) entry<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; this handler is required to be able to handle<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; pmd_trans_huge() pmds.&nbsp; They may simply choose to<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; split_huge_page() instead of handling it explicitly.<BR>&nbsp;* @pte_entry: if set, called for each non-empty PTE (4th-level) entry<BR>&nbsp;* @pte_hole: if set, called for each hole at all levels<BR>&nbsp;* @hugetlb_entry: if set, called for each hugetlb entry<BR>&nbsp;* @test_walk: caller specific callback function to determine whether<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; we walk over the current vma or not. A positive returned<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; value means "do page table walk over the current vma,"<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; and a negative one means "abort current page table walk<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; right now." 0 means "skip the current vma."<BR>&nbsp;* @mm:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; mm_struct representing the target process of page table walk<BR>&nbsp;* @vma:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; vma currently walked (NULL if walking outside vmas)<BR>&nbsp;* @private:&nbsp;&nbsp; private data for callbacks' usage<BR>&nbsp;*<BR>&nbsp;* (see the comment on walk_page_range() for more details)<BR>&nbsp;*/<BR>struct mm_walk {<BR>&nbsp;int (*pmd_entry)(pmd_t *pmd, unsigned long addr,<BR>&nbsp;&nbsp;&nbsp; unsigned long next, struct mm_walk *walk);<BR>&nbsp;int (*pte_entry)(pte_t *pte, unsigned long addr,<BR>&nbsp;&nbsp;&nbsp; unsigned long next, struct mm_walk *walk);<BR>&nbsp;int (*pte_hole)(unsigned long addr, unsigned long next,<BR>&nbsp;&nbsp;&nbsp;struct mm_walk *walk);<BR>&nbsp;int (*hugetlb_entry)(pte_t *pte, unsigned long hmask,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; unsigned long addr, unsigned long next,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; struct mm_walk *walk);<BR>&nbsp;int (*test_walk)(unsigned long addr, unsigned long next,<BR>&nbsp;&nbsp;&nbsp;struct mm_walk *walk);<BR>&nbsp;struct mm_struct *mm;<BR>&nbsp;struct vm_area_struct *vma;<BR>&nbsp;void *private;<BR>};</P>
<P>int walk_page_range(unsigned long addr, unsigned long end,<BR>&nbsp;&nbsp;struct mm_walk *walk);<BR>int walk_page_vma(struct vm_area_struct *vma, struct mm_walk *walk);<BR>void free_pgd_range(struct mmu_gather *tlb, unsigned long addr,<BR>&nbsp;&nbsp;unsigned long end, unsigned long floor, unsigned long ceiling);<BR>int copy_page_range(struct mm_struct *dst, struct mm_struct *src,<BR>&nbsp;&nbsp;&nbsp;struct vm_area_struct *vma);<BR>void unmap_mapping_range(struct address_space *mapping,<BR>&nbsp;&nbsp;loff_t const holebegin, loff_t const holelen, int even_cows);<BR>int follow_pfn(struct vm_area_struct *vma, unsigned long address,<BR>&nbsp;unsigned long *pfn);<BR>int follow_phys(struct vm_area_struct *vma, unsigned long address,<BR>&nbsp;&nbsp;unsigned int flags, unsigned long *prot, resource_size_t *phys);<BR>int generic_access_phys(struct vm_area_struct *vma, unsigned long addr,<BR>&nbsp;&nbsp;&nbsp;void *buf, int len, int write);</P>
<P>static inline void unmap_shared_mapping_range(struct address_space *mapping,<BR>&nbsp;&nbsp;loff_t const holebegin, loff_t const holelen)<BR>{<BR>&nbsp;unmap_mapping_range(mapping, holebegin, holelen, 0);<BR>}</P>
<P>extern void truncate_pagecache(struct inode *inode, loff_t new);<BR>extern void truncate_setsize(struct inode *inode, loff_t newsize);<BR>void pagecache_isize_extended(struct inode *inode, loff_t from, loff_t to);<BR>void truncate_pagecache_range(struct inode *inode, loff_t offset, loff_t end);<BR>int truncate_inode_page(struct address_space *mapping, struct page *page);<BR>int generic_error_remove_page(struct address_space *mapping, struct page *page);<BR>int invalidate_inode_page(struct page *page);</P>
<P>#ifdef CONFIG_MMU<BR>extern int handle_mm_fault(struct mm_struct *mm, struct vm_area_struct *vma,<BR>&nbsp;&nbsp;&nbsp;unsigned long address, unsigned int flags);<BR>extern int fixup_user_fault(struct task_struct *tsk, struct mm_struct *mm,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; unsigned long address, unsigned int fault_flags);<BR>#else<BR>static inline int handle_mm_fault(struct mm_struct *mm,<BR>&nbsp;&nbsp;&nbsp;struct vm_area_struct *vma, unsigned long address,<BR>&nbsp;&nbsp;&nbsp;unsigned int flags)<BR>{<BR>&nbsp;/* should never happen if there's no MMU */<BR>&nbsp;BUG();<BR>&nbsp;return VM_FAULT_SIGBUS;<BR>}<BR>static inline int fixup_user_fault(struct task_struct *tsk,<BR>&nbsp;&nbsp;struct mm_struct *mm, unsigned long address,<BR>&nbsp;&nbsp;unsigned int fault_flags)<BR>{<BR>&nbsp;/* should never happen if there's no MMU */<BR>&nbsp;BUG();<BR>&nbsp;return -EFAULT;<BR>}<BR>#endif</P>
<P>extern int access_process_vm(struct task_struct *tsk, unsigned long addr, void *buf, int len, int write);<BR>extern int access_remote_vm(struct mm_struct *mm, unsigned long addr,<BR>&nbsp;&nbsp;void *buf, int len, int write);</P>
<P>long __get_user_pages(struct task_struct *tsk, struct mm_struct *mm,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; unsigned long start, unsigned long nr_pages,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; unsigned int foll_flags, struct page **pages,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; struct vm_area_struct **vmas, int *nonblocking);<BR>long get_user_pages(struct task_struct *tsk, struct mm_struct *mm,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; unsigned long start, unsigned long nr_pages,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; int write, int force, struct page **pages,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; struct vm_area_struct **vmas);<BR>long get_user_pages_locked(struct task_struct *tsk, struct mm_struct *mm,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; unsigned long start, unsigned long nr_pages,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; int write, int force, struct page **pages,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; int *locked);<BR>long __get_user_pages_unlocked(struct task_struct *tsk, struct mm_struct *mm,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; unsigned long start, unsigned long nr_pages,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; int write, int force, struct page **pages,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; unsigned int gup_flags);<BR>long get_user_pages_unlocked(struct task_struct *tsk, struct mm_struct *mm,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; unsigned long start, unsigned long nr_pages,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; int write, int force, struct page **pages);<BR>int get_user_pages_fast(unsigned long start, int nr_pages, int write,<BR>&nbsp;&nbsp;&nbsp;struct page **pages);<BR>struct kvec;<BR>int get_kernel_pages(const struct kvec *iov, int nr_pages, int write,<BR>&nbsp;&nbsp;&nbsp;struct page **pages);<BR>int get_kernel_page(unsigned long start, int write, struct page **pages);<BR>struct page *get_dump_page(unsigned long addr);</P>
<P>extern int try_to_release_page(struct page * page, gfp_t gfp_mask);<BR>extern void do_invalidatepage(struct page *page, unsigned int offset,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; unsigned int length);</P>
<P>int __set_page_dirty_nobuffers(struct page *page);<BR>int __set_page_dirty_no_writeback(struct page *page);<BR>int redirty_page_for_writepage(struct writeback_control *wbc,<BR>&nbsp;&nbsp;&nbsp;&nbsp;struct page *page);<BR>void account_page_dirtied(struct page *page, struct address_space *mapping,<BR>&nbsp;&nbsp;&nbsp;&nbsp; struct mem_cgroup *memcg);<BR>void account_page_cleaned(struct page *page, struct address_space *mapping,<BR>&nbsp;&nbsp;&nbsp;&nbsp; struct mem_cgroup *memcg, struct bdi_writeback *wb);<BR>int set_page_dirty(struct page *page);<BR>int set_page_dirty_lock(struct page *page);<BR>void cancel_dirty_page(struct page *page);<BR>int clear_page_dirty_for_io(struct page *page);</P>
<P>int get_cmdline(struct task_struct *task, char *buffer, int buflen);</P>
<P>/* Is the vma a continuation of the stack vma above it? */<BR>static inline int vma_growsdown(struct vm_area_struct *vma, unsigned long addr)<BR>{<BR>&nbsp;return vma &amp;&amp; (vma-&gt;vm_end == addr) &amp;&amp; (vma-&gt;vm_flags &amp; VM_GROWSDOWN);<BR>}</P>
<P>static inline int stack_guard_page_start(struct vm_area_struct *vma,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; unsigned long addr)<BR>{<BR>&nbsp;return (vma-&gt;vm_flags &amp; VM_GROWSDOWN) &amp;&amp;<BR>&nbsp;&nbsp;(vma-&gt;vm_start == addr) &amp;&amp;<BR>&nbsp;&nbsp;!vma_growsdown(vma-&gt;vm_prev, addr);<BR>}</P>
<P>/* Is the vma a continuation of the stack vma below it? */<BR>static inline int vma_growsup(struct vm_area_struct *vma, unsigned long addr)<BR>{<BR>&nbsp;return vma &amp;&amp; (vma-&gt;vm_start == addr) &amp;&amp; (vma-&gt;vm_flags &amp; VM_GROWSUP);<BR>}</P>
<P>static inline int stack_guard_page_end(struct vm_area_struct *vma,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; unsigned long addr)<BR>{<BR>&nbsp;return (vma-&gt;vm_flags &amp; VM_GROWSUP) &amp;&amp;<BR>&nbsp;&nbsp;(vma-&gt;vm_end == addr) &amp;&amp;<BR>&nbsp;&nbsp;!vma_growsup(vma-&gt;vm_next, addr);<BR>}</P>
<P>extern struct task_struct *task_of_stack(struct task_struct *task,<BR>&nbsp;&nbsp;&nbsp;&nbsp;struct vm_area_struct *vma, bool in_group);</P>
<P>extern unsigned long move_page_tables(struct vm_area_struct *vma,<BR>&nbsp;&nbsp;unsigned long old_addr, struct vm_area_struct *new_vma,<BR>&nbsp;&nbsp;unsigned long new_addr, unsigned long len,<BR>&nbsp;&nbsp;bool need_rmap_locks);<BR>extern unsigned long change_protection(struct vm_area_struct *vma, unsigned long start,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; unsigned long end, pgprot_t newprot,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; int dirty_accountable, int prot_numa);<BR>extern int mprotect_fixup(struct vm_area_struct *vma,<BR>&nbsp;&nbsp;&nbsp;&nbsp; struct vm_area_struct **pprev, unsigned long start,<BR>&nbsp;&nbsp;&nbsp;&nbsp; unsigned long end, unsigned long newflags);</P>
<P>/*<BR>&nbsp;* doesn't attempt to fault and will return short.<BR>&nbsp;*/<BR>int __get_user_pages_fast(unsigned long start, int nr_pages, int write,<BR>&nbsp;&nbsp;&nbsp;&nbsp; struct page **pages);<BR>/*<BR>&nbsp;* per-process(per-mm_struct) statistics.<BR>&nbsp;*/<BR>static inline unsigned long get_mm_counter(struct mm_struct *mm, int member)<BR>{<BR>&nbsp;long val = atomic_long_read(&amp;mm-&gt;rss_stat.count[member]);</P>
<P>#ifdef SPLIT_RSS_COUNTING<BR>&nbsp;/*<BR>&nbsp; * counter is updated in asynchronous manner and may go to minus.<BR>&nbsp; * But it's never be expected number for users.<BR>&nbsp; */<BR>&nbsp;if (val &lt; 0)<BR>&nbsp;&nbsp;val = 0;<BR>#endif<BR>&nbsp;return (unsigned long)val;<BR>}</P>
<P>static inline void add_mm_counter(struct mm_struct *mm, int member, long value)<BR>{<BR>&nbsp;atomic_long_add(value, &amp;mm-&gt;rss_stat.count[member]);<BR>}</P>
<P>static inline void inc_mm_counter(struct mm_struct *mm, int member)<BR>{<BR>&nbsp;atomic_long_inc(&amp;mm-&gt;rss_stat.count[member]);<BR>}</P>
<P>static inline void dec_mm_counter(struct mm_struct *mm, int member)<BR>{<BR>&nbsp;atomic_long_dec(&amp;mm-&gt;rss_stat.count[member]);<BR>}</P>
<P>static inline unsigned long get_mm_rss(struct mm_struct *mm)<BR>{<BR>&nbsp;return get_mm_counter(mm, MM_FILEPAGES) +<BR>&nbsp;&nbsp;get_mm_counter(mm, MM_ANONPAGES);<BR>}</P>
<P>static inline unsigned long get_mm_hiwater_rss(struct mm_struct *mm)<BR>{<BR>&nbsp;return max(mm-&gt;hiwater_rss, get_mm_rss(mm));<BR>}</P>
<P>static inline unsigned long get_mm_hiwater_vm(struct mm_struct *mm)<BR>{<BR>&nbsp;return max(mm-&gt;hiwater_vm, mm-&gt;total_vm);<BR>}</P>
<P>static inline void update_hiwater_rss(struct mm_struct *mm)<BR>{<BR>&nbsp;unsigned long _rss = get_mm_rss(mm);</P>
<P>&nbsp;if ((mm)-&gt;hiwater_rss &lt; _rss)<BR>&nbsp;&nbsp;(mm)-&gt;hiwater_rss = _rss;<BR>}</P>
<P>static inline void update_hiwater_vm(struct mm_struct *mm)<BR>{<BR>&nbsp;if (mm-&gt;hiwater_vm &lt; mm-&gt;total_vm)<BR>&nbsp;&nbsp;mm-&gt;hiwater_vm = mm-&gt;total_vm;<BR>}</P>
<P>static inline void reset_mm_hiwater_rss(struct mm_struct *mm)<BR>{<BR>&nbsp;mm-&gt;hiwater_rss = get_mm_rss(mm);<BR>}</P>
<P>static inline void setmax_mm_hiwater_rss(unsigned long *maxrss,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; struct mm_struct *mm)<BR>{<BR>&nbsp;unsigned long hiwater_rss = get_mm_hiwater_rss(mm);</P>
<P>&nbsp;if (*maxrss &lt; hiwater_rss)<BR>&nbsp;&nbsp;*maxrss = hiwater_rss;<BR>}</P>
<P>#if defined(SPLIT_RSS_COUNTING)<BR>void sync_mm_rss(struct mm_struct *mm);<BR>#else<BR>static inline void sync_mm_rss(struct mm_struct *mm)<BR>{<BR>}<BR>#endif</P>
<P>int vma_wants_writenotify(struct vm_area_struct *vma);</P>
<P>extern pte_t *__get_locked_pte(struct mm_struct *mm, unsigned long addr,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; spinlock_t **ptl);<BR>static inline pte_t *get_locked_pte(struct mm_struct *mm, unsigned long addr,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; spinlock_t **ptl)<BR>{<BR>&nbsp;pte_t *ptep;<BR>&nbsp;__cond_lock(*ptl, ptep = __get_locked_pte(mm, addr, ptl));<BR>&nbsp;return ptep;<BR>}</P>
<P>#ifdef __PAGETABLE_PUD_FOLDED<BR>static inline int __pud_alloc(struct mm_struct *mm, pgd_t *pgd,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;unsigned long address)<BR>{<BR>&nbsp;return 0;<BR>}<BR>#else<BR>int __pud_alloc(struct mm_struct *mm, pgd_t *pgd, unsigned long address);<BR>#endif</P>
<P>#if defined(__PAGETABLE_PMD_FOLDED) || !defined(CONFIG_MMU)<BR>static inline int __pmd_alloc(struct mm_struct *mm, pud_t *pud,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;unsigned long address)<BR>{<BR>&nbsp;return 0;<BR>}</P>
<P>static inline void mm_nr_pmds_init(struct mm_struct *mm) {}</P>
<P>static inline unsigned long mm_nr_pmds(struct mm_struct *mm)<BR>{<BR>&nbsp;return 0;<BR>}</P>
<P>static inline void mm_inc_nr_pmds(struct mm_struct *mm) {}<BR>static inline void mm_dec_nr_pmds(struct mm_struct *mm) {}</P>
<P>#else<BR>int __pmd_alloc(struct mm_struct *mm, pud_t *pud, unsigned long address);</P>
<P>static inline void mm_nr_pmds_init(struct mm_struct *mm)<BR>{<BR>&nbsp;atomic_long_set(&amp;mm-&gt;nr_pmds, 0);<BR>}</P>
<P>static inline unsigned long mm_nr_pmds(struct mm_struct *mm)<BR>{<BR>&nbsp;return atomic_long_read(&amp;mm-&gt;nr_pmds);<BR>}</P>
<P>static inline void mm_inc_nr_pmds(struct mm_struct *mm)<BR>{<BR>&nbsp;atomic_long_inc(&amp;mm-&gt;nr_pmds);<BR>}</P>
<P>static inline void mm_dec_nr_pmds(struct mm_struct *mm)<BR>{<BR>&nbsp;atomic_long_dec(&amp;mm-&gt;nr_pmds);<BR>}<BR>#endif</P>
<P>int __pte_alloc(struct mm_struct *mm, struct vm_area_struct *vma,<BR>&nbsp;&nbsp;pmd_t *pmd, unsigned long address);<BR>int __pte_alloc_kernel(pmd_t *pmd, unsigned long address);</P>
<P>/*<BR>&nbsp;* The following ifdef needed to get the 4level-fixup.h header to work.<BR>&nbsp;* Remove it when 4level-fixup.h has been removed.<BR>&nbsp;*/<BR>#if defined(CONFIG_MMU) &amp;&amp; !defined(__ARCH_HAS_4LEVEL_HACK)<BR>static inline pud_t *pud_alloc(struct mm_struct *mm, pgd_t *pgd, unsigned long address)<BR>{<BR>&nbsp;return (unlikely(pgd_none(*pgd)) &amp;&amp; __pud_alloc(mm, pgd, address))?<BR>&nbsp;&nbsp;NULL: pud_offset(pgd, address);<BR>}</P>
<P>static inline pmd_t *pmd_alloc(struct mm_struct *mm, pud_t *pud, unsigned long address)<BR>{<BR>&nbsp;return (unlikely(pud_none(*pud)) &amp;&amp; __pmd_alloc(mm, pud, address))?<BR>&nbsp;&nbsp;NULL: pmd_offset(pud, address);<BR>}<BR>#endif /* CONFIG_MMU &amp;&amp; !__ARCH_HAS_4LEVEL_HACK */</P>
<P>#if USE_SPLIT_PTE_PTLOCKS<BR>#if ALLOC_SPLIT_PTLOCKS<BR>void __init ptlock_cache_init(void);<BR>extern bool ptlock_alloc(struct page *page);<BR>extern void ptlock_free(struct page *page);</P>
<P>static inline spinlock_t *ptlock_ptr(struct page *page)<BR>{<BR>&nbsp;return page-&gt;ptl;<BR>}<BR>#else /* ALLOC_SPLIT_PTLOCKS */<BR>static inline void ptlock_cache_init(void)<BR>{<BR>}</P>
<P>static inline bool ptlock_alloc(struct page *page)<BR>{<BR>&nbsp;return true;<BR>}</P>
<P>static inline void ptlock_free(struct page *page)<BR>{<BR>}</P>
<P>static inline spinlock_t *ptlock_ptr(struct page *page)<BR>{<BR>&nbsp;return &amp;page-&gt;ptl;<BR>}<BR>#endif /* ALLOC_SPLIT_PTLOCKS */</P>
<P>static inline spinlock_t *pte_lockptr(struct mm_struct *mm, pmd_t *pmd)<BR>{<BR>&nbsp;return ptlock_ptr(pmd_page(*pmd));<BR>}</P>
<P>static inline bool ptlock_init(struct page *page)<BR>{<BR>&nbsp;/*<BR>&nbsp; * prep_new_page() initialize page-&gt;private (and therefore page-&gt;ptl)<BR>&nbsp; * with 0. Make sure nobody took it in use in between.<BR>&nbsp; *<BR>&nbsp; * It can happen if arch try to use slab for page table allocation:<BR>&nbsp; * slab code uses page-&gt;slab_cache and page-&gt;first_page (for tail<BR>&nbsp; * pages), which share storage with page-&gt;ptl.<BR>&nbsp; */<BR>&nbsp;VM_BUG_ON_PAGE(*(unsigned long *)&amp;page-&gt;ptl, page);<BR>&nbsp;if (!ptlock_alloc(page))<BR>&nbsp;&nbsp;return false;<BR>&nbsp;spin_lock_init(ptlock_ptr(page));<BR>&nbsp;return true;<BR>}</P>
<P>/* Reset page-&gt;mapping so free_pages_check won't complain. */<BR>static inline void pte_lock_deinit(struct page *page)<BR>{<BR>&nbsp;page-&gt;mapping = NULL;<BR>&nbsp;ptlock_free(page);<BR>}</P>
<P>#else&nbsp;/* !USE_SPLIT_PTE_PTLOCKS */<BR>/*<BR>&nbsp;* We use mm-&gt;page_table_lock to guard all pagetable pages of the mm.<BR>&nbsp;*/<BR>static inline spinlock_t *pte_lockptr(struct mm_struct *mm, pmd_t *pmd)<BR>{<BR>&nbsp;return &amp;mm-&gt;page_table_lock;<BR>}<BR>static inline void ptlock_cache_init(void) {}<BR>static inline bool ptlock_init(struct page *page) { return true; }<BR>static inline void pte_lock_deinit(struct page *page) {}<BR>#endif /* USE_SPLIT_PTE_PTLOCKS */</P>
<P>static inline void pgtable_init(void)<BR>{<BR>&nbsp;ptlock_cache_init();<BR>&nbsp;pgtable_cache_init();<BR>}</P>
<P>static inline bool pgtable_page_ctor(struct page *page)<BR>{<BR>&nbsp;inc_zone_page_state(page, NR_PAGETABLE);<BR>&nbsp;return ptlock_init(page);<BR>}</P>
<P>static inline void pgtable_page_dtor(struct page *page)<BR>{<BR>&nbsp;pte_lock_deinit(page);<BR>&nbsp;dec_zone_page_state(page, NR_PAGETABLE);<BR>}</P>
<P>#define pte_offset_map_lock(mm, pmd, address, ptlp)&nbsp;\<BR>({&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;spinlock_t *__ptl = pte_lockptr(mm, pmd);&nbsp;\<BR>&nbsp;pte_t *__pte = pte_offset_map(pmd, address);&nbsp;\<BR>&nbsp;*(ptlp) = __ptl;&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;spin_lock(__ptl);&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;__pte;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\<BR>})</P>
<P>#define pte_unmap_unlock(pte, ptl)&nbsp;do {&nbsp;&nbsp;\<BR>&nbsp;spin_unlock(ptl);&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;pte_unmap(pte);&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\<BR>} while (0)</P>
<P>#define pte_alloc_map(mm, vma, pmd, address)&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;((unlikely(pmd_none(*(pmd))) &amp;&amp; __pte_alloc(mm, vma,&nbsp;\<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;pmd, address))?&nbsp;\<BR>&nbsp; NULL: pte_offset_map(pmd, address))</P>
<P>#define pte_alloc_map_lock(mm, pmd, address, ptlp)&nbsp;\<BR>&nbsp;((unlikely(pmd_none(*(pmd))) &amp;&amp; __pte_alloc(mm, NULL,&nbsp;\<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;pmd, address))?&nbsp;\<BR>&nbsp;&nbsp;NULL: pte_offset_map_lock(mm, pmd, address, ptlp))</P>
<P>#define pte_alloc_kernel(pmd, address)&nbsp;&nbsp;&nbsp;\<BR>&nbsp;((unlikely(pmd_none(*(pmd))) &amp;&amp; __pte_alloc_kernel(pmd, address))? \<BR>&nbsp;&nbsp;NULL: pte_offset_kernel(pmd, address))</P>
<P>#if USE_SPLIT_PMD_PTLOCKS</P>
<P>static struct page *pmd_to_page(pmd_t *pmd)<BR>{<BR>&nbsp;unsigned long mask = ~(PTRS_PER_PMD * sizeof(pmd_t) - 1);<BR>&nbsp;return virt_to_page((void *)((unsigned long) pmd &amp; mask));<BR>}</P>
<P>static inline spinlock_t *pmd_lockptr(struct mm_struct *mm, pmd_t *pmd)<BR>{<BR>&nbsp;return ptlock_ptr(pmd_to_page(pmd));<BR>}</P>
<P>static inline bool pgtable_pmd_page_ctor(struct page *page)<BR>{<BR>#ifdef CONFIG_TRANSPARENT_HUGEPAGE<BR>&nbsp;page-&gt;pmd_huge_pte = NULL;<BR>#endif<BR>&nbsp;return ptlock_init(page);<BR>}</P>
<P>static inline void pgtable_pmd_page_dtor(struct page *page)<BR>{<BR>#ifdef CONFIG_TRANSPARENT_HUGEPAGE<BR>&nbsp;VM_BUG_ON_PAGE(page-&gt;pmd_huge_pte, page);<BR>#endif<BR>&nbsp;ptlock_free(page);<BR>}</P>
<P>#define pmd_huge_pte(mm, pmd) (pmd_to_page(pmd)-&gt;pmd_huge_pte)</P>
<P>#else</P>
<P>static inline spinlock_t *pmd_lockptr(struct mm_struct *mm, pmd_t *pmd)<BR>{<BR>&nbsp;return &amp;mm-&gt;page_table_lock;<BR>}</P>
<P>static inline bool pgtable_pmd_page_ctor(struct page *page) { return true; }<BR>static inline void pgtable_pmd_page_dtor(struct page *page) {}</P>
<P>#define pmd_huge_pte(mm, pmd) ((mm)-&gt;pmd_huge_pte)</P>
<P>#endif</P>
<P>static inline spinlock_t *pmd_lock(struct mm_struct *mm, pmd_t *pmd)<BR>{<BR>&nbsp;spinlock_t *ptl = pmd_lockptr(mm, pmd);<BR>&nbsp;spin_lock(ptl);<BR>&nbsp;return ptl;<BR>}</P>
<P>extern void free_area_init(unsigned long * zones_size);<BR>extern void free_area_init_node(int nid, unsigned long * zones_size,<BR>&nbsp;&nbsp;unsigned long zone_start_pfn, unsigned long *zholes_size);<BR>extern void free_initmem(void);</P>
<P>/*<BR>&nbsp;* Free reserved pages within range [PAGE_ALIGN(start), end &amp; PAGE_MASK)<BR>&nbsp;* into the buddy system. The freed pages will be poisoned with pattern<BR>&nbsp;* "poison" if it's within range [0, UCHAR_MAX].<BR>&nbsp;* Return pages freed into the buddy system.<BR>&nbsp;*/<BR>extern unsigned long free_reserved_area(void *start, void *end,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;int poison, char *s);</P>
<P>#ifdef&nbsp;CONFIG_HIGHMEM<BR>/*<BR>&nbsp;* Free a highmem page into the buddy system, adjusting totalhigh_pages<BR>&nbsp;* and totalram_pages.<BR>&nbsp;*/<BR>extern void free_highmem_page(struct page *page);<BR>#endif</P>
<P>extern void adjust_managed_page_count(struct page *page, long count);<BR>extern void mem_init_print_info(const char *str);</P>
<P>extern void reserve_bootmem_region(unsigned long start, unsigned long end);</P>
<P>/* Free the reserved page into the buddy system, so it gets managed. */<BR>static inline void __free_reserved_page(struct page *page)<BR>{<BR>&nbsp;ClearPageReserved(page);<BR>&nbsp;init_page_count(page);<BR>&nbsp;__free_page(page);<BR>}</P>
<P>static inline void free_reserved_page(struct page *page)<BR>{<BR>&nbsp;__free_reserved_page(page);<BR>&nbsp;adjust_managed_page_count(page, 1);<BR>}</P>
<P>static inline void mark_page_reserved(struct page *page)<BR>{<BR>&nbsp;SetPageReserved(page);<BR>&nbsp;adjust_managed_page_count(page, -1);<BR>}</P>
<P>/*<BR>&nbsp;* Default method to free all the __init memory into the buddy system.<BR>&nbsp;* The freed pages will be poisoned with pattern "poison" if it's within<BR>&nbsp;* range [0, UCHAR_MAX].<BR>&nbsp;* Return pages freed into the buddy system.<BR>&nbsp;*/<BR>static inline unsigned long free_initmem_default(int poison)<BR>{<BR>&nbsp;extern char __init_begin[], __init_end[];</P>
<P>&nbsp;return free_reserved_area(&amp;__init_begin, &amp;__init_end,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; poison, "unused kernel");<BR>}</P>
<P>static inline unsigned long get_num_physpages(void)<BR>{<BR>&nbsp;int nid;<BR>&nbsp;unsigned long phys_pages = 0;</P>
<P>&nbsp;for_each_online_node(nid)<BR>&nbsp;&nbsp;phys_pages += node_present_pages(nid);</P>
<P>&nbsp;return phys_pages;<BR>}</P>
<P>#ifdef CONFIG_HAVE_MEMBLOCK_NODE_MAP<BR>/*<BR>&nbsp;* With CONFIG_HAVE_MEMBLOCK_NODE_MAP set, an architecture may initialise its<BR>&nbsp;* zones, allocate the backing mem_map and account for memory holes in a more<BR>&nbsp;* architecture independent manner. This is a substitute for creating the<BR>&nbsp;* zone_sizes[] and zholes_size[] arrays and passing them to<BR>&nbsp;* free_area_init_node()<BR>&nbsp;*<BR>&nbsp;* An architecture is expected to register range of page frames backed by<BR>&nbsp;* physical memory with memblock_add[_node]() before calling<BR>&nbsp;* free_area_init_nodes() passing in the PFN each zone ends at. At a basic<BR>&nbsp;* usage, an architecture is expected to do something like<BR>&nbsp;*<BR>&nbsp;* unsigned long max_zone_pfns[MAX_NR_ZONES] = {max_dma, max_normal_pfn,<BR>&nbsp;* &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; max_highmem_pfn};<BR>&nbsp;* for_each_valid_physical_page_range()<BR>&nbsp;* &nbsp;memblock_add_node(base, size, nid)<BR>&nbsp;* free_area_init_nodes(max_zone_pfns);<BR>&nbsp;*<BR>&nbsp;* free_bootmem_with_active_regions() calls free_bootmem_node() for each<BR>&nbsp;* registered physical page range.&nbsp; Similarly<BR>&nbsp;* sparse_memory_present_with_active_regions() calls memory_present() for<BR>&nbsp;* each range when SPARSEMEM is enabled.<BR>&nbsp;*<BR>&nbsp;* See mm/page_alloc.c for more information on each function exposed by<BR>&nbsp;* CONFIG_HAVE_MEMBLOCK_NODE_MAP.<BR>&nbsp;*/<BR>extern void free_area_init_nodes(unsigned long *max_zone_pfn);<BR>unsigned long node_map_pfn_alignment(void);<BR>unsigned long __absent_pages_in_range(int nid, unsigned long start_pfn,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;unsigned long end_pfn);<BR>extern unsigned long absent_pages_in_range(unsigned long start_pfn,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;unsigned long end_pfn);<BR>extern void get_pfn_range_for_nid(unsigned int nid,<BR>&nbsp;&nbsp;&nbsp;unsigned long *start_pfn, unsigned long *end_pfn);<BR>extern unsigned long find_min_pfn_with_active_regions(void);<BR>extern void free_bootmem_with_active_regions(int nid,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;unsigned long max_low_pfn);<BR>extern void sparse_memory_present_with_active_regions(int nid);</P>
<P>#endif /* CONFIG_HAVE_MEMBLOCK_NODE_MAP */</P>
<P>#if !defined(CONFIG_HAVE_MEMBLOCK_NODE_MAP) &amp;&amp; \<BR>&nbsp;&nbsp;&nbsp; !defined(CONFIG_HAVE_ARCH_EARLY_PFN_TO_NID)<BR>static inline int __early_pfn_to_nid(unsigned long pfn,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;struct mminit_pfnnid_cache *state)<BR>{<BR>&nbsp;return 0;<BR>}<BR>#else<BR>/* please see mm/page_alloc.c */<BR>extern int __meminit early_pfn_to_nid(unsigned long pfn);<BR>/* there is a per-arch backend function. */<BR>extern int __meminit __early_pfn_to_nid(unsigned long pfn,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;struct mminit_pfnnid_cache *state);<BR>#endif</P>
<P>extern void set_dma_reserve(unsigned long new_dma_reserve);<BR>extern void memmap_init_zone(unsigned long, int, unsigned long,<BR>&nbsp;&nbsp;&nbsp;&nbsp;unsigned long, enum memmap_context);<BR>extern void setup_per_zone_wmarks(void);<BR>extern int __meminit init_per_zone_wmark_min(void);<BR>extern void mem_init(void);<BR>extern void __init mmap_init(void);<BR>extern void show_mem(unsigned int flags);<BR>extern void si_meminfo(struct sysinfo * val);<BR>extern void si_meminfo_node(struct sysinfo *val, int nid);</P>
<P>extern __printf(3, 4)<BR>void warn_alloc_failed(gfp_t gfp_mask, int order, const char *fmt, ...);</P>
<P>extern void setup_per_cpu_pageset(void);</P>
<P>extern void zone_pcp_update(struct zone *zone);<BR>extern void zone_pcp_reset(struct zone *zone);</P>
<P>/* page_alloc.c */<BR>extern int min_free_kbytes;</P>
<P>/* nommu.c */<BR>extern atomic_long_t mmap_pages_allocated;<BR>extern int nommu_shrink_inode_mappings(struct inode *, size_t, size_t);</P>
<P>/* interval_tree.c */<BR>void vma_interval_tree_insert(struct vm_area_struct *node,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; struct rb_root *root);<BR>void vma_interval_tree_insert_after(struct vm_area_struct *node,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; struct vm_area_struct *prev,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; struct rb_root *root);<BR>void vma_interval_tree_remove(struct vm_area_struct *node,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; struct rb_root *root);<BR>struct vm_area_struct *vma_interval_tree_iter_first(struct rb_root *root,<BR>&nbsp;&nbsp;&nbsp;&nbsp;unsigned long start, unsigned long last);<BR>struct vm_area_struct *vma_interval_tree_iter_next(struct vm_area_struct *node,<BR>&nbsp;&nbsp;&nbsp;&nbsp;unsigned long start, unsigned long last);</P>
<P>#define vma_interval_tree_foreach(vma, root, start, last)&nbsp;&nbsp;\<BR>&nbsp;for (vma = vma_interval_tree_iter_first(root, start, last);&nbsp;\<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; vma; vma = vma_interval_tree_iter_next(vma, start, last))</P>
<P>void anon_vma_interval_tree_insert(struct anon_vma_chain *node,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; struct rb_root *root);<BR>void anon_vma_interval_tree_remove(struct anon_vma_chain *node,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; struct rb_root *root);<BR>struct anon_vma_chain *anon_vma_interval_tree_iter_first(<BR>&nbsp;struct rb_root *root, unsigned long start, unsigned long last);<BR>struct anon_vma_chain *anon_vma_interval_tree_iter_next(<BR>&nbsp;struct anon_vma_chain *node, unsigned long start, unsigned long last);<BR>#ifdef CONFIG_DEBUG_VM_RB<BR>void anon_vma_interval_tree_verify(struct anon_vma_chain *node);<BR>#endif</P>
<P>#define anon_vma_interval_tree_foreach(avc, root, start, last)&nbsp;&nbsp; \<BR>&nbsp;for (avc = anon_vma_interval_tree_iter_first(root, start, last); \<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; avc; avc = anon_vma_interval_tree_iter_next(avc, start, last))</P>
<P>/* mmap.c */<BR>extern int __vm_enough_memory(struct mm_struct *mm, long pages, int cap_sys_admin);<BR>extern int vma_adjust(struct vm_area_struct *vma, unsigned long start,<BR>&nbsp;unsigned long end, pgoff_t pgoff, struct vm_area_struct *insert);<BR>extern struct vm_area_struct *vma_merge(struct mm_struct *,<BR>&nbsp;struct vm_area_struct *prev, unsigned long addr, unsigned long end,<BR>&nbsp;unsigned long vm_flags, struct anon_vma *, struct file *, pgoff_t,<BR>&nbsp;struct mempolicy *);<BR>extern struct anon_vma *find_mergeable_anon_vma(struct vm_area_struct *);<BR>extern int split_vma(struct mm_struct *,<BR>&nbsp;struct vm_area_struct *, unsigned long addr, int new_below);<BR>extern int insert_vm_struct(struct mm_struct *, struct vm_area_struct *);<BR>extern void __vma_link_rb(struct mm_struct *, struct vm_area_struct *,<BR>&nbsp;struct rb_node **, struct rb_node *);<BR>extern void unlink_file_vma(struct vm_area_struct *);<BR>extern struct vm_area_struct *copy_vma(struct vm_area_struct **,<BR>&nbsp;unsigned long addr, unsigned long len, pgoff_t pgoff,<BR>&nbsp;bool *need_rmap_locks);<BR>extern void exit_mmap(struct mm_struct *);</P>
<P>static inline int check_data_rlimit(unsigned long rlim,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; unsigned long new,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; unsigned long start,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; unsigned long end_data,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; unsigned long start_data)<BR>{<BR>&nbsp;if (rlim &lt; RLIM_INFINITY) {<BR>&nbsp;&nbsp;if (((new - start) + (end_data - start_data)) &gt; rlim)<BR>&nbsp;&nbsp;&nbsp;return -ENOSPC;<BR>&nbsp;}</P>
<P>&nbsp;return 0;<BR>}</P>
<P>extern int mm_take_all_locks(struct mm_struct *mm);<BR>extern void mm_drop_all_locks(struct mm_struct *mm);</P>
<P>extern void set_mm_exe_file(struct mm_struct *mm, struct file *new_exe_file);<BR>extern struct file *get_mm_exe_file(struct mm_struct *mm);</P>
<P>extern int may_expand_vm(struct mm_struct *mm, unsigned long npages);<BR>extern struct vm_area_struct *_install_special_mapping(struct mm_struct *mm,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; unsigned long addr, unsigned long len,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; unsigned long flags,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; const struct vm_special_mapping *spec);<BR>/* This is an obsolete alternative to _install_special_mapping. */<BR>extern int install_special_mapping(struct mm_struct *mm,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; unsigned long addr, unsigned long len,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; unsigned long flags, struct page **pages);</P>
<P>extern unsigned long get_unmapped_area(struct file *, unsigned long, unsigned long, unsigned long, unsigned long);</P>
<P>extern unsigned long mmap_region(struct file *file, unsigned long addr,<BR>&nbsp;unsigned long len, vm_flags_t vm_flags, unsigned long pgoff);<BR>extern unsigned long do_mmap_pgoff(struct file *file, unsigned long addr,<BR>&nbsp;unsigned long len, unsigned long prot, unsigned long flags,<BR>&nbsp;unsigned long pgoff, unsigned long *populate);<BR>extern int do_munmap(struct mm_struct *, unsigned long, size_t);</P>
<P>#ifdef CONFIG_MMU<BR>extern int __mm_populate(unsigned long addr, unsigned long len,<BR>&nbsp;&nbsp;&nbsp; int ignore_errors);<BR>static inline void mm_populate(unsigned long addr, unsigned long len)<BR>{<BR>&nbsp;/* Ignore errors */<BR>&nbsp;(void) __mm_populate(addr, len, 1);<BR>}<BR>#else<BR>static inline void mm_populate(unsigned long addr, unsigned long len) {}<BR>#endif</P>
<P>/* These take the mm semaphore themselves */<BR>extern unsigned long vm_brk(unsigned long, unsigned long);<BR>extern int vm_munmap(unsigned long, size_t);<BR>extern unsigned long vm_mmap(struct file *, unsigned long,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; unsigned long, unsigned long,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; unsigned long, unsigned long);</P>
<P>struct vm_unmapped_area_info {<BR>#define VM_UNMAPPED_AREA_TOPDOWN 1<BR>&nbsp;unsigned long flags;<BR>&nbsp;unsigned long length;<BR>&nbsp;unsigned long low_limit;<BR>&nbsp;unsigned long high_limit;<BR>&nbsp;unsigned long align_mask;<BR>&nbsp;unsigned long align_offset;<BR>};</P>
<P>extern unsigned long unmapped_area(struct vm_unmapped_area_info *info);<BR>extern unsigned long unmapped_area_topdown(struct vm_unmapped_area_info *info);</P>
<P>/*<BR>&nbsp;* Search for an unmapped address range.<BR>&nbsp;*<BR>&nbsp;* We are looking for a range that:<BR>&nbsp;* - does not intersect with any VMA;<BR>&nbsp;* - is contained within the [low_limit, high_limit) interval;<BR>&nbsp;* - is at least the desired size.<BR>&nbsp;* - satisfies (begin_addr &amp; align_mask) == (align_offset &amp; align_mask)<BR>&nbsp;*/<BR>static inline unsigned long<BR>vm_unmapped_area(struct vm_unmapped_area_info *info)<BR>{<BR>&nbsp;if (info-&gt;flags &amp; VM_UNMAPPED_AREA_TOPDOWN)<BR>&nbsp;&nbsp;return unmapped_area_topdown(info);<BR>&nbsp;else<BR>&nbsp;&nbsp;return unmapped_area(info);<BR>}</P>
<P>/* truncate.c */<BR>extern void truncate_inode_pages(struct address_space *, loff_t);<BR>extern void truncate_inode_pages_range(struct address_space *,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; loff_t lstart, loff_t lend);<BR>extern void truncate_inode_pages_final(struct address_space *);</P>
<P>/* generic vm_area_ops exported for stackable file systems */<BR>extern int filemap_fault(struct vm_area_struct *, struct vm_fault *);<BR>extern void filemap_map_pages(struct vm_area_struct *vma, struct vm_fault *vmf);<BR>extern int filemap_page_mkwrite(struct vm_area_struct *vma, struct vm_fault *vmf);</P>
<P>/* mm/page-writeback.c */<BR>int write_one_page(struct page *page, int wait);<BR>void task_dirty_inc(struct task_struct *tsk);</P>
<P>/* readahead.c */<BR>#define VM_MAX_READAHEAD&nbsp;128&nbsp;/* kbytes */<BR>#define VM_MIN_READAHEAD&nbsp;16&nbsp;/* kbytes (includes current page) */</P>
<P>int force_page_cache_readahead(struct address_space *mapping, struct file *filp,<BR>&nbsp;&nbsp;&nbsp;pgoff_t offset, unsigned long nr_to_read);</P>
<P>void page_cache_sync_readahead(struct address_space *mapping,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; struct file_ra_state *ra,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; struct file *filp,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; pgoff_t offset,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; unsigned long size);</P>
<P>void page_cache_async_readahead(struct address_space *mapping,<BR>&nbsp;&nbsp;&nbsp;&nbsp;struct file_ra_state *ra,<BR>&nbsp;&nbsp;&nbsp;&nbsp;struct file *filp,<BR>&nbsp;&nbsp;&nbsp;&nbsp;struct page *pg,<BR>&nbsp;&nbsp;&nbsp;&nbsp;pgoff_t offset,<BR>&nbsp;&nbsp;&nbsp;&nbsp;unsigned long size);</P>
<P>unsigned long max_sane_readahead(unsigned long nr);</P>
<P>/* Generic expand stack which grows the stack according to GROWS{UP,DOWN} */<BR>extern int expand_stack(struct vm_area_struct *vma, unsigned long address);</P>
<P>/* CONFIG_STACK_GROWSUP still needs to to grow downwards at some places */<BR>extern int expand_downwards(struct vm_area_struct *vma,<BR>&nbsp;&nbsp;unsigned long address);<BR>#if VM_GROWSUP<BR>extern int expand_upwards(struct vm_area_struct *vma, unsigned long address);<BR>#else<BR>&nbsp; #define expand_upwards(vma, address) (0)<BR>#endif</P>
<P>/* Look up the first VMA which satisfies&nbsp; addr &lt; vm_end,&nbsp; NULL if none. */<BR>extern struct vm_area_struct * find_vma(struct mm_struct * mm, unsigned long addr);<BR>extern struct vm_area_struct * find_vma_prev(struct mm_struct * mm, unsigned long addr,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; struct vm_area_struct **pprev);</P>
<P>/* Look up the first VMA which intersects the interval start_addr..end_addr-1,<BR>&nbsp;&nbsp; NULL if none.&nbsp; Assume start_addr &lt; end_addr. */<BR>static inline struct vm_area_struct * find_vma_intersection(struct mm_struct * mm, unsigned long start_addr, unsigned long end_addr)<BR>{<BR>&nbsp;struct vm_area_struct * vma = find_vma(mm,start_addr);</P>
<P>&nbsp;if (vma &amp;&amp; end_addr &lt;= vma-&gt;vm_start)<BR>&nbsp;&nbsp;vma = NULL;<BR>&nbsp;return vma;<BR>}</P>
<P>static inline unsigned long vma_pages(struct vm_area_struct *vma)<BR>{<BR>&nbsp;return (vma-&gt;vm_end - vma-&gt;vm_start) &gt;&gt; PAGE_SHIFT;<BR>}</P>
<P>/* Look up the first VMA which exactly match the interval vm_start ... vm_end */<BR>static inline struct vm_area_struct *find_exact_vma(struct mm_struct *mm,<BR>&nbsp;&nbsp;&nbsp;&nbsp;unsigned long vm_start, unsigned long vm_end)<BR>{<BR>&nbsp;struct vm_area_struct *vma = find_vma(mm, vm_start);</P>
<P>&nbsp;if (vma &amp;&amp; (vma-&gt;vm_start != vm_start || vma-&gt;vm_end != vm_end))<BR>&nbsp;&nbsp;vma = NULL;</P>
<P>&nbsp;return vma;<BR>}</P>
<P>#ifdef CONFIG_MMU<BR>pgprot_t vm_get_page_prot(unsigned long vm_flags);<BR>void vma_set_page_prot(struct vm_area_struct *vma);<BR>#else<BR>static inline pgprot_t vm_get_page_prot(unsigned long vm_flags)<BR>{<BR>&nbsp;return __pgprot(0);<BR>}<BR>static inline void vma_set_page_prot(struct vm_area_struct *vma)<BR>{<BR>&nbsp;vma-&gt;vm_page_prot = vm_get_page_prot(vma-&gt;vm_flags);<BR>}<BR>#endif</P>
<P>#ifdef CONFIG_NUMA_BALANCING<BR>unsigned long change_prot_numa(struct vm_area_struct *vma,<BR>&nbsp;&nbsp;&nbsp;unsigned long start, unsigned long end);<BR>#endif</P>
<P>struct vm_area_struct *find_extend_vma(struct mm_struct *, unsigned long addr);<BR>int remap_pfn_range(struct vm_area_struct *, unsigned long addr,<BR>&nbsp;&nbsp;&nbsp;unsigned long pfn, unsigned long size, pgprot_t);<BR>int vm_insert_page(struct vm_area_struct *, unsigned long addr, struct page *);<BR>int vm_insert_pfn(struct vm_area_struct *vma, unsigned long addr,<BR>&nbsp;&nbsp;&nbsp;unsigned long pfn);<BR>int vm_insert_mixed(struct vm_area_struct *vma, unsigned long addr,<BR>&nbsp;&nbsp;&nbsp;unsigned long pfn);<BR>int vm_iomap_memory(struct vm_area_struct *vma, phys_addr_t start, unsigned long len);</P>
<P><BR>struct page *follow_page_mask(struct vm_area_struct *vma,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; unsigned long address, unsigned int foll_flags,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; unsigned int *page_mask);</P>
<P>static inline struct page *follow_page(struct vm_area_struct *vma,<BR>&nbsp;&nbsp;unsigned long address, unsigned int foll_flags)<BR>{<BR>&nbsp;unsigned int unused_page_mask;<BR>&nbsp;return follow_page_mask(vma, address, foll_flags, &amp;unused_page_mask);<BR>}</P>
<P>#define FOLL_WRITE&nbsp;0x01&nbsp;/* check pte is writable */<BR>#define FOLL_TOUCH&nbsp;0x02&nbsp;/* mark page accessed */<BR>#define FOLL_GET&nbsp;0x04&nbsp;/* do get_page on page */<BR>#define FOLL_DUMP&nbsp;0x08&nbsp;/* give error on hole if it would be zero */<BR>#define FOLL_FORCE&nbsp;0x10&nbsp;/* get_user_pages read/write w/o permission */<BR>#define FOLL_NOWAIT&nbsp;0x20&nbsp;/* if a disk transfer is needed, start the IO<BR>&nbsp;&nbsp;&nbsp;&nbsp; * and return without waiting upon it */<BR>#define FOLL_POPULATE&nbsp;0x40&nbsp;/* fault in page */<BR>#define FOLL_SPLIT&nbsp;0x80&nbsp;/* don't return transhuge pages, split them */<BR>#define FOLL_HWPOISON&nbsp;0x100&nbsp;/* check page is hwpoisoned */<BR>#define FOLL_NUMA&nbsp;0x200&nbsp;/* force NUMA hinting page fault */<BR>#define FOLL_MIGRATION&nbsp;0x400&nbsp;/* wait for page to replace migration entry */<BR>#define FOLL_TRIED&nbsp;0x800&nbsp;/* a retry, previous pass started an IO */</P>
<P>typedef int (*pte_fn_t)(pte_t *pte, pgtable_t token, unsigned long addr,<BR>&nbsp;&nbsp;&nbsp;void *data);<BR>extern int apply_to_page_range(struct mm_struct *mm, unsigned long address,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; unsigned long size, pte_fn_t fn, void *data);</P>
<P>#ifdef CONFIG_PROC_FS<BR>void vm_stat_account(struct mm_struct *, unsigned long, struct file *, long);<BR>#else<BR>static inline void vm_stat_account(struct mm_struct *mm,<BR>&nbsp;&nbsp;&nbsp;unsigned long flags, struct file *file, long pages)<BR>{<BR>&nbsp;mm-&gt;total_vm += pages;<BR>}<BR>#endif /* CONFIG_PROC_FS */</P>
<P>#ifdef CONFIG_DEBUG_PAGEALLOC<BR>extern bool _debug_pagealloc_enabled;<BR>extern void __kernel_map_pages(struct page *page, int numpages, int enable);</P>
<P>static inline bool debug_pagealloc_enabled(void)<BR>{<BR>&nbsp;return _debug_pagealloc_enabled;<BR>}</P>
<P>static inline void<BR>kernel_map_pages(struct page *page, int numpages, int enable)<BR>{<BR>&nbsp;if (!debug_pagealloc_enabled())<BR>&nbsp;&nbsp;return;</P>
<P>&nbsp;__kernel_map_pages(page, numpages, enable);<BR>}<BR>#ifdef CONFIG_HIBERNATION<BR>extern bool kernel_page_present(struct page *page);<BR>#endif /* CONFIG_HIBERNATION */<BR>#else<BR>static inline void<BR>kernel_map_pages(struct page *page, int numpages, int enable) {}<BR>#ifdef CONFIG_HIBERNATION<BR>static inline bool kernel_page_present(struct page *page) { return true; }<BR>#endif /* CONFIG_HIBERNATION */<BR>#endif</P>
<P>#ifdef __HAVE_ARCH_GATE_AREA<BR>extern struct vm_area_struct *get_gate_vma(struct mm_struct *mm);<BR>extern int in_gate_area_no_mm(unsigned long addr);<BR>extern int in_gate_area(struct mm_struct *mm, unsigned long addr);<BR>#else<BR>static inline struct vm_area_struct *get_gate_vma(struct mm_struct *mm)<BR>{<BR>&nbsp;return NULL;<BR>}<BR>static inline int in_gate_area_no_mm(unsigned long addr) { return 0; }<BR>static inline int in_gate_area(struct mm_struct *mm, unsigned long addr)<BR>{<BR>&nbsp;return 0;<BR>}<BR>#endif&nbsp;/* __HAVE_ARCH_GATE_AREA */</P>
<P>#ifdef CONFIG_SYSCTL<BR>extern int sysctl_drop_caches;<BR>int drop_caches_sysctl_handler(struct ctl_table *, int,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;void __user *, size_t *, loff_t *);<BR>#endif</P>
<P>void drop_slab(void);<BR>void drop_slab_node(int nid);</P>
<P>#ifndef CONFIG_MMU<BR>#define randomize_va_space 0<BR>#else<BR>extern int randomize_va_space;<BR>#endif</P>
<P>const char * arch_vma_name(struct vm_area_struct *vma);<BR>void print_vma_addr(char *prefix, unsigned long rip);</P>
<P>void sparse_mem_maps_populate_node(struct page **map_map,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; unsigned long pnum_begin,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; unsigned long pnum_end,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; unsigned long map_count,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; int nodeid);</P>
<P>struct page *sparse_mem_map_populate(unsigned long pnum, int nid);<BR>pgd_t *vmemmap_pgd_populate(unsigned long addr, int node);<BR>pud_t *vmemmap_pud_populate(pgd_t *pgd, unsigned long addr, int node);<BR>pmd_t *vmemmap_pmd_populate(pud_t *pud, unsigned long addr, int node);<BR>pte_t *vmemmap_pte_populate(pmd_t *pmd, unsigned long addr, int node);<BR>void *vmemmap_alloc_block(unsigned long size, int node);<BR>void *vmemmap_alloc_block_buf(unsigned long size, int node);<BR>void vmemmap_verify(pte_t *, int, unsigned long, unsigned long);<BR>int vmemmap_populate_basepages(unsigned long start, unsigned long end,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; int node);<BR>int vmemmap_populate(unsigned long start, unsigned long end, int node);<BR>void vmemmap_populate_print_last(void);<BR>#ifdef CONFIG_MEMORY_HOTPLUG<BR>void vmemmap_free(unsigned long start, unsigned long end);<BR>#endif<BR>void register_page_bootmem_memmap(unsigned long section_nr, struct page *map,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; unsigned long size);</P>
<P>enum mf_flags {<BR>&nbsp;MF_COUNT_INCREASED = 1 &lt;&lt; 0,<BR>&nbsp;MF_ACTION_REQUIRED = 1 &lt;&lt; 1,<BR>&nbsp;MF_MUST_KILL = 1 &lt;&lt; 2,<BR>&nbsp;MF_SOFT_OFFLINE = 1 &lt;&lt; 3,<BR>};<BR>extern int memory_failure(unsigned long pfn, int trapno, int flags);<BR>extern void memory_failure_queue(unsigned long pfn, int trapno, int flags);<BR>extern int unpoison_memory(unsigned long pfn);<BR>extern int get_hwpoison_page(struct page *page);<BR>extern int sysctl_memory_failure_early_kill;<BR>extern int sysctl_memory_failure_recovery;<BR>extern void shake_page(struct page *p, int access);<BR>extern atomic_long_t num_poisoned_pages;<BR>extern int soft_offline_page(struct page *page, int flags);</P>
<P><BR>/*<BR>&nbsp;* Error handlers for various types of pages.<BR>&nbsp;*/<BR>enum mf_result {<BR>&nbsp;MF_IGNORED,&nbsp;/* Error: cannot be handled */<BR>&nbsp;MF_FAILED,&nbsp;/* Error: handling failed */<BR>&nbsp;MF_DELAYED,&nbsp;/* Will be handled later */<BR>&nbsp;MF_RECOVERED,&nbsp;/* Successfully recovered */<BR>};</P>
<P>enum mf_action_page_type {<BR>&nbsp;MF_MSG_KERNEL,<BR>&nbsp;MF_MSG_KERNEL_HIGH_ORDER,<BR>&nbsp;MF_MSG_SLAB,<BR>&nbsp;MF_MSG_DIFFERENT_COMPOUND,<BR>&nbsp;MF_MSG_POISONED_HUGE,<BR>&nbsp;MF_MSG_HUGE,<BR>&nbsp;MF_MSG_FREE_HUGE,<BR>&nbsp;MF_MSG_UNMAP_FAILED,<BR>&nbsp;MF_MSG_DIRTY_SWAPCACHE,<BR>&nbsp;MF_MSG_CLEAN_SWAPCACHE,<BR>&nbsp;MF_MSG_DIRTY_MLOCKED_LRU,<BR>&nbsp;MF_MSG_CLEAN_MLOCKED_LRU,<BR>&nbsp;MF_MSG_DIRTY_UNEVICTABLE_LRU,<BR>&nbsp;MF_MSG_CLEAN_UNEVICTABLE_LRU,<BR>&nbsp;MF_MSG_DIRTY_LRU,<BR>&nbsp;MF_MSG_CLEAN_LRU,<BR>&nbsp;MF_MSG_TRUNCATED_LRU,<BR>&nbsp;MF_MSG_BUDDY,<BR>&nbsp;MF_MSG_BUDDY_2ND,<BR>&nbsp;MF_MSG_UNKNOWN,<BR>};</P>
<P>#if defined(CONFIG_TRANSPARENT_HUGEPAGE) || defined(CONFIG_HUGETLBFS)<BR>extern void clear_huge_page(struct page *page,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; unsigned long addr,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; unsigned int pages_per_huge_page);<BR>extern void copy_user_huge_page(struct page *dst, struct page *src,<BR>&nbsp;&nbsp;&nbsp;&nbsp;unsigned long addr, struct vm_area_struct *vma,<BR>&nbsp;&nbsp;&nbsp;&nbsp;unsigned int pages_per_huge_page);<BR>#endif /* CONFIG_TRANSPARENT_HUGEPAGE || CONFIG_HUGETLBFS */</P>
<P>extern struct page_ext_operations debug_guardpage_ops;<BR>extern struct page_ext_operations page_poisoning_ops;</P>
<P>#ifdef CONFIG_DEBUG_PAGEALLOC<BR>extern unsigned int _debug_guardpage_minorder;<BR>extern bool _debug_guardpage_enabled;</P>
<P>static inline unsigned int debug_guardpage_minorder(void)<BR>{<BR>&nbsp;return _debug_guardpage_minorder;<BR>}</P>
<P>static inline bool debug_guardpage_enabled(void)<BR>{<BR>&nbsp;return _debug_guardpage_enabled;<BR>}</P>
<P>static inline bool page_is_guard(struct page *page)<BR>{<BR>&nbsp;struct page_ext *page_ext;</P>
<P>&nbsp;if (!debug_guardpage_enabled())<BR>&nbsp;&nbsp;return false;</P>
<P>&nbsp;page_ext = lookup_page_ext(page);<BR>&nbsp;return test_bit(PAGE_EXT_DEBUG_GUARD, &amp;page_ext-&gt;flags);<BR>}<BR>#else<BR>static inline unsigned int debug_guardpage_minorder(void) { return 0; }<BR>static inline bool debug_guardpage_enabled(void) { return false; }<BR>static inline bool page_is_guard(struct page *page) { return false; }<BR>#endif /* CONFIG_DEBUG_PAGEALLOC */</P>
<P>#if MAX_NUMNODES &gt; 1<BR>void __init setup_nr_node_ids(void);<BR>#else<BR>static inline void setup_nr_node_ids(void) {}<BR>#endif</P>
<P>#endif /* __KERNEL__ */<BR>#endif /* _LINUX_MM_H */