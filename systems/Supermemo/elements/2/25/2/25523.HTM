mm/util.c 
<P></P>
<P>#include &lt;linux/mm.h&gt;<BR>#include &lt;linux/slab.h&gt;<BR>#include &lt;linux/string.h&gt;<BR>#include &lt;linux/compiler.h&gt;<BR>#include &lt;linux/export.h&gt;<BR>#include &lt;linux/err.h&gt;<BR>#include &lt;linux/sched.h&gt;<BR>#include &lt;linux/security.h&gt;<BR>#include &lt;linux/swap.h&gt;<BR>#include &lt;linux/swapops.h&gt;<BR>#include &lt;linux/mman.h&gt;<BR>#include &lt;linux/hugetlb.h&gt;<BR>#include &lt;linux/vmalloc.h&gt;</P>
<P></P>
<P>#include &lt;asm/sections.h&gt;<BR>#include &lt;asm/uaccess.h&gt;</P>
<P>#include "internal.h"</P>
<P><FONT class=extract>static inline int is_kernel_rodata(unsigned long addr)<BR>{<BR>&nbsp;return addr &gt;= (unsigned long)__start_rodata &amp;&amp;<BR>&nbsp;&nbsp;addr &lt; (unsigned long)__end_rodata;<BR>}</FONT></P>
<P><FONT class=extract>/**<BR>&nbsp;* kfree_const - conditionally free memory<BR>&nbsp;* @x: pointer to the memory<BR>&nbsp;*<BR>&nbsp;* Function calls kfree only if @x is not in .rodata section.<BR>&nbsp;*/<BR>void kfree_const(const void *x)<BR>{<BR>&nbsp;if (!is_kernel_rodata((unsigned long)x))<BR>&nbsp;&nbsp;kfree(x);<BR>}<BR>EXPORT_SYMBOL(kfree_const);</FONT></P>
<P><FONT class=extract>/**<BR>&nbsp;* kstrdup - allocate space for and copy an existing string<BR>&nbsp;* @s: the string to duplicate<BR>&nbsp;* @gfp: the GFP mask used in the kmalloc() call when allocating memory<BR>&nbsp;*/<BR>char *kstrdup(const char *s, gfp_t gfp)<BR>{<BR>&nbsp;size_t len;<BR>&nbsp;char *buf;</FONT></P>
<P><FONT class=extract>&nbsp;if (!s)<BR>&nbsp;&nbsp;return NULL;</FONT></P>
<P><FONT class=extract>&nbsp;len = strlen(s) + 1;<BR>&nbsp;buf = kmalloc_track_caller(len, gfp);<BR>&nbsp;if (buf)<BR>&nbsp;&nbsp;memcpy(buf, s, len);<BR>&nbsp;return buf;<BR>}<BR>EXPORT_SYMBOL(kstrdup);</FONT></P>
<P><FONT class=extract>/**<BR>&nbsp;* kstrdup_const - conditionally duplicate an existing const string<BR>&nbsp;* @s: the string to duplicate<BR>&nbsp;* @gfp: the GFP mask used in the kmalloc() call when allocating memory<BR>&nbsp;*<BR>&nbsp;* Function returns source string if it is in .rodata section otherwise it<BR>&nbsp;* fallbacks to kstrdup.<BR>&nbsp;* Strings allocated by kstrdup_const should be freed by kfree_const.<BR>&nbsp;*/<BR>const char *kstrdup_const(const char *s, gfp_t gfp)<BR>{<BR>&nbsp;if (is_kernel_rodata((unsigned long)s))<BR>&nbsp;&nbsp;return s;</FONT></P>
<P><FONT class=extract>&nbsp;return kstrdup(s, gfp);<BR>}<BR>EXPORT_SYMBOL(kstrdup_const);</FONT></P>
<P><FONT class=extract>/**<BR>&nbsp;* kstrndup - allocate space for and copy an existing string<BR>&nbsp;* @s: the string to duplicate<BR>&nbsp;* @max: read at most @max chars from @s<BR>&nbsp;* @gfp: the GFP mask used in the kmalloc() call when allocating memory<BR>&nbsp;*/<BR>char *kstrndup(const char *s, size_t max, gfp_t gfp)<BR>{<BR>&nbsp;size_t len;<BR>&nbsp;char *buf;</FONT></P>
<P><FONT class=extract>&nbsp;if (!s)<BR>&nbsp;&nbsp;return NULL;</FONT></P>
<P><FONT class=extract>&nbsp;len = strnlen(s, max);<BR>&nbsp;buf = kmalloc_track_caller(len+1, gfp);<BR>&nbsp;if (buf) {<BR>&nbsp;&nbsp;memcpy(buf, s, len);<BR>&nbsp;&nbsp;buf[len] = '\0';<BR>&nbsp;}<BR>&nbsp;return buf;<BR>}<BR>EXPORT_SYMBOL(kstrndup);</FONT></P>
<P><FONT class=extract>/**<BR>&nbsp;* kmemdup - duplicate region of memory<BR>&nbsp;*<BR>&nbsp;* @src: memory region to duplicate<BR>&nbsp;* @len: memory region length<BR>&nbsp;* @gfp: GFP mask to use<BR>&nbsp;*/<BR>void *kmemdup(const void *src, size_t len, gfp_t gfp)<BR>{<BR>&nbsp;void *p;</FONT></P>
<P><FONT class=extract>&nbsp;p = kmalloc_track_caller(len, gfp);<BR>&nbsp;if (p)<BR>&nbsp;&nbsp;memcpy(p, src, len);<BR>&nbsp;return p;<BR>}<BR>EXPORT_SYMBOL(kmemdup);</FONT></P>
<P><FONT class=extract>/**<BR>&nbsp;* memdup_user - duplicate memory region from user space<BR>&nbsp;*<BR>&nbsp;* @src: source address in user space<BR>&nbsp;* @len: number of bytes to copy<BR>&nbsp;*<BR>&nbsp;* Returns an ERR_PTR() on failure.<BR>&nbsp;*/<BR>void *memdup_user(const void __user *src, size_t len)<BR>{<BR>&nbsp;void *p;</FONT></P>
<P><FONT class=extract>&nbsp;/*<BR>&nbsp; * Always use GFP_KERNEL, since copy_from_user() can sleep and<BR>&nbsp; * cause pagefault, which makes it pointless to use GFP_NOFS<BR>&nbsp; * or GFP_ATOMIC.<BR>&nbsp; */<BR>&nbsp;p = kmalloc_track_caller(len, GFP_KERNEL);<BR>&nbsp;if (!p)<BR>&nbsp;&nbsp;return ERR_PTR(-ENOMEM);</FONT></P>
<P><FONT class=extract>&nbsp;if (copy_from_user(p, src, len)) {<BR>&nbsp;&nbsp;kfree(p);<BR>&nbsp;&nbsp;return ERR_PTR(-EFAULT);<BR>&nbsp;}</FONT></P>
<P><FONT class=extract>&nbsp;return p;<BR>}<BR>EXPORT_SYMBOL(memdup_user);</FONT></P>
<P><FONT class=extract>/*<BR>&nbsp;* strndup_user - duplicate an existing string from user space<BR>&nbsp;* @s: The string to duplicate<BR>&nbsp;* @n: Maximum number of bytes to copy, including the trailing NUL.<BR>&nbsp;*/<BR>char *strndup_user(const char __user *s, long n)<BR>{<BR>&nbsp;char *p;<BR>&nbsp;long length;</FONT></P>
<P><FONT class=extract>&nbsp;length = strnlen_user(s, n);</FONT></P>
<P><FONT class=extract>&nbsp;if (!length)<BR>&nbsp;&nbsp;return ERR_PTR(-EFAULT);</FONT></P>
<P><FONT class=extract>&nbsp;if (length &gt; n)<BR>&nbsp;&nbsp;return ERR_PTR(-EINVAL);</FONT></P>
<P><FONT class=extract>&nbsp;p = memdup_user(s, length);</FONT></P>
<P><FONT class=extract>&nbsp;if (IS_ERR(p))<BR>&nbsp;&nbsp;return p;</FONT></P>
<P><FONT class=extract>&nbsp;p[length - 1] = '\0';</FONT></P>
<P><FONT class=extract>&nbsp;return p;<BR>}<BR>EXPORT_SYMBOL(strndup_user);</FONT></P>
<P><FONT class=extract>void __vma_link_list(struct mm_struct *mm, struct vm_area_struct *vma,<BR>&nbsp;&nbsp;struct vm_area_struct *prev, struct rb_node *rb_parent)<BR>{<BR>&nbsp;struct vm_area_struct *next;</FONT></P>
<P><FONT class=extract>&nbsp;vma-&gt;vm_prev = prev;<BR>&nbsp;if (prev) {<BR>&nbsp;&nbsp;next = prev-&gt;vm_next;<BR>&nbsp;&nbsp;prev-&gt;vm_next = vma;<BR>&nbsp;} else {<BR>&nbsp;&nbsp;mm-&gt;mmap = vma;<BR>&nbsp;&nbsp;if (rb_parent)<BR>&nbsp;&nbsp;&nbsp;next = rb_entry(rb_parent,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;struct vm_area_struct, vm_rb);<BR>&nbsp;&nbsp;else<BR>&nbsp;&nbsp;&nbsp;next = NULL;<BR>&nbsp;}<BR>&nbsp;vma-&gt;vm_next = next;<BR>&nbsp;if (next)<BR>&nbsp;&nbsp;next-&gt;vm_prev = vma;<BR>}</FONT></P>
<P><FONT class=extract>/* Check if the vma is being used as a stack by this task */<BR>static int vm_is_stack_for_task(struct task_struct *t,<BR>&nbsp;&nbsp;&nbsp;&nbsp;struct vm_area_struct *vma)<BR>{<BR>&nbsp;return (vma-&gt;vm_start &lt;= KSTK_ESP(t) &amp;&amp; vma-&gt;vm_end &gt;= KSTK_ESP(t));<BR>}</FONT></P>
<P><FONT class=extract>/*<BR>&nbsp;* Check if the vma is being used as a stack.<BR>&nbsp;* If is_group is non-zero, check in the entire thread group or else<BR>&nbsp;* just check in the current task. Returns the task_struct of the task<BR>&nbsp;* that the vma is stack for. Must be called under rcu_read_lock().<BR>&nbsp;*/<BR>struct task_struct *task_of_stack(struct task_struct *task,<BR>&nbsp;&nbsp;&nbsp;&nbsp;struct vm_area_struct *vma, bool in_group)<BR>{<BR>&nbsp;if (vm_is_stack_for_task(task, vma))<BR>&nbsp;&nbsp;return task;</FONT></P>
<P><FONT class=extract>&nbsp;if (in_group) {<BR>&nbsp;&nbsp;struct task_struct *t;</FONT></P>
<P><FONT class=extract>&nbsp;&nbsp;for_each_thread(task, t) {<BR>&nbsp;&nbsp;&nbsp;if (vm_is_stack_for_task(t, vma))<BR>&nbsp;&nbsp;&nbsp;&nbsp;return t;<BR>&nbsp;&nbsp;}<BR>&nbsp;}</FONT></P>
<P><FONT class=extract>&nbsp;return NULL;<BR>}</FONT></P>
<P><FONT class=extract>#if defined(CONFIG_MMU) &amp;&amp; !defined(HAVE_ARCH_PICK_MMAP_LAYOUT)<BR>void arch_pick_mmap_layout(struct mm_struct *mm)<BR>{<BR>&nbsp;mm-&gt;mmap_base = TASK_UNMAPPED_BASE;<BR>&nbsp;mm-&gt;get_unmapped_area = arch_get_unmapped_area;<BR>}<BR>#endif</FONT></P>
<P><FONT class=extract>/*<BR>&nbsp;* Like get_user_pages_fast() except its IRQ-safe in that it won't fall<BR>&nbsp;* back to the regular GUP.<BR>&nbsp;* If the architecture not support this function, simply return with no<BR>&nbsp;* page pinned<BR>&nbsp;*/<BR>int __weak __get_user_pages_fast(unsigned long start,<BR>&nbsp;&nbsp;&nbsp;&nbsp; int nr_pages, int write, struct page **pages)<BR>{<BR>&nbsp;return 0;<BR>}<BR>EXPORT_SYMBOL_GPL(__get_user_pages_fast);</FONT></P>
<P><FONT class=extract>/**<BR>&nbsp;* get_user_pages_fast() - pin user pages in memory<BR>&nbsp;* @start:&nbsp;starting user address<BR>&nbsp;* @nr_pages:&nbsp;number of pages from start to pin<BR>&nbsp;* @write:&nbsp;whether pages will be written to<BR>&nbsp;* @pages:&nbsp;array that receives pointers to the pages pinned.<BR>&nbsp;*&nbsp;&nbsp;Should be at least nr_pages long.<BR>&nbsp;*<BR>&nbsp;* Returns number of pages pinned. This may be fewer than the number<BR>&nbsp;* requested. If nr_pages is 0 or negative, returns 0. If no pages<BR>&nbsp;* were pinned, returns -errno.<BR>&nbsp;*<BR>&nbsp;* get_user_pages_fast provides equivalent functionality to get_user_pages,<BR>&nbsp;* operating on current and current-&gt;mm, with force=0 and vma=NULL. However<BR>&nbsp;* unlike get_user_pages, it must be called without mmap_sem held.<BR>&nbsp;*<BR>&nbsp;* get_user_pages_fast may take mmap_sem and page table locks, so no<BR>&nbsp;* assumptions can be made about lack of locking. get_user_pages_fast is to be<BR>&nbsp;* implemented in a way that is advantageous (vs get_user_pages()) when the<BR>&nbsp;* user memory area is already faulted in and present in ptes. However if the<BR>&nbsp;* pages have to be faulted in, it may turn out to be slightly slower so<BR>&nbsp;* callers need to carefully consider what to use. On many architectures,<BR>&nbsp;* get_user_pages_fast simply falls back to get_user_pages.<BR>&nbsp;*/<BR>int __weak get_user_pages_fast(unsigned long start,<BR>&nbsp;&nbsp;&nbsp;&nbsp;int nr_pages, int write, struct page **pages)<BR>{<BR>&nbsp;struct mm_struct *mm = current-&gt;mm;<BR>&nbsp;return get_user_pages_unlocked(current, mm, start, nr_pages,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; write, 0, pages);<BR>}<BR>EXPORT_SYMBOL_GPL(get_user_pages_fast);</FONT></P>
<P>unsigned long vm_mmap_pgoff(struct file *file, unsigned long addr,<BR>&nbsp;unsigned long len, unsigned long prot,<BR>&nbsp;unsigned long flag, unsigned long pgoff)<BR>{<BR>&nbsp;unsigned long ret;<BR>&nbsp;struct mm_struct *mm = current-&gt;mm;<BR>&nbsp;unsigned long populate;</P>
<P>&nbsp;ret = security_mmap_file(file, prot, flag);<BR>&nbsp;if (!ret) {<BR>&nbsp;&nbsp;down_write(&amp;mm-&gt;mmap_sem);<BR>&nbsp;&nbsp;ret = do_mmap_pgoff(file, addr, len, prot, flag, pgoff,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;populate);<BR>&nbsp;&nbsp;up_write(&amp;mm-&gt;mmap_sem);<BR>&nbsp;&nbsp;if (populate)<BR>&nbsp;&nbsp;&nbsp;mm_populate(ret, populate);<BR>&nbsp;}<BR>&nbsp;return ret;<BR>}</P>
<P>unsigned long vm_mmap(struct file *file, unsigned long addr,<BR>&nbsp;unsigned long len, unsigned long prot,<BR>&nbsp;unsigned long flag, unsigned long offset)<BR>{<BR>&nbsp;if (unlikely(offset + PAGE_ALIGN(len) &lt; offset))<BR>&nbsp;&nbsp;return -EINVAL;<BR>&nbsp;if (unlikely(offset &amp; ~PAGE_MASK))<BR>&nbsp;&nbsp;return -EINVAL;</P>
<P>&nbsp;return vm_mmap_pgoff(file, addr, len, prot, flag, offset &gt;&gt; PAGE_SHIFT);<BR>}<BR>EXPORT_SYMBOL(vm_mmap);</P>
<P>void kvfree(const void *addr)<BR>{<BR>&nbsp;if (is_vmalloc_addr(addr))<BR>&nbsp;&nbsp;vfree(addr);<BR>&nbsp;else<BR>&nbsp;&nbsp;kfree(addr);<BR>}<BR>EXPORT_SYMBOL(kvfree);</P>
<P>static inline void *__page_rmapping(struct page *page)<BR>{<BR>&nbsp;unsigned long mapping;</P>
<P>&nbsp;mapping = (unsigned long)page-&gt;mapping;<BR>&nbsp;mapping &amp;= ~PAGE_MAPPING_FLAGS;</P>
<P>&nbsp;return (void *)mapping;<BR>}</P>
<P>/* Neutral page-&gt;mapping pointer to address_space or anon_vma or other */<BR>void *page_rmapping(struct page *page)<BR>{<BR>&nbsp;page = compound_head(page);<BR>&nbsp;return __page_rmapping(page);<BR>}</P>
<P>struct anon_vma *page_anon_vma(struct page *page)<BR>{<BR>&nbsp;unsigned long mapping;</P>
<P>&nbsp;page = compound_head(page);<BR>&nbsp;mapping = (unsigned long)page-&gt;mapping;<BR>&nbsp;if ((mapping &amp; PAGE_MAPPING_FLAGS) != PAGE_MAPPING_ANON)<BR>&nbsp;&nbsp;return NULL;<BR>&nbsp;return __page_rmapping(page);<BR>}</P>
<P>struct address_space *page_mapping(struct page *page)<BR>{<BR>&nbsp;unsigned long mapping;</P>
<P>&nbsp;/* This happens if someone calls flush_dcache_page on slab page */<BR>&nbsp;if (unlikely(PageSlab(page)))<BR>&nbsp;&nbsp;return NULL;</P>
<P>&nbsp;if (unlikely(PageSwapCache(page))) {<BR>&nbsp;&nbsp;swp_entry_t entry;</P>
<P>&nbsp;&nbsp;entry.val = page_private(page);<BR>&nbsp;&nbsp;return swap_address_space(entry);<BR>&nbsp;}</P>
<P>&nbsp;mapping = (unsigned long)page-&gt;mapping;<BR>&nbsp;if (mapping &amp; PAGE_MAPPING_FLAGS)<BR>&nbsp;&nbsp;return NULL;<BR>&nbsp;return page-&gt;mapping;<BR>}</P>
<P>int overcommit_ratio_handler(struct ctl_table *table, int write,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; void __user *buffer, size_t *lenp,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; loff_t *ppos)<BR>{<BR>&nbsp;int ret;</P>
<P>&nbsp;ret = proc_dointvec(table, write, buffer, lenp, ppos);<BR>&nbsp;if (ret == 0 &amp;&amp; write)<BR>&nbsp;&nbsp;sysctl_overcommit_kbytes = 0;<BR>&nbsp;return ret;<BR>}</P>
<P>int overcommit_kbytes_handler(struct ctl_table *table, int write,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; void __user *buffer, size_t *lenp,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; loff_t *ppos)<BR>{<BR>&nbsp;int ret;</P>
<P>&nbsp;ret = proc_doulongvec_minmax(table, write, buffer, lenp, ppos);<BR>&nbsp;if (ret == 0 &amp;&amp; write)<BR>&nbsp;&nbsp;sysctl_overcommit_ratio = 0;<BR>&nbsp;return ret;<BR>}</P>
<P>/*<BR>&nbsp;* Committed memory limit enforced when OVERCOMMIT_NEVER policy is used<BR>&nbsp;*/<BR>unsigned long vm_commit_limit(void)<BR>{<BR>&nbsp;unsigned long allowed;</P>
<P>&nbsp;if (sysctl_overcommit_kbytes)<BR>&nbsp;&nbsp;allowed = sysctl_overcommit_kbytes &gt;&gt; (PAGE_SHIFT - 10);<BR>&nbsp;else<BR>&nbsp;&nbsp;allowed = ((totalram_pages - hugetlb_total_pages())<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; * sysctl_overcommit_ratio / 100);<BR>&nbsp;allowed += total_swap_pages;</P>
<P>&nbsp;return allowed;<BR>}</P>
<P>/**<BR>&nbsp;* get_cmdline() - copy the cmdline value to a buffer.<BR>&nbsp;* @task:&nbsp;&nbsp;&nbsp;&nbsp; the task whose cmdline value to copy.<BR>&nbsp;* @buffer:&nbsp;&nbsp; the buffer to copy to.<BR>&nbsp;* @buflen:&nbsp;&nbsp; the length of the buffer. Larger cmdline values are truncated<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; to this length.<BR>&nbsp;* Returns the size of the cmdline field copied. Note that the copy does<BR>&nbsp;* not guarantee an ending NULL byte.<BR>&nbsp;*/<BR>int get_cmdline(struct task_struct *task, char *buffer, int buflen)<BR>{<BR>&nbsp;int res = 0;<BR>&nbsp;unsigned int len;<BR>&nbsp;struct mm_struct *mm = get_task_mm(task);<BR>&nbsp;if (!mm)<BR>&nbsp;&nbsp;goto out;<BR>&nbsp;if (!mm-&gt;arg_end)<BR>&nbsp;&nbsp;goto out_mm;&nbsp;/* Shh! No looking before we're done */</P>
<P>&nbsp;len = mm-&gt;arg_end - mm-&gt;arg_start;</P>
<P>&nbsp;if (len &gt; buflen)<BR>&nbsp;&nbsp;len = buflen;</P>
<P>&nbsp;res = access_process_vm(task, mm-&gt;arg_start, buffer, len, 0);</P>
<P>&nbsp;/*<BR>&nbsp; * If the nul at the end of args has been overwritten, then<BR>&nbsp; * assume application is using setproctitle(3).<BR>&nbsp; */<BR>&nbsp;if (res &gt; 0 &amp;&amp; buffer[res-1] != '\0' &amp;&amp; len &lt; buflen) {<BR>&nbsp;&nbsp;len = strnlen(buffer, res);<BR>&nbsp;&nbsp;if (len &lt; res) {<BR>&nbsp;&nbsp;&nbsp;res = len;<BR>&nbsp;&nbsp;} else {<BR>&nbsp;&nbsp;&nbsp;len = mm-&gt;env_end - mm-&gt;env_start;<BR>&nbsp;&nbsp;&nbsp;if (len &gt; buflen - res)<BR>&nbsp;&nbsp;&nbsp;&nbsp;len = buflen - res;<BR>&nbsp;&nbsp;&nbsp;res += access_process_vm(task, mm-&gt;env_start,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; buffer+res, len, 0);<BR>&nbsp;&nbsp;&nbsp;res = strnlen(buffer, res);<BR>&nbsp;&nbsp;}<BR>&nbsp;}<BR>out_mm:<BR>&nbsp;mmput(mm);<BR>out:<BR>&nbsp;return res;<BR>}