# Documentation/crypto/async-tx-api.txt&nbsp; 
<P></P>
<P>&nbsp;&nbsp; Asynchronous Transfers/Transforms API</P>
<P></P>
<P>1 INTRODUCTION</P>
<P>2 GENEALOGY</P>
<P>3 USAGE<BR>3.1 General format of the API<BR>3.2 Supported operations<BR>3.3 Descriptor management<BR>3.4 When does the operation execute?<BR>3.5 When does the operation complete?<BR>3.6 Constraints<BR>3.7 Example</P>
<P>4 DMAENGINE DRIVER DEVELOPER NOTES<BR>4.1 Conformance points<BR>4.2 "My application needs exclusive control of hardware channels"</P>
<P>5 SOURCE</P>
<P>---</P>
<P>1 INTRODUCTION</P>
<P>The async_tx API provides methods for describing a chain of asynchronous<BR>bulk memory transfers/transforms with support for inter-transactional<BR>dependencies.&nbsp; It is implemented as a dmaengine client that smooths over<BR>the details of different hardware offload engine implementations.&nbsp; Code<BR>that is written to the API can optimize for asynchronous operation and<BR>the API will fit the chain of operations to the available offload<BR>resources.</P>
<P>2 GENEALOGY</P>
<P>The API was initially designed to offload the memory copy and<BR>xor-parity-calculations of the md-raid5 driver using the offload engines<BR>present in the Intel(R) Xscale series of I/O processors.&nbsp; It also built<BR>on the 'dmaengine' layer developed for offloading memory copies in the<BR>network stack using Intel(R) I/OAT engines.&nbsp; The following design<BR>features surfaced as a result:<BR>1/ implicit synchronous path: users of the API do not need to know if<BR>&nbsp;&nbsp; the platform they are running on has offload capabilities.&nbsp; The<BR>&nbsp;&nbsp; operation will be offloaded when an engine is available and carried out<BR>&nbsp;&nbsp; in software otherwise.<BR>2/ cross channel dependency chains: the API allows a chain of dependent<BR>&nbsp;&nbsp; operations to be submitted, like xor-&gt;copy-&gt;xor in the raid5 case.&nbsp; The<BR>&nbsp;&nbsp; API automatically handles cases where the transition from one operation<BR>&nbsp;&nbsp; to another implies a hardware channel switch.<BR>3/ dmaengine extensions to support multiple clients and operation types<BR>&nbsp;&nbsp; beyond 'memcpy'</P>
<P>3 USAGE</P>
<P>3.1 General format of the API:<BR>struct dma_async_tx_descriptor *<BR>async_&lt;operation&gt;(&lt;op specific parameters&gt;, struct async_submit ctl *submit)</P>
<P>3.2 Supported operations:<BR>memcpy&nbsp; - memory copy between a source and a destination buffer<BR>memset&nbsp; - fill a destination buffer with a byte value<BR>xor&nbsp;&nbsp;&nbsp;&nbsp; - xor a series of source buffers and write the result to a<BR>&nbsp;&nbsp; destination buffer<BR>xor_val - xor a series of source buffers and set a flag if the<BR>&nbsp;&nbsp; result is zero.&nbsp; The implementation attempts to prevent<BR>&nbsp;&nbsp; writes to memory<BR>pq&nbsp;- generate the p+q (raid6 syndrome) from a series of source buffers<BR>pq_val&nbsp; - validate that a p and or q buffer are in sync with a given series of<BR>&nbsp;&nbsp; sources<BR>datap&nbsp;- (raid6_datap_recov) recover a raid6 data block and the p block<BR>&nbsp;&nbsp; from the given sources<BR>2data&nbsp;- (raid6_2data_recov) recover 2 raid6 data blocks from the given<BR>&nbsp;&nbsp; sources</P>
<P>3.3 Descriptor management:<BR>The return value is non-NULL and points to a 'descriptor' when the operation<BR>has been queued to execute asynchronously.&nbsp; Descriptors are recycled<BR>resources, under control of the offload engine driver, to be reused as<BR>operations complete.&nbsp; When an application needs to submit a chain of<BR>operations it must guarantee that the descriptor is not automatically recycled<BR>before the dependency is submitted.&nbsp; This requires that all descriptors be<BR>acknowledged by the application before the offload engine driver is allowed to<BR>recycle (or free) the descriptor.&nbsp; A descriptor can be acked by one of the<BR>following methods:<BR>1/ setting the ASYNC_TX_ACK flag if no child operations are to be submitted<BR>2/ submitting an unacknowledged descriptor as a dependency to another<BR>&nbsp;&nbsp; async_tx call will implicitly set the acknowledged state.<BR>3/ calling async_tx_ack() on the descriptor.</P>
<P>3.4 When does the operation execute?<BR>Operations do not immediately issue after return from the<BR>async_&lt;operation&gt; call.&nbsp; Offload engine drivers batch operations to<BR>improve performance by reducing the number of mmio cycles needed to<BR>manage the channel.&nbsp; Once a driver-specific threshold is met the driver<BR>automatically issues pending operations.&nbsp; An application can force this<BR>event by calling async_tx_issue_pending_all().&nbsp; This operates on all<BR>channels since the application has no knowledge of channel to operation<BR>mapping.</P>
<P>3.5 When does the operation complete?<BR>There are two methods for an application to learn about the completion<BR>of an operation.<BR>1/ Call dma_wait_for_async_tx().&nbsp; This call causes the CPU to spin while<BR>&nbsp;&nbsp; it polls for the completion of the operation.&nbsp; It handles dependency<BR>&nbsp;&nbsp; chains and issuing pending operations.<BR>2/ Specify a completion callback.&nbsp; The callback routine runs in tasklet<BR>&nbsp;&nbsp; context if the offload engine driver supports interrupts, or it is<BR>&nbsp;&nbsp; called in application context if the operation is carried out<BR>&nbsp;&nbsp; synchronously in software.&nbsp; The callback can be set in the call to<BR>&nbsp;&nbsp; async_&lt;operation&gt;, or when the application needs to submit a chain of<BR>&nbsp;&nbsp; unknown length it can use the async_trigger_callback() routine to set a<BR>&nbsp;&nbsp; completion interrupt/callback at the end of the chain.</P>
<P>3.6 Constraints:<BR>1/ Calls to async_&lt;operation&gt; are not permitted in IRQ context.&nbsp; Other<BR>&nbsp;&nbsp; contexts are permitted provided constraint #2 is not violated.<BR>2/ Completion callback routines cannot submit new operations.&nbsp; This<BR>&nbsp;&nbsp; results in recursion in the synchronous case and spin_locks being<BR>&nbsp;&nbsp; acquired twice in the asynchronous case.</P>
<P>3.7 Example:<BR>Perform a xor-&gt;copy-&gt;xor operation where each operation depends on the<BR>result from the previous operation:</P>
<P>void callback(void *param)<BR>{<BR>&nbsp;struct completion *cmp = param;</P>
<P>&nbsp;complete(cmp);<BR>}</P>
<P>void run_xor_copy_xor(struct page **xor_srcs,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; int xor_src_cnt,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; struct page *xor_dest,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; size_t xor_len,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; struct page *copy_src,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; struct page *copy_dest,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; size_t copy_len)<BR>{<BR>&nbsp;struct dma_async_tx_descriptor *tx;<BR>&nbsp;addr_conv_t addr_conv[xor_src_cnt];<BR>&nbsp;struct async_submit_ctl submit;<BR>&nbsp;addr_conv_t addr_conv[NDISKS];<BR>&nbsp;struct completion cmp;</P>
<P>&nbsp;init_async_submit(&amp;submit, ASYNC_TX_XOR_DROP_DST, NULL, NULL, NULL,<BR>&nbsp;&nbsp;&nbsp;&nbsp; addr_conv);<BR>&nbsp;tx = async_xor(xor_dest, xor_srcs, 0, xor_src_cnt, xor_len, &amp;submit)</P>
<P>&nbsp;submit-&gt;depend_tx = tx;<BR>&nbsp;tx = async_memcpy(copy_dest, copy_src, 0, 0, copy_len, &amp;submit);</P>
<P>&nbsp;init_completion(&amp;cmp);<BR>&nbsp;init_async_submit(&amp;submit, ASYNC_TX_XOR_DROP_DST | ASYNC_TX_ACK, tx,<BR>&nbsp;&nbsp;&nbsp;&nbsp; callback, &amp;cmp, addr_conv);<BR>&nbsp;tx = async_xor(xor_dest, xor_srcs, 0, xor_src_cnt, xor_len, &amp;submit);</P>
<P>&nbsp;async_tx_issue_pending_all();</P>
<P>&nbsp;wait_for_completion(&amp;cmp);<BR>}</P>
<P>See include/linux/async_tx.h for more information on the flags.&nbsp; See the<BR>ops_run_* and ops_complete_* routines in drivers/md/raid5.c for more<BR>implementation examples.</P>
<P>4 DRIVER DEVELOPMENT NOTES</P>
<P>4.1 Conformance points:<BR>There are a few conformance points required in dmaengine drivers to<BR>accommodate assumptions made by applications using the async_tx API:<BR>1/ Completion callbacks are expected to happen in tasklet context<BR>2/ dma_async_tx_descriptor fields are never manipulated in IRQ context<BR>3/ Use async_tx_run_dependencies() in the descriptor clean up path to<BR>&nbsp;&nbsp; handle submission of dependent operations</P>
<P>4.2 "My application needs exclusive control of hardware channels"<BR>Primarily this requirement arises from cases where a DMA engine driver<BR>is being used to support device-to-memory operations.&nbsp; A channel that is<BR>performing these operations cannot, for many platform specific reasons,<BR>be shared.&nbsp; For these cases the dma_request_channel() interface is<BR>provided.</P>
<P>The interface is:<BR>struct dma_chan *dma_request_channel(dma_cap_mask_t mask,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; dma_filter_fn filter_fn,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; void *filter_param);</P>
<P>Where dma_filter_fn is defined as:<BR>typedef bool (*dma_filter_fn)(struct dma_chan *chan, void *filter_param);</P>
<P>When the optional 'filter_fn' parameter is set to NULL<BR>dma_request_channel simply returns the first channel that satisfies the<BR>capability mask.&nbsp; Otherwise, when the mask parameter is insufficient for<BR>specifying the necessary channel, the filter_fn routine can be used to<BR>disposition the available channels in the system. The filter_fn routine<BR>is called once for each free channel in the system.&nbsp; Upon seeing a<BR>suitable channel filter_fn returns DMA_ACK which flags that channel to<BR>be the return value from dma_request_channel.&nbsp; A channel allocated via<BR>this interface is exclusive to the caller, until dma_release_channel()<BR>is called.</P>
<P>The DMA_PRIVATE capability flag is used to tag dma devices that should<BR>not be used by the general-purpose allocator.&nbsp; It can be set at<BR>initialization time if it is known that a channel will always be<BR>private.&nbsp; Alternatively, it is set when dma_request_channel() finds an<BR>unused "public" channel.</P>
<P>A couple caveats to note when implementing a driver and consumer:<BR>1/ Once a channel has been privately allocated it will no longer be<BR>&nbsp;&nbsp; considered by the general-purpose allocator even after a call to<BR>&nbsp;&nbsp; dma_release_channel().<BR>2/ Since capabilities are specified at the device level a dma_device<BR>&nbsp;&nbsp; with multiple channels will either have all channels public, or all<BR>&nbsp;&nbsp; channels private.</P>
<P>5 SOURCE</P>
<P>include/linux/dmaengine.h: core header file for DMA drivers and api users<BR>drivers/dma/dmaengine.c: offload engine channel management routines<BR>drivers/dma/: location for offload engine drivers<BR>include/linux/async_tx.h: core header file for the async_tx api<BR>crypto/async_tx/async_tx.c: async_tx interface to dmaengine and common code<BR>crypto/async_tx/async_memcpy.c: copy offload<BR>crypto/async_tx/async_memset.c: memory fill offload<BR>crypto/async_tx/async_xor.c: xor and xor zero sum offload