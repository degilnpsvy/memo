# Documentation/iostats.txt
<P></P>
<P>I/O statistics fields<BR>---------------</P>
<P></P>
<P>Since 2.4.20 (and some versions before, with patches), and 2.5.45,<BR>more extensive disk statistics have been introduced to help measure disk<BR>activity. Tools such as sar and iostat typically interpret these and do<BR>the work for you, but in case you are interested in creating your own<BR>tools, the fields are explained here.</P>
<P>In 2.4 now, the information is found as additional fields in<BR>/proc/partitions.&nbsp; In 2.6, the same information is found in two<BR>places: one is in the file /proc/diskstats, and the other is within<BR>the sysfs file system, which must be mounted in order to obtain<BR>the information. Throughout this document we'll assume that sysfs<BR>is mounted on /sys, although of course it may be mounted anywhere.<BR>Both /proc/diskstats and sysfs use the same source for the information<BR>and so should not differ.</P>
<P>Here are examples of these different formats:</P>
<P>2.4:<BR>&nbsp;&nbsp; 3&nbsp;&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp; 39082680 hda 446216 784926 9550688 4382310 424847 312726 5922052 19310380 0 3376340 23705160<BR>&nbsp;&nbsp; 3&nbsp;&nbsp;&nbsp;&nbsp; 1&nbsp;&nbsp;&nbsp; 9221278 hda1 35486 0 35496 38030 0 0 0 0 0 38030 38030</P>
<P><BR>2.6 sysfs:<BR>&nbsp;&nbsp; 446216 784926 9550688 4382310 424847 312726 5922052 19310380 0 3376340 23705160<BR>&nbsp;&nbsp; 35486&nbsp;&nbsp;&nbsp; 38030&nbsp;&nbsp;&nbsp; 38030&nbsp;&nbsp;&nbsp; 38030</P>
<P>2.6 diskstats:<BR>&nbsp;&nbsp; 3&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp; hda 446216 784926 9550688 4382310 424847 312726 5922052 19310380 0 3376340 23705160<BR>&nbsp;&nbsp; 3&nbsp;&nbsp;&nbsp; 1&nbsp;&nbsp; hda1 35486 38030 38030 38030</P>
<P>On 2.4 you might execute "grep 'hda ' /proc/partitions". On 2.6, you have<BR>a choice of "cat /sys/block/hda/stat" or "grep 'hda ' /proc/diskstats".<BR>The advantage of one over the other is that the sysfs choice works well<BR>if you are watching a known, small set of disks.&nbsp; /proc/diskstats may<BR>be a better choice if you are watching a large number of disks because<BR>you'll avoid the overhead of 50, 100, or 500 or more opens/closes with<BR>each snapshot of your disk statistics.</P>
<P>In 2.4, the statistics fields are those after the device name. In<BR>the above example, the first field of statistics would be 446216.<BR>By contrast, in 2.6 if you look at /sys/block/hda/stat, you'll<BR>find just the eleven fields, beginning with 446216.&nbsp; If you look at<BR>/proc/diskstats, the eleven fields will be preceded by the major and<BR>minor device numbers, and device name.&nbsp; Each of these formats provides<BR>eleven fields of statistics, each meaning exactly the same things.<BR>All fields except field 9 are cumulative since boot.&nbsp; Field 9 should<BR>go to zero as I/Os complete; all others only increase (unless they<BR>overflow and wrap).&nbsp; Yes, these are (32-bit or 64-bit) unsigned long<BR>(native word size) numbers, and on a very busy or long-lived system they<BR>may wrap. Applications should be prepared to deal with that; unless<BR>your observations are measured in large numbers of minutes or hours,<BR>they should not wrap twice before you notice them.</P>
<P>Each set of stats only applies to the indicated device; if you want<BR>system-wide stats you'll have to find all the devices and sum them all up.</P>
<P>Field&nbsp; 1 -- # of reads completed<BR>&nbsp;&nbsp;&nbsp; This is the total number of reads completed successfully.<BR>Field&nbsp; 2 -- # of reads merged, field 6 -- # of writes merged<BR>&nbsp;&nbsp;&nbsp; Reads and writes which are adjacent to each other may be merged for<BR>&nbsp;&nbsp;&nbsp; efficiency.&nbsp; Thus two 4K reads may become one 8K read before it is<BR>&nbsp;&nbsp;&nbsp; ultimately handed to the disk, and so it will be counted (and queued)<BR>&nbsp;&nbsp;&nbsp; as only one I/O.&nbsp; This field lets you know how often this was done.<BR>Field&nbsp; 3 -- # of sectors read<BR>&nbsp;&nbsp;&nbsp; This is the total number of sectors read successfully.<BR>Field&nbsp; 4 -- # of milliseconds spent reading<BR>&nbsp;&nbsp;&nbsp; This is the total number of milliseconds spent by all reads (as<BR>&nbsp;&nbsp;&nbsp; measured from __make_request() to end_that_request_last()).<BR>Field&nbsp; 5 -- # of writes completed<BR>&nbsp;&nbsp;&nbsp; This is the total number of writes completed successfully.<BR>Field&nbsp; 6 -- # of writes merged<BR>&nbsp;&nbsp;&nbsp; See the description of field 2.<BR>Field&nbsp; 7 -- # of sectors written<BR>&nbsp;&nbsp;&nbsp; This is the total number of sectors written successfully.<BR>Field&nbsp; 8 -- # of milliseconds spent writing<BR>&nbsp;&nbsp;&nbsp; This is the total number of milliseconds spent by all writes (as<BR>&nbsp;&nbsp;&nbsp; measured from __make_request() to end_that_request_last()).<BR>Field&nbsp; 9 -- # of I/Os currently in progress<BR>&nbsp;&nbsp;&nbsp; The only field that should go to zero. Incremented as requests are<BR>&nbsp;&nbsp;&nbsp; given to appropriate struct request_queue and decremented as they finish.<BR>Field 10 -- # of milliseconds spent doing I/Os<BR>&nbsp;&nbsp;&nbsp; This field increases so long as field 9 is nonzero.<BR>Field 11 -- weighted # of milliseconds spent doing I/Os<BR>&nbsp;&nbsp;&nbsp; This field is incremented at each I/O start, I/O completion, I/O<BR>&nbsp;&nbsp;&nbsp; merge, or read of these stats by the number of I/Os in progress<BR>&nbsp;&nbsp;&nbsp; (field 9) times the number of milliseconds spent doing I/O since the<BR>&nbsp;&nbsp;&nbsp; last update of this field.&nbsp; This can provide an easy measure of both<BR>&nbsp;&nbsp;&nbsp; I/O completion time and the backlog that may be accumulating.</P>
<P><BR>To avoid introducing performance bottlenecks, no locks are held while<BR>modifying these counters.&nbsp; This implies that minor inaccuracies may be<BR>introduced when changes collide, so (for instance) adding up all the<BR>read I/Os issued per partition should equal those made to the disks ...<BR>but due to the lack of locking it may only be very close.</P>
<P>In 2.6, there are counters for each CPU, which make the lack of locking<BR>almost a non-issue.&nbsp; When the statistics are read, the per-CPU counters<BR>are summed (possibly overflowing the unsigned long variable they are<BR>summed to) and the result given to the user.&nbsp; There is no convenient<BR>user interface for accessing the per-CPU counters themselves.</P>
<P>Disks vs Partitions<BR>-------------------</P>
<P>There were significant changes between 2.4 and 2.6 in the I/O subsystem.<BR>As a result, some statistic information disappeared. The translation from<BR>a disk address relative to a partition to the disk address relative to<BR>the host disk happens much earlier.&nbsp; All merges and timings now happen<BR>at the disk level rather than at both the disk and partition level as<BR>in 2.4.&nbsp; Consequently, you'll see a different statistics output on 2.6 for<BR>partitions from that for disks.&nbsp; There are only *four* fields available<BR>for partitions on 2.6 machines.&nbsp; This is reflected in the examples above.</P>
<P>Field&nbsp; 1 -- # of reads issued<BR>&nbsp;&nbsp;&nbsp; This is the total number of reads issued to this partition.<BR>Field&nbsp; 2 -- # of sectors read<BR>&nbsp;&nbsp;&nbsp; This is the total number of sectors requested to be read from this<BR>&nbsp;&nbsp;&nbsp; partition.<BR>Field&nbsp; 3 -- # of writes issued<BR>&nbsp;&nbsp;&nbsp; This is the total number of writes issued to this partition.<BR>Field&nbsp; 4 -- # of sectors written<BR>&nbsp;&nbsp;&nbsp; This is the total number of sectors requested to be written to<BR>&nbsp;&nbsp;&nbsp; this partition.</P>
<P>Note that since the address is translated to a disk-relative one, and no<BR>record of the partition-relative address is kept, the subsequent success<BR>or failure of the read cannot be attributed to the partition.&nbsp; In other<BR>words, the number of reads for partitions is counted slightly before time<BR>of queuing for partitions, and at completion for whole disks.&nbsp; This is<BR>a subtle distinction that is probably uninteresting for most cases.</P>
<P>More significant is the error induced by counting the numbers of<BR>reads/writes before merges for partitions and after for disks. Since a<BR>typical workload usually contains a lot of successive and adjacent requests,<BR>the number of reads/writes issued can be several times higher than the<BR>number of reads/writes completed.</P>
<P>In 2.6.25, the full statistic set is again available for partitions and<BR>disk and partition statistics are consistent again. Since we still don't<BR>keep record of the partition-relative address, an operation is attributed to<BR>the partition which contains the first sector of the request after the<BR>eventual merges. As requests can be merged across partition, this could lead<BR>to some (probably insignificant) inaccuracy.</P>
<P>Additional notes<BR>----------------</P>
<P>In 2.6, sysfs is not mounted by default.&nbsp; If your distribution of<BR>Linux hasn't added it already, here's the line you'll want to add to<BR>your /etc/fstab:</P>
<P>none /sys sysfs defaults 0 0</P>
<P><BR>In 2.6, all disk statistics were removed from /proc/stat.&nbsp; In 2.4, they<BR>appear in both /proc/partitions and /proc/stat, although the ones in<BR>/proc/stat take a very different format from those in /proc/partitions<BR>(see proc(5), if your system has it.)</P>
<P>-- <A href="mailto:ricklind@us.ibm.com">ricklind@us.ibm.com</A>