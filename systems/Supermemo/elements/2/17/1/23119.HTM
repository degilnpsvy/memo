<H3 id=-100000 class=docSection1Title>Network Throughput</H3>
<P class=docText><A name="Several tools"></A>Several tools are available to benchmark network performance. <SPAN class=docEmphasis>Netperf</SPAN><A name="free from"></A>, available for free from <A class=docLink href="http://www.netperf.org/" target=_blank>www.netperf.org</A><A name="set up"></A>, can set up complex TCP/UDP connection scenarios. You can use scripts to control characteristics such as protocol parameters, number of simultaneous sessions, and size of data blocks. Benchmarking is accomplished by comparing the resulting throughput with the maximum practical bandwidth that the networking technology yields. For example, a 155Mbps ATM adapter produces a maximum IP throughput of 135Mbps, taking into account the ATM cell header size, overheads due to the <SPAN class=docEmphasis>ATM Adaptation Layer</SPAN> (AAL), and the occasional maintenance cells sent by the physical <SPAN class=docEmphasis>Synchronous Optical Networking</SPAN> (SONET) layer.</P>
<P class=docText><A name="driver for"></A>To obtain optimal throughput, you have to design your NIC driver for high performance. In addition, you need an in-depth understanding of the network protocol that your driver ferries.</P><A name=ch15lev2sec12></A>
<H4 id=title-ID0EOHAO class=docSection2Title>Driver Performance</H4>
<P class=docText><A name="driver design"></A>Let's take a look at some driver design issues that can affect the horsepower of your NIC:</P>
<UL>
<LI>
<P class=docList><A name="path is"></A>Minimizing the number of instructions in the main data path is a key criterion while designing drivers for fast NICs. Consider a 1Gbps Ethernet adapter with 1MB of on-board memory. At line rate, the card memory can hold up to 8 milliseconds of received data. This directly translates to the maximum allowable instruction path length. Within this path length, incoming packets have to be reassembled, DMAed to memory, processed by the driver, protected from concurrent access, and delivered to higher layer protocols.</P></LI>
<LI>
<P class=docList><A name=iddle3569></A><A name=iddle3672></A>During <SPAN class=docEmphasis>programmed I/O</SPAN><A name="device to"></A> (PIO), data travels all the way from the device to the CPU, before it gets written to memory. Moreover, the CPU gets interrupted whenever the device needs to transfer data, and this contributes to latencies and context switch delays. DMAs do not suffer from these bottlenecks, but can turn out to be more expensive than PIOs if the data to be transferred is less than a threshold. This is because small DMAs have high relative overheads for building descriptors and flushing corresponding processor cache lines for data coherency. A performance-sensitive device driver might use PIO for small packets and DMA for larger ones, after experimentally determining the threshold.</P></LI>
<LI>
<P class=docList><A name="determine the"></A>For PCI network cards having DMA mastering capability, you have to determine the optimal DMA burst size, which is the time for which the card controls the bus at one stretch. If the card bursts for a long duration, it may hog the bus and prevent the processor from keeping up with data DMA-ed previously. PCI drivers program the burst size via a register in the PCI configuration space. Normally the NIC's burst size is programmed to be the same as the cache line size of the processor, which is the number of bytes that the processor reads from system memory each time there is a cache miss. In practice, however, you might need to connect a bus analyzer to determine the beneficial burst duration because factors such as the presence of a split bus (multiple bus types like ISA and PCI) on your system can influence the optimal value.</P></LI>
<LI>
<P class=docList><A name="the capability"></A>Many high-speed NICs offer the capability to offload the CPU-intensive computation of TCP checksums from the protocol stack. Some support DMA scatter-gather that we visited in <A class=docLink href="http://www.embeddedlinux.org.cn/EssentialLinuxDeviceDrivers/final/ch10.html#ch10">Chapter 10</A><A name="needs to"></A>. The driver needs to leverage these capabilities to achieve the maximum practical bandwidth that the underlying network yields.</P></LI>
<LI>
<P class=docList><A name="a driver"></A>Sometimes, a driver optimization might create unexpected speed bumps if it's not sensitive to the implementation details of higher protocols. Consider an NFS-mounted filesystem on a computer equipped with a high-speed NIC. Assume that the NIC driver takes only occasional transmit complete interrupts to minimize latencies, but that the NFS server implementation uses freeing of its transmit buffers as a flow-control mechanism. Because the driver frees NFS transmit buffers only during the sparsely generated transmit complete interrupts, file copies over NFS crawl, even as Internet downloads zip along yielding maximum throughput.</P></LI></UL><A name=ch15lev2sec13></A>
<H4 id=title-ID0ELJAO class=docSection2Title>Protocol Performance</H4>
<P class=docText><A name=iddle3258></A><A name=iddle3282></A><A name=iddle3288></A><A name=iddle3557></A><A name=iddle4097></A><A name=iddle4255></A><A name="into some"></A>Let's now dig into some protocol-specific characteristics that can boost or hurt network throughput:</P>
<UL>
<LI>
<P class=docList><A name="window size"></A>TCP window size can impact throughput. The window size provides a measure of the amount of data that can be transmitted before receiving an acknowledgment. For fast NICs, a small window size might result in TCP sitting idle, waiting for acknowledgments of packets already transmitted. Even with a large window size, a small number of lost TCP packets can affect performance because lost frames can use up the window at line speeds. In the case of UDP, the window size is not relevant because it does not support acknowledgments. However, a small packet loss can spiral into a big rate drop due to the absence of flow-control mechanisms.</P></LI>
<LI>
<P class=docList><A name="TCP sockets"></A>As the block size of application data written to TCP sockets increases, the number of buffers copied from user space to kernel space decreases. This lowers the demand on processor utilization and is good for performance. If the block size crosses the MTU corresponding to the network protocol, however, processor cycles get wasted on fragmentation. The desirable block size is thus the outgoing interface MTU, or the largest packet that can be sent without fragmentation through an IP path if Path MTU discovery mechanisms are in operation. While running IP over ATM, for example, because the ATM adaptation layer has a 64K MTU, there is virtually no upper bound on block size. (RFC 1626 defaults this to 9180.) If you are running IP over ATM LANE, however, the block size should mirror the MTU size of the respective LAN technology being emulated. It should thus be 1500 for standard Ethernet, 8000 for jumbo Gigabit Ethernet, and 18K for 16Mbps Token Ring.</P></LI></UL>