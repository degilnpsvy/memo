# Documentation/RCU/whatisRCU.txt&nbsp;
<P></P>
<P>Please note that the "What is RCU?" LWN series is an excellent place<BR>to start learning about RCU:</P>
<P></P>
<P>1.&nbsp;What is RCU, Fundamentally?&nbsp; <A href="http://lwn.net/Articles/262464/">http://lwn.net/Articles/262464/</A><BR>2.&nbsp;What is RCU? Part 2: Usage&nbsp;&nbsp; <A href="http://lwn.net/Articles/263130/">http://lwn.net/Articles/263130/</A><BR>3.&nbsp;RCU part 3: the RCU API&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <A href="http://lwn.net/Articles/264090/">http://lwn.net/Articles/264090/</A><BR>4.&nbsp;The RCU API, 2010 Edition&nbsp;&nbsp;&nbsp; <A href="http://lwn.net/Articles/418853/">http://lwn.net/Articles/418853/</A></P>
<P><BR>What is RCU?</P>
<P>RCU is a synchronization mechanism that was added to the Linux kernel<BR>during the 2.5 development effort that is optimized for read-mostly<BR>situations.&nbsp; Although RCU is actually quite simple once you understand it,<BR>getting there can sometimes be a challenge.&nbsp; Part of the problem is that<BR>most of the past descriptions of RCU have been written with the mistaken<BR>assumption that there is "one true way" to describe RCU.&nbsp; Instead,<BR>the experience has been that different people must take different paths<BR>to arrive at an understanding of RCU.&nbsp; This document provides several<BR>different paths, as follows:</P>
<P>1.&nbsp;RCU OVERVIEW<BR>2.&nbsp;WHAT IS RCU'S CORE API?<BR>3.&nbsp;WHAT ARE SOME EXAMPLE USES OF CORE RCU API?<BR>4.&nbsp;WHAT IF MY UPDATING THREAD CANNOT BLOCK?<BR>5.&nbsp;WHAT ARE SOME SIMPLE IMPLEMENTATIONS OF RCU?<BR>6.&nbsp;ANALOGY WITH READER-WRITER LOCKING<BR>7.&nbsp;FULL LIST OF RCU APIs<BR>8.&nbsp;ANSWERS TO QUICK QUIZZES</P>
<P>People who prefer starting with a conceptual overview should focus on<BR>Section 1, though most readers will profit by reading this section at<BR>some point.&nbsp; People who prefer to start with an API that they can then<BR>experiment with should focus on Section 2.&nbsp; People who prefer to start<BR>with example uses should focus on Sections 3 and 4.&nbsp; People who need to<BR>understand the RCU implementation should focus on Section 5, then dive<BR>into the kernel source code.&nbsp; People who reason best by analogy should<BR>focus on Section 6.&nbsp; Section 7 serves as an index to the docbook API<BR>documentation, and Section 8 is the traditional answer key.</P>
<P>So, start with the section that makes the most sense to you and your<BR>preferred method of learning.&nbsp; If you need to know everything about<BR>everything, feel free to read the whole thing -- but if you are really<BR>that type of person, you have perused the source code and will therefore<BR>never need this document anyway.&nbsp; ;-)</P>
<P><BR>1.&nbsp; RCU OVERVIEW</P>
<P>The basic idea behind RCU is to split updates into "removal" and<BR>"reclamation" phases.&nbsp; The removal phase removes references to data items<BR>within a data structure (possibly by replacing them with references to<BR>new versions of these data items), and can run concurrently with readers.<BR>The reason that it is safe to run the removal phase concurrently with<BR>readers is the semantics of modern CPUs guarantee that readers will see<BR>either the old or the new version of the data structure rather than a<BR>partially updated reference.&nbsp; The reclamation phase does the work of reclaiming<BR>(e.g., freeing) the data items removed from the data structure during the<BR>removal phase.&nbsp; Because reclaiming data items can disrupt any readers<BR>concurrently referencing those data items, the reclamation phase must<BR>not start until readers no longer hold references to those data items.</P>
<P>Splitting the update into removal and reclamation phases permits the<BR>updater to perform the removal phase immediately, and to defer the<BR>reclamation phase until all readers active during the removal phase have<BR>completed, either by blocking until they finish or by registering a<BR>callback that is invoked after they finish.&nbsp; Only readers that are active<BR>during the removal phase need be considered, because any reader starting<BR>after the removal phase will be unable to gain a reference to the removed<BR>data items, and therefore cannot be disrupted by the reclamation phase.</P>
<P>So the typical RCU update sequence goes something like the following:</P>
<P>a.&nbsp;Remove pointers to a data structure, so that subsequent<BR>&nbsp;readers cannot gain a reference to it.</P>
<P>b.&nbsp;Wait for all previous readers to complete their RCU read-side<BR>&nbsp;critical sections.</P>
<P>c.&nbsp;At this point, there cannot be any readers who hold references<BR>&nbsp;to the data structure, so it now may safely be reclaimed<BR>&nbsp;(e.g., kfree()d).</P>
<P>Step (b) above is the key idea underlying RCU's deferred destruction.<BR>The ability to wait until all readers are done allows RCU readers to<BR>use much lighter-weight synchronization, in some cases, absolutely no<BR>synchronization at all.&nbsp; In contrast, in more conventional lock-based<BR>schemes, readers must use heavy-weight synchronization in order to<BR>prevent an updater from deleting the data structure out from under them.<BR>This is because lock-based updaters typically update data items in place,<BR>and must therefore exclude readers.&nbsp; In contrast, RCU-based updaters<BR>typically take advantage of the fact that writes to single aligned<BR>pointers are atomic on modern CPUs, allowing atomic insertion, removal,<BR>and replacement of data items in a linked structure without disrupting<BR>readers.&nbsp; Concurrent RCU readers can then continue accessing the old<BR>versions, and can dispense with the atomic operations, memory barriers,<BR>and communications cache misses that are so expensive on present-day<BR>SMP computer systems, even in absence of lock contention.</P>
<P>In the three-step procedure shown above, the updater is performing both<BR>the removal and the reclamation step, but it is often helpful for an<BR>entirely different thread to do the reclamation, as is in fact the case<BR>in the Linux kernel's directory-entry cache (dcache).&nbsp; Even if the same<BR>thread performs both the update step (step (a) above) and the reclamation<BR>step (step (c) above), it is often helpful to think of them separately.<BR>For example, RCU readers and updaters need not communicate at all,<BR>but RCU provides implicit low-overhead communication between readers<BR>and reclaimers, namely, in step (b) above.</P>
<P>So how the heck can a reclaimer tell when a reader is done, given<BR>that readers are not doing any sort of synchronization operations???<BR>Read on to learn about how RCU's API makes this easy.</P>
<P><BR>2.&nbsp; WHAT IS RCU'S CORE API?</P>
<P>The core RCU API is quite small:</P>
<P>a.&nbsp;rcu_read_lock()<BR>b.&nbsp;rcu_read_unlock()<BR>c.&nbsp;synchronize_rcu() / call_rcu()<BR>d.&nbsp;rcu_assign_pointer()<BR>e.&nbsp;rcu_dereference()</P>
<P>There are many other members of the RCU API, but the rest can be<BR>expressed in terms of these five, though most implementations instead<BR>express synchronize_rcu() in terms of the call_rcu() callback API.</P>
<P>The five core RCU APIs are described below, the other 18 will be enumerated<BR>later.&nbsp; See the kernel docbook documentation for more info, or look directly<BR>at the function header comments.</P>
<P>rcu_read_lock()</P>
<P>&nbsp;void rcu_read_lock(void);</P>
<P>&nbsp;Used by a reader to inform the reclaimer that the reader is<BR>&nbsp;entering an RCU read-side critical section.&nbsp; It is illegal<BR>&nbsp;to block while in an RCU read-side critical section, though<BR>&nbsp;kernels built with CONFIG_TREE_PREEMPT_RCU can preempt RCU<BR>&nbsp;read-side critical sections.&nbsp; Any RCU-protected data structure<BR>&nbsp;accessed during an RCU read-side critical section is guaranteed to<BR>&nbsp;remain unreclaimed for the full duration of that critical section.<BR>&nbsp;Reference counts may be used in conjunction with RCU to maintain<BR>&nbsp;longer-term references to data structures.</P>
<P>rcu_read_unlock()</P>
<P>&nbsp;void rcu_read_unlock(void);</P>
<P>&nbsp;Used by a reader to inform the reclaimer that the reader is<BR>&nbsp;exiting an RCU read-side critical section.&nbsp; Note that RCU<BR>&nbsp;read-side critical sections may be nested and/or overlapping.</P>
<P>synchronize_rcu()</P>
<P>&nbsp;void synchronize_rcu(void);</P>
<P>&nbsp;Marks the end of updater code and the beginning of reclaimer<BR>&nbsp;code.&nbsp; It does this by blocking until all pre-existing RCU<BR>&nbsp;read-side critical sections on all CPUs have completed.<BR>&nbsp;Note that synchronize_rcu() will -not- necessarily wait for<BR>&nbsp;any subsequent RCU read-side critical sections to complete.<BR>&nbsp;For example, consider the following sequence of events:</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; CPU 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; CPU 1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; CPU 2<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ----------------- ------------------------- ---------------<BR>&nbsp; 1.&nbsp; rcu_read_lock()<BR>&nbsp; 2.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; enters synchronize_rcu()<BR>&nbsp; 3.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; rcu_read_lock()<BR>&nbsp; 4.&nbsp; rcu_read_unlock()<BR>&nbsp; 5.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; exits synchronize_rcu()<BR>&nbsp; 6.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; rcu_read_unlock()</P>
<P>&nbsp;To reiterate, synchronize_rcu() waits only for ongoing RCU<BR>&nbsp;read-side critical sections to complete, not necessarily for<BR>&nbsp;any that begin after synchronize_rcu() is invoked.</P>
<P>&nbsp;Of course, synchronize_rcu() does not necessarily return<BR>&nbsp;-immediately- after the last pre-existing RCU read-side critical<BR>&nbsp;section completes.&nbsp; For one thing, there might well be scheduling<BR>&nbsp;delays.&nbsp; For another thing, many RCU implementations process<BR>&nbsp;requests in batches in order to improve efficiencies, which can<BR>&nbsp;further delay synchronize_rcu().</P>
<P>&nbsp;Since synchronize_rcu() is the API that must figure out when<BR>&nbsp;readers are done, its implementation is key to RCU.&nbsp; For RCU<BR>&nbsp;to be useful in all but the most read-intensive situations,<BR>&nbsp;synchronize_rcu()'s overhead must also be quite small.</P>
<P>&nbsp;The call_rcu() API is a callback form of synchronize_rcu(),<BR>&nbsp;and is described in more detail in a later section.&nbsp; Instead of<BR>&nbsp;blocking, it registers a function and argument which are invoked<BR>&nbsp;after all ongoing RCU read-side critical sections have completed.<BR>&nbsp;This callback variant is particularly useful in situations where<BR>&nbsp;it is illegal to block or where update-side performance is<BR>&nbsp;critically important.</P>
<P>&nbsp;However, the call_rcu() API should not be used lightly, as use<BR>&nbsp;of the synchronize_rcu() API generally results in simpler code.<BR>&nbsp;In addition, the synchronize_rcu() API has the nice property<BR>&nbsp;of automatically limiting update rate should grace periods<BR>&nbsp;be delayed.&nbsp; This property results in system resilience in face<BR>&nbsp;of denial-of-service attacks.&nbsp; Code using call_rcu() should limit<BR>&nbsp;update rate in order to gain this same sort of resilience.&nbsp; See<BR>&nbsp;checklist.txt for some approaches to limiting the update rate.</P>
<P>rcu_assign_pointer()</P>
<P>&nbsp;typeof(p) rcu_assign_pointer(p, typeof(p) v);</P>
<P>&nbsp;Yes, rcu_assign_pointer() -is- implemented as a macro, though it<BR>&nbsp;would be cool to be able to declare a function in this manner.<BR>&nbsp;(Compiler experts will no doubt disagree.)</P>
<P>&nbsp;The updater uses this function to assign a new value to an<BR>&nbsp;RCU-protected pointer, in order to safely communicate the change<BR>&nbsp;in value from the updater to the reader.&nbsp; This function returns<BR>&nbsp;the new value, and also executes any memory-barrier instructions<BR>&nbsp;required for a given CPU architecture.</P>
<P>&nbsp;Perhaps just as important, it serves to document (1) which<BR>&nbsp;pointers are protected by RCU and (2) the point at which a<BR>&nbsp;given structure becomes accessible to other CPUs.&nbsp; That said,<BR>&nbsp;rcu_assign_pointer() is most frequently used indirectly, via<BR>&nbsp;the _rcu list-manipulation primitives such as list_add_rcu().</P>
<P>rcu_dereference()</P>
<P>&nbsp;typeof(p) rcu_dereference(p);</P>
<P>&nbsp;Like rcu_assign_pointer(), rcu_dereference() must be implemented<BR>&nbsp;as a macro.</P>
<P>&nbsp;The reader uses rcu_dereference() to fetch an RCU-protected<BR>&nbsp;pointer, which returns a value that may then be safely<BR>&nbsp;dereferenced.&nbsp; Note that rcu_deference() does not actually<BR>&nbsp;dereference the pointer, instead, it protects the pointer for<BR>&nbsp;later dereferencing.&nbsp; It also executes any needed memory-barrier<BR>&nbsp;instructions for a given CPU architecture.&nbsp; Currently, only Alpha<BR>&nbsp;needs memory barriers within rcu_dereference() -- on other CPUs,<BR>&nbsp;it compiles to nothing, not even a compiler directive.</P>
<P>&nbsp;Common coding practice uses rcu_dereference() to copy an<BR>&nbsp;RCU-protected pointer to a local variable, then dereferences<BR>&nbsp;this local variable, for example as follows:</P>
<P>&nbsp;&nbsp;p = rcu_dereference(head.next);<BR>&nbsp;&nbsp;return p-&gt;data;</P>
<P>&nbsp;However, in this case, one could just as easily combine these<BR>&nbsp;into one statement:</P>
<P>&nbsp;&nbsp;return rcu_dereference(head.next)-&gt;data;</P>
<P>&nbsp;If you are going to be fetching multiple fields from the<BR>&nbsp;RCU-protected structure, using the local variable is of<BR>&nbsp;course preferred.&nbsp; Repeated rcu_dereference() calls look<BR>&nbsp;ugly and incur unnecessary overhead on Alpha CPUs.</P>
<P>&nbsp;Note that the value returned by rcu_dereference() is valid<BR>&nbsp;only within the enclosing RCU read-side critical section.<BR>&nbsp;For example, the following is -not- legal:</P>
<P>&nbsp;&nbsp;rcu_read_lock();<BR>&nbsp;&nbsp;p = rcu_dereference(head.next);<BR>&nbsp;&nbsp;rcu_read_unlock();<BR>&nbsp;&nbsp;x = p-&gt;address;&nbsp;/* BUG!!! */<BR>&nbsp;&nbsp;rcu_read_lock();<BR>&nbsp;&nbsp;y = p-&gt;data;&nbsp;/* BUG!!! */<BR>&nbsp;&nbsp;rcu_read_unlock();</P>
<P>&nbsp;Holding a reference from one RCU read-side critical section<BR>&nbsp;to another is just as illegal as holding a reference from<BR>&nbsp;one lock-based critical section to another!&nbsp; Similarly,<BR>&nbsp;using a reference outside of the critical section in which<BR>&nbsp;it was acquired is just as illegal as doing so with normal<BR>&nbsp;locking.</P>
<P>&nbsp;As with rcu_assign_pointer(), an important function of<BR>&nbsp;rcu_dereference() is to document which pointers are protected by<BR>&nbsp;RCU, in particular, flagging a pointer that is subject to changing<BR>&nbsp;at any time, including immediately after the rcu_dereference().<BR>&nbsp;And, again like rcu_assign_pointer(), rcu_dereference() is<BR>&nbsp;typically used indirectly, via the _rcu list-manipulation<BR>&nbsp;primitives, such as list_for_each_entry_rcu().</P>
<P>The following diagram shows how each API communicates among the<BR>reader, updater, and reclaimer.</P>
<P><BR>&nbsp;&nbsp;&nbsp;&nbsp; rcu_assign_pointer()<BR>&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; +--------+<BR>&nbsp;&nbsp;&nbsp;&nbsp; +----------------------&gt;| reader |---------+<BR>&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; +--------+&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |<BR>&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |<BR>&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | Protect:<BR>&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | rcu_read_lock()<BR>&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | rcu_read_unlock()<BR>&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; rcu_dereference()&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; +---------+&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | updater |&lt;---------------------+&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; +---------+&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; V<BR>&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; +-----------+<BR>&nbsp;&nbsp;&nbsp;&nbsp; +-----------------------------------&gt;| reclaimer |<BR>&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; +-----------+<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Defer:<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; synchronize_rcu() &amp; call_rcu()</P>
<P><BR>The RCU infrastructure observes the time sequence of rcu_read_lock(),<BR>rcu_read_unlock(), synchronize_rcu(), and call_rcu() invocations in<BR>order to determine when (1) synchronize_rcu() invocations may return<BR>to their callers and (2) call_rcu() callbacks may be invoked.&nbsp; Efficient<BR>implementations of the RCU infrastructure make heavy use of batching in<BR>order to amortize their overhead over many uses of the corresponding APIs.</P>
<P>There are no fewer than three RCU mechanisms in the Linux kernel; the<BR>diagram above shows the first one, which is by far the most commonly used.<BR>The rcu_dereference() and rcu_assign_pointer() primitives are used for<BR>all three mechanisms, but different defer and protect primitives are<BR>used as follows:</P>
<P>&nbsp;Defer&nbsp;&nbsp;&nbsp;Protect</P>
<P>a.&nbsp;synchronize_rcu()&nbsp;rcu_read_lock() / rcu_read_unlock()<BR>&nbsp;call_rcu()&nbsp;&nbsp;rcu_dereference()</P>
<P>b.&nbsp;call_rcu_bh()&nbsp;&nbsp;rcu_read_lock_bh() / rcu_read_unlock_bh()<BR>&nbsp;&nbsp;&nbsp;&nbsp;rcu_dereference_bh()</P>
<P>c.&nbsp;synchronize_sched()&nbsp;rcu_read_lock_sched() / rcu_read_unlock_sched()<BR>&nbsp;&nbsp;&nbsp;&nbsp;preempt_disable() / preempt_enable()<BR>&nbsp;&nbsp;&nbsp;&nbsp;local_irq_save() / local_irq_restore()<BR>&nbsp;&nbsp;&nbsp;&nbsp;hardirq enter / hardirq exit<BR>&nbsp;&nbsp;&nbsp;&nbsp;NMI enter / NMI exit<BR>&nbsp;&nbsp;&nbsp;&nbsp;rcu_dereference_sched()</P>
<P>These three mechanisms are used as follows:</P>
<P>a.&nbsp;RCU applied to normal data structures.</P>
<P>b.&nbsp;RCU applied to networking data structures that may be subjected<BR>&nbsp;to remote denial-of-service attacks.</P>
<P>c.&nbsp;RCU applied to scheduler and interrupt/NMI-handler tasks.</P>
<P>Again, most uses will be of (a).&nbsp; The (b) and (c) cases are important<BR>for specialized uses, but are relatively uncommon.</P>
<P><BR>3.&nbsp; WHAT ARE SOME EXAMPLE USES OF CORE RCU API?</P>
<P>This section shows a simple use of the core RCU API to protect a<BR>global pointer to a dynamically allocated structure.&nbsp; More-typical<BR>uses of RCU may be found in listRCU.txt, arrayRCU.txt, and NMI-RCU.txt.</P>
<P>&nbsp;struct foo {<BR>&nbsp;&nbsp;int a;<BR>&nbsp;&nbsp;char b;<BR>&nbsp;&nbsp;long c;<BR>&nbsp;};<BR>&nbsp;DEFINE_SPINLOCK(foo_mutex);</P>
<P>&nbsp;struct foo *gbl_foo;</P>
<P>&nbsp;/*<BR>&nbsp; * Create a new struct foo that is the same as the one currently<BR>&nbsp; * pointed to by gbl_foo, except that field "a" is replaced<BR>&nbsp; * with "new_a".&nbsp; Points gbl_foo to the new structure, and<BR>&nbsp; * frees up the old structure after a grace period.<BR>&nbsp; *<BR>&nbsp; * Uses rcu_assign_pointer() to ensure that concurrent readers<BR>&nbsp; * see the initialized version of the new structure.<BR>&nbsp; *<BR>&nbsp; * Uses synchronize_rcu() to ensure that any readers that might<BR>&nbsp; * have references to the old structure complete before freeing<BR>&nbsp; * the old structure.<BR>&nbsp; */<BR>&nbsp;void foo_update_a(int new_a)<BR>&nbsp;{<BR>&nbsp;&nbsp;struct foo *new_fp;<BR>&nbsp;&nbsp;struct foo *old_fp;</P>
<P>&nbsp;&nbsp;new_fp = kmalloc(sizeof(*new_fp), GFP_KERNEL);<BR>&nbsp;&nbsp;spin_lock(&amp;foo_mutex);<BR>&nbsp;&nbsp;old_fp = gbl_foo;<BR>&nbsp;&nbsp;*new_fp = *old_fp;<BR>&nbsp;&nbsp;new_fp-&gt;a = new_a;<BR>&nbsp;&nbsp;rcu_assign_pointer(gbl_foo, new_fp);<BR>&nbsp;&nbsp;spin_unlock(&amp;foo_mutex);<BR>&nbsp;&nbsp;synchronize_rcu();<BR>&nbsp;&nbsp;kfree(old_fp);<BR>&nbsp;}</P>
<P>&nbsp;/*<BR>&nbsp; * Return the value of field "a" of the current gbl_foo<BR>&nbsp; * structure.&nbsp; Use rcu_read_lock() and rcu_read_unlock()<BR>&nbsp; * to ensure that the structure does not get deleted out<BR>&nbsp; * from under us, and use rcu_dereference() to ensure that<BR>&nbsp; * we see the initialized version of the structure (important<BR>&nbsp; * for DEC Alpha and for people reading the code).<BR>&nbsp; */<BR>&nbsp;int foo_get_a(void)<BR>&nbsp;{<BR>&nbsp;&nbsp;int retval;</P>
<P>&nbsp;&nbsp;rcu_read_lock();<BR>&nbsp;&nbsp;retval = rcu_dereference(gbl_foo)-&gt;a;<BR>&nbsp;&nbsp;rcu_read_unlock();<BR>&nbsp;&nbsp;return retval;<BR>&nbsp;}</P>
<P>So, to sum up:</P>
<P>o&nbsp;Use rcu_read_lock() and rcu_read_unlock() to guard RCU<BR>&nbsp;read-side critical sections.</P>
<P>o&nbsp;Within an RCU read-side critical section, use rcu_dereference()<BR>&nbsp;to dereference RCU-protected pointers.</P>
<P>o&nbsp;Use some solid scheme (such as locks or semaphores) to<BR>&nbsp;keep concurrent updates from interfering with each other.</P>
<P>o&nbsp;Use rcu_assign_pointer() to update an RCU-protected pointer.<BR>&nbsp;This primitive protects concurrent readers from the updater,<BR>&nbsp;-not- concurrent updates from each other!&nbsp; You therefore still<BR>&nbsp;need to use locking (or something similar) to keep concurrent<BR>&nbsp;rcu_assign_pointer() primitives from interfering with each other.</P>
<P>o&nbsp;Use synchronize_rcu() -after- removing a data element from an<BR>&nbsp;RCU-protected data structure, but -before- reclaiming/freeing<BR>&nbsp;the data element, in order to wait for the completion of all<BR>&nbsp;RCU read-side critical sections that might be referencing that<BR>&nbsp;data item.</P>
<P>See checklist.txt for additional rules to follow when using RCU.<BR>And again, more-typical uses of RCU may be found in listRCU.txt,<BR>arrayRCU.txt, and NMI-RCU.txt.</P>
<P><BR>4.&nbsp; WHAT IF MY UPDATING THREAD CANNOT BLOCK?</P>
<P>In the example above, foo_update_a() blocks until a grace period elapses.<BR>This is quite simple, but in some cases one cannot afford to wait so<BR>long -- there might be other high-priority work to be done.</P>
<P>In such cases, one uses call_rcu() rather than synchronize_rcu().<BR>The call_rcu() API is as follows:</P>
<P>&nbsp;void call_rcu(struct rcu_head * head,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; void (*func)(struct rcu_head *head));</P>
<P>This function invokes func(head) after a grace period has elapsed.<BR>This invocation might happen from either softirq or process context,<BR>so the function is not permitted to block.&nbsp; The foo struct needs to<BR>have an rcu_head structure added, perhaps as follows:</P>
<P>&nbsp;struct foo {<BR>&nbsp;&nbsp;int a;<BR>&nbsp;&nbsp;char b;<BR>&nbsp;&nbsp;long c;<BR>&nbsp;&nbsp;struct rcu_head rcu;<BR>&nbsp;};</P>
<P>The foo_update_a() function might then be written as follows:</P>
<P>&nbsp;/*<BR>&nbsp; * Create a new struct foo that is the same as the one currently<BR>&nbsp; * pointed to by gbl_foo, except that field "a" is replaced<BR>&nbsp; * with "new_a".&nbsp; Points gbl_foo to the new structure, and<BR>&nbsp; * frees up the old structure after a grace period.<BR>&nbsp; *<BR>&nbsp; * Uses rcu_assign_pointer() to ensure that concurrent readers<BR>&nbsp; * see the initialized version of the new structure.<BR>&nbsp; *<BR>&nbsp; * Uses call_rcu() to ensure that any readers that might have<BR>&nbsp; * references to the old structure complete before freeing the<BR>&nbsp; * old structure.<BR>&nbsp; */<BR>&nbsp;void foo_update_a(int new_a)<BR>&nbsp;{<BR>&nbsp;&nbsp;struct foo *new_fp;<BR>&nbsp;&nbsp;struct foo *old_fp;</P>
<P>&nbsp;&nbsp;new_fp = kmalloc(sizeof(*new_fp), GFP_KERNEL);<BR>&nbsp;&nbsp;spin_lock(&amp;foo_mutex);<BR>&nbsp;&nbsp;old_fp = gbl_foo;<BR>&nbsp;&nbsp;*new_fp = *old_fp;<BR>&nbsp;&nbsp;new_fp-&gt;a = new_a;<BR>&nbsp;&nbsp;rcu_assign_pointer(gbl_foo, new_fp);<BR>&nbsp;&nbsp;spin_unlock(&amp;foo_mutex);<BR>&nbsp;&nbsp;call_rcu(&amp;old_fp-&gt;rcu, foo_reclaim);<BR>&nbsp;}</P>
<P>The foo_reclaim() function might appear as follows:</P>
<P>&nbsp;void foo_reclaim(struct rcu_head *rp)<BR>&nbsp;{<BR>&nbsp;&nbsp;struct foo *fp = container_of(rp, struct foo, rcu);</P>
<P>&nbsp;&nbsp;foo_cleanup(fp-&gt;a);</P>
<P>&nbsp;&nbsp;kfree(fp);<BR>&nbsp;}</P>
<P>The container_of() primitive is a macro that, given a pointer into a<BR>struct, the type of the struct, and the pointed-to field within the<BR>struct, returns a pointer to the beginning of the struct.</P>
<P>The use of call_rcu() permits the caller of foo_update_a() to<BR>immediately regain control, without needing to worry further about the<BR>old version of the newly updated element.&nbsp; It also clearly shows the<BR>RCU distinction between updater, namely foo_update_a(), and reclaimer,<BR>namely foo_reclaim().</P>
<P>The summary of advice is the same as for the previous section, except<BR>that we are now using call_rcu() rather than synchronize_rcu():</P>
<P>o&nbsp;Use call_rcu() -after- removing a data element from an<BR>&nbsp;RCU-protected data structure in order to register a callback<BR>&nbsp;function that will be invoked after the completion of all RCU<BR>&nbsp;read-side critical sections that might be referencing that<BR>&nbsp;data item.</P>
<P>If the callback for call_rcu() is not doing anything more than calling<BR>kfree() on the structure, you can use kfree_rcu() instead of call_rcu()<BR>to avoid having to write your own callback:</P>
<P>&nbsp;kfree_rcu(old_fp, rcu);</P>
<P>Again, see checklist.txt for additional rules governing the use of RCU.</P>
<P><BR>5.&nbsp; WHAT ARE SOME SIMPLE IMPLEMENTATIONS OF RCU?</P>
<P>One of the nice things about RCU is that it has extremely simple "toy"<BR>implementations that are a good first step towards understanding the<BR>production-quality implementations in the Linux kernel.&nbsp; This section<BR>presents two such "toy" implementations of RCU, one that is implemented<BR>in terms of familiar locking primitives, and another that more closely<BR>resembles "classic" RCU.&nbsp; Both are way too simple for real-world use,<BR>lacking both functionality and performance.&nbsp; However, they are useful<BR>in getting a feel for how RCU works.&nbsp; See kernel/rcupdate.c for a<BR>production-quality implementation, and see:</P>
<P>&nbsp;<A href="http://www.rdrop.com/users/paulmck/RCU">http://www.rdrop.com/users/paulmck/RCU</A></P>
<P>for papers describing the Linux kernel RCU implementation.&nbsp; The OLS'01<BR>and OLS'02 papers are a good introduction, and the dissertation provides<BR>more details on the current implementation as of early 2004.</P>
<P><BR>5A.&nbsp; "TOY" IMPLEMENTATION #1: LOCKING</P>
<P>This section presents a "toy" RCU implementation that is based on<BR>familiar locking primitives.&nbsp; Its overhead makes it a non-starter for<BR>real-life use, as does its lack of scalability.&nbsp; It is also unsuitable<BR>for realtime use, since it allows scheduling latency to "bleed" from<BR>one read-side critical section to another.</P>
<P>However, it is probably the easiest implementation to relate to, so is<BR>a good starting point.</P>
<P>It is extremely simple:</P>
<P>&nbsp;static DEFINE_RWLOCK(rcu_gp_mutex);</P>
<P>&nbsp;void rcu_read_lock(void)<BR>&nbsp;{<BR>&nbsp;&nbsp;read_lock(&amp;rcu_gp_mutex);<BR>&nbsp;}</P>
<P>&nbsp;void rcu_read_unlock(void)<BR>&nbsp;{<BR>&nbsp;&nbsp;read_unlock(&amp;rcu_gp_mutex);<BR>&nbsp;}</P>
<P>&nbsp;void synchronize_rcu(void)<BR>&nbsp;{<BR>&nbsp;&nbsp;write_lock(&amp;rcu_gp_mutex);<BR>&nbsp;&nbsp;write_unlock(&amp;rcu_gp_mutex);<BR>&nbsp;}</P>
<P>[You can ignore rcu_assign_pointer() and rcu_dereference() without<BR>missing much.&nbsp; But here they are anyway.&nbsp; And whatever you do, don't<BR>forget about them when submitting patches making use of RCU!]</P>
<P>&nbsp;#define rcu_assign_pointer(p, v)&nbsp;({ \<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;smp_wmb(); \<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(p) = (v); \<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;})</P>
<P>&nbsp;#define rcu_dereference(p)&nbsp;&nbsp;&nbsp;&nbsp; ({ \<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;typeof(p) _________p1 = p; \<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;smp_read_barrier_depends(); \<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(_________p1); \<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;})</P>
<P><BR>The rcu_read_lock() and rcu_read_unlock() primitive read-acquire<BR>and release a global reader-writer lock.&nbsp; The synchronize_rcu()<BR>primitive write-acquires this same lock, then immediately releases<BR>it.&nbsp; This means that once synchronize_rcu() exits, all RCU read-side<BR>critical sections that were in progress before synchronize_rcu() was<BR>called are guaranteed to have completed -- there is no way that<BR>synchronize_rcu() would have been able to write-acquire the lock<BR>otherwise.</P>
<P>It is possible to nest rcu_read_lock(), since reader-writer locks may<BR>be recursively acquired.&nbsp; Note also that rcu_read_lock() is immune<BR>from deadlock (an important property of RCU).&nbsp; The reason for this is<BR>that the only thing that can block rcu_read_lock() is a synchronize_rcu().<BR>But synchronize_rcu() does not acquire any locks while holding rcu_gp_mutex,<BR>so there can be no deadlock cycle.</P>
<P>Quick Quiz #1:&nbsp;Why is this argument naive?&nbsp; How could a deadlock<BR>&nbsp;&nbsp;occur when using this algorithm in a real-world Linux<BR>&nbsp;&nbsp;kernel?&nbsp; How could this deadlock be avoided?</P>
<P><BR>5B.&nbsp; "TOY" EXAMPLE #2: CLASSIC RCU</P>
<P>This section presents a "toy" RCU implementation that is based on<BR>"classic RCU".&nbsp; It is also short on performance (but only for updates) and<BR>on features such as hotplug CPU and the ability to run in CONFIG_PREEMPT<BR>kernels.&nbsp; The definitions of rcu_dereference() and rcu_assign_pointer()<BR>are the same as those shown in the preceding section, so they are omitted.</P>
<P>&nbsp;void rcu_read_lock(void) { }</P>
<P>&nbsp;void rcu_read_unlock(void) { }</P>
<P>&nbsp;void synchronize_rcu(void)<BR>&nbsp;{<BR>&nbsp;&nbsp;int cpu;</P>
<P>&nbsp;&nbsp;for_each_possible_cpu(cpu)<BR>&nbsp;&nbsp;&nbsp;run_on(cpu);<BR>&nbsp;}</P>
<P>Note that rcu_read_lock() and rcu_read_unlock() do absolutely nothing.<BR>This is the great strength of classic RCU in a non-preemptive kernel:<BR>read-side overhead is precisely zero, at least on non-Alpha CPUs.<BR>And there is absolutely no way that rcu_read_lock() can possibly<BR>participate in a deadlock cycle!</P>
<P>The implementation of synchronize_rcu() simply schedules itself on each<BR>CPU in turn.&nbsp; The run_on() primitive can be implemented straightforwardly<BR>in terms of the sched_setaffinity() primitive.&nbsp; Of course, a somewhat less<BR>"toy" implementation would restore the affinity upon completion rather<BR>than just leaving all tasks running on the last CPU, but when I said<BR>"toy", I meant -toy-!</P>
<P>So how the heck is this supposed to work???</P>
<P>Remember that it is illegal to block while in an RCU read-side critical<BR>section.&nbsp; Therefore, if a given CPU executes a context switch, we know<BR>that it must have completed all preceding RCU read-side critical sections.<BR>Once -all- CPUs have executed a context switch, then -all- preceding<BR>RCU read-side critical sections will have completed.</P>
<P>So, suppose that we remove a data item from its structure and then invoke<BR>synchronize_rcu().&nbsp; Once synchronize_rcu() returns, we are guaranteed<BR>that there are no RCU read-side critical sections holding a reference<BR>to that data item, so we can safely reclaim it.</P>
<P>Quick Quiz #2:&nbsp;Give an example where Classic RCU's read-side<BR>&nbsp;&nbsp;overhead is -negative-.</P>
<P>Quick Quiz #3:&nbsp; If it is illegal to block in an RCU read-side<BR>&nbsp;&nbsp;critical section, what the heck do you do in<BR>&nbsp;&nbsp;PREEMPT_RT, where normal spinlocks can block???</P>
<P><BR>6.&nbsp; ANALOGY WITH READER-WRITER LOCKING</P>
<P>Although RCU can be used in many different ways, a very common use of<BR>RCU is analogous to reader-writer locking.&nbsp; The following unified<BR>diff shows how closely related RCU and reader-writer locking can be.</P>
<P>&nbsp;@@ -13,15 +14,15 @@<BR>&nbsp;&nbsp;struct list_head *lp;<BR>&nbsp;&nbsp;struct el *p;</P>
<P>&nbsp;-&nbsp;read_lock();<BR>&nbsp;-&nbsp;list_for_each_entry(p, head, lp) {<BR>&nbsp;+&nbsp;rcu_read_lock();<BR>&nbsp;+&nbsp;list_for_each_entry_rcu(p, head, lp) {<BR>&nbsp;&nbsp;&nbsp;if (p-&gt;key == key) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;*result = p-&gt;data;<BR>&nbsp;-&nbsp;&nbsp;&nbsp;read_unlock();<BR>&nbsp;+&nbsp;&nbsp;&nbsp;rcu_read_unlock();<BR>&nbsp;&nbsp;&nbsp;&nbsp;return 1;<BR>&nbsp;&nbsp;&nbsp;}<BR>&nbsp;&nbsp;}<BR>&nbsp;-&nbsp;read_unlock();<BR>&nbsp;+&nbsp;rcu_read_unlock();<BR>&nbsp;&nbsp;return 0;<BR>&nbsp; }</P>
<P>&nbsp;@@ -29,15 +30,16 @@<BR>&nbsp; {<BR>&nbsp;&nbsp;struct el *p;</P>
<P>&nbsp;-&nbsp;write_lock(&amp;listmutex);<BR>&nbsp;+&nbsp;spin_lock(&amp;listmutex);<BR>&nbsp;&nbsp;list_for_each_entry(p, head, lp) {<BR>&nbsp;&nbsp;&nbsp;if (p-&gt;key == key) {<BR>&nbsp;-&nbsp;&nbsp;&nbsp;list_del(&amp;p-&gt;list);<BR>&nbsp;-&nbsp;&nbsp;&nbsp;write_unlock(&amp;listmutex);<BR>&nbsp;+&nbsp;&nbsp;&nbsp;list_del_rcu(&amp;p-&gt;list);<BR>&nbsp;+&nbsp;&nbsp;&nbsp;spin_unlock(&amp;listmutex);<BR>&nbsp;+&nbsp;&nbsp;&nbsp;synchronize_rcu();<BR>&nbsp;&nbsp;&nbsp;&nbsp;kfree(p);<BR>&nbsp;&nbsp;&nbsp;&nbsp;return 1;<BR>&nbsp;&nbsp;&nbsp;}<BR>&nbsp;&nbsp;}<BR>&nbsp;-&nbsp;write_unlock(&amp;listmutex);<BR>&nbsp;+&nbsp;spin_unlock(&amp;listmutex);<BR>&nbsp;&nbsp;return 0;<BR>&nbsp; }</P>
<P>Or, for those who prefer a side-by-side listing:</P>
<P>&nbsp;1 struct el {&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1 struct el {<BR>&nbsp;2&nbsp;&nbsp; struct list_head list;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2&nbsp;&nbsp; struct list_head list;<BR>&nbsp;3&nbsp;&nbsp; long key;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3&nbsp;&nbsp; long key;<BR>&nbsp;4&nbsp;&nbsp; spinlock_t mutex;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4&nbsp;&nbsp; spinlock_t mutex;<BR>&nbsp;5&nbsp;&nbsp; int data;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5&nbsp;&nbsp; int data;<BR>&nbsp;6&nbsp;&nbsp; /* Other data fields */&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6&nbsp;&nbsp; /* Other data fields */<BR>&nbsp;7 };&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 7 };<BR>&nbsp;8 spinlock_t listmutex;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 8 spinlock_t listmutex;<BR>&nbsp;9 struct el head;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 9 struct el head;</P>
<P>&nbsp;1 int search(long key, int *result)&nbsp;&nbsp;&nbsp; 1 int search(long key, int *result)<BR>&nbsp;2 {&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2 {<BR>&nbsp;3&nbsp;&nbsp; struct list_head *lp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3&nbsp;&nbsp; struct list_head *lp;<BR>&nbsp;4&nbsp;&nbsp; struct el *p;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4&nbsp;&nbsp; struct el *p;<BR>&nbsp;5&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5<BR>&nbsp;6&nbsp;&nbsp; read_lock();&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6&nbsp;&nbsp; rcu_read_lock();<BR>&nbsp;7&nbsp;&nbsp; list_for_each_entry(p, head, lp) { 7&nbsp;&nbsp; list_for_each_entry_rcu(p, head, lp) {<BR>&nbsp;8&nbsp;&nbsp;&nbsp;&nbsp; if (p-&gt;key == key) {&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 8&nbsp;&nbsp;&nbsp;&nbsp; if (p-&gt;key == key) {<BR>&nbsp;9&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; *result = p-&gt;data;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 9&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; *result = p-&gt;data;<BR>10&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; read_unlock();&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 10&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; rcu_read_unlock();<BR>11&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return 1;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 11&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return 1;<BR>12&nbsp;&nbsp;&nbsp;&nbsp; }&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 12&nbsp;&nbsp;&nbsp;&nbsp; }<BR>13&nbsp;&nbsp; }&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 13&nbsp;&nbsp; }<BR>14&nbsp;&nbsp; read_unlock();&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 14&nbsp;&nbsp; rcu_read_unlock();<BR>15&nbsp;&nbsp; return 0;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 15&nbsp;&nbsp; return 0;<BR>16 }&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 16 }</P>
<P>&nbsp;1 int delete(long key)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1 int delete(long key)<BR>&nbsp;2 {&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2 {<BR>&nbsp;3&nbsp;&nbsp; struct el *p;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3&nbsp;&nbsp; struct el *p;<BR>&nbsp;4&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4<BR>&nbsp;5&nbsp;&nbsp; write_lock(&amp;listmutex);&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5&nbsp;&nbsp; spin_lock(&amp;listmutex);<BR>&nbsp;6&nbsp;&nbsp; list_for_each_entry(p, head, lp) { 6&nbsp;&nbsp; list_for_each_entry(p, head, lp) {<BR>&nbsp;7&nbsp;&nbsp;&nbsp;&nbsp; if (p-&gt;key == key) {&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 7&nbsp;&nbsp;&nbsp;&nbsp; if (p-&gt;key == key) {<BR>&nbsp;8&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; list_del(&amp;p-&gt;list);&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 8&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; list_del_rcu(&amp;p-&gt;list);<BR>&nbsp;9&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; write_unlock(&amp;listmutex);&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 9&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; spin_unlock(&amp;listmutex);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 10&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; synchronize_rcu();<BR>10&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; kfree(p);&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 11&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; kfree(p);<BR>11&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return 1;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 12&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return 1;<BR>12&nbsp;&nbsp;&nbsp;&nbsp; }&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 13&nbsp;&nbsp;&nbsp;&nbsp; }<BR>13&nbsp;&nbsp; }&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 14&nbsp;&nbsp; }<BR>14&nbsp;&nbsp; write_unlock(&amp;listmutex);&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 15&nbsp;&nbsp; spin_unlock(&amp;listmutex);<BR>15&nbsp;&nbsp; return 0;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 16&nbsp;&nbsp; return 0;<BR>16 }&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 17 }</P>
<P>Either way, the differences are quite small.&nbsp; Read-side locking moves<BR>to rcu_read_lock() and rcu_read_unlock, update-side locking moves from<BR>a reader-writer lock to a simple spinlock, and a synchronize_rcu()<BR>precedes the kfree().</P>
<P>However, there is one potential catch: the read-side and update-side<BR>critical sections can now run concurrently.&nbsp; In many cases, this will<BR>not be a problem, but it is necessary to check carefully regardless.<BR>For example, if multiple independent list updates must be seen as<BR>a single atomic update, converting to RCU will require special care.</P>
<P>Also, the presence of synchronize_rcu() means that the RCU version of<BR>delete() can now block.&nbsp; If this is a problem, there is a callback-based<BR>mechanism that never blocks, namely call_rcu() or kfree_rcu(), that can<BR>be used in place of synchronize_rcu().</P>
<P><BR>7.&nbsp; FULL LIST OF RCU APIs</P>
<P>The RCU APIs are documented in docbook-format header comments in the<BR>Linux-kernel source code, but it helps to have a full list of the<BR>APIs, since there does not appear to be a way to categorize them<BR>in docbook.&nbsp; Here is the list, by category.</P>
<P>RCU list traversal:</P>
<P>&nbsp;list_for_each_entry_rcu<BR>&nbsp;hlist_for_each_entry_rcu<BR>&nbsp;hlist_nulls_for_each_entry_rcu<BR>&nbsp;list_for_each_entry_continue_rcu</P>
<P>RCU pointer/list update:</P>
<P>&nbsp;rcu_assign_pointer<BR>&nbsp;list_add_rcu<BR>&nbsp;list_add_tail_rcu<BR>&nbsp;list_del_rcu<BR>&nbsp;list_replace_rcu<BR>&nbsp;hlist_del_rcu<BR>&nbsp;hlist_add_after_rcu<BR>&nbsp;hlist_add_before_rcu<BR>&nbsp;hlist_add_head_rcu<BR>&nbsp;hlist_replace_rcu<BR>&nbsp;list_splice_init_rcu()</P>
<P>RCU:&nbsp;Critical sections&nbsp;Grace period&nbsp;&nbsp;Barrier</P>
<P>&nbsp;rcu_read_lock&nbsp;&nbsp;synchronize_net&nbsp;&nbsp;rcu_barrier<BR>&nbsp;rcu_read_unlock&nbsp;&nbsp;synchronize_rcu<BR>&nbsp;rcu_dereference&nbsp;&nbsp;synchronize_rcu_expedited<BR>&nbsp;&nbsp;&nbsp;&nbsp;call_rcu<BR>&nbsp;&nbsp;&nbsp;&nbsp;kfree_rcu</P>
<P><BR>bh:&nbsp;Critical sections&nbsp;Grace period&nbsp;&nbsp;Barrier</P>
<P>&nbsp;rcu_read_lock_bh&nbsp;call_rcu_bh&nbsp;&nbsp;rcu_barrier_bh<BR>&nbsp;rcu_read_unlock_bh&nbsp;synchronize_rcu_bh<BR>&nbsp;rcu_dereference_bh&nbsp;synchronize_rcu_bh_expedited</P>
<P><BR>sched:&nbsp;Critical sections&nbsp;Grace period&nbsp;&nbsp;Barrier</P>
<P>&nbsp;rcu_read_lock_sched&nbsp;synchronize_sched&nbsp;rcu_barrier_sched<BR>&nbsp;rcu_read_unlock_sched&nbsp;call_rcu_sched<BR>&nbsp;[preempt_disable]&nbsp;synchronize_sched_expedited<BR>&nbsp;[and friends]<BR>&nbsp;rcu_dereference_sched</P>
<P><BR>SRCU:&nbsp;Critical sections&nbsp;Grace period&nbsp;&nbsp;Barrier</P>
<P>&nbsp;srcu_read_lock&nbsp;&nbsp;synchronize_srcu&nbsp;srcu_barrier<BR>&nbsp;srcu_read_unlock&nbsp;call_srcu<BR>&nbsp;srcu_read_lock_raw&nbsp;synchronize_srcu_expedited<BR>&nbsp;srcu_read_unlock_raw<BR>&nbsp;srcu_dereference</P>
<P>SRCU:&nbsp;Initialization/cleanup<BR>&nbsp;init_srcu_struct<BR>&nbsp;cleanup_srcu_struct</P>
<P>All:&nbsp; lockdep-checked RCU-protected pointer access</P>
<P>&nbsp;rcu_dereference_check<BR>&nbsp;rcu_dereference_protected<BR>&nbsp;rcu_access_pointer</P>
<P>See the comment headers in the source code (or the docbook generated<BR>from them) for more information.</P>
<P>However, given that there are no fewer than four families of RCU APIs<BR>in the Linux kernel, how do you choose which one to use?&nbsp; The following<BR>list can be helpful:</P>
<P>a.&nbsp;Will readers need to block?&nbsp; If so, you need SRCU.</P>
<P>b.&nbsp;Is it necessary to start a read-side critical section in a<BR>&nbsp;hardirq handler or exception handler, and then to complete<BR>&nbsp;this read-side critical section in the task that was<BR>&nbsp;interrupted?&nbsp; If so, you need SRCU's srcu_read_lock_raw() and<BR>&nbsp;srcu_read_unlock_raw() primitives.</P>
<P>c.&nbsp;What about the -rt patchset?&nbsp; If readers would need to block<BR>&nbsp;in an non-rt kernel, you need SRCU.&nbsp; If readers would block<BR>&nbsp;in a -rt kernel, but not in a non-rt kernel, SRCU is not<BR>&nbsp;necessary.</P>
<P>d.&nbsp;Do you need to treat NMI handlers, hardirq handlers,<BR>&nbsp;and code segments with preemption disabled (whether<BR>&nbsp;via preempt_disable(), local_irq_save(), local_bh_disable(),<BR>&nbsp;or some other mechanism) as if they were explicit RCU readers?<BR>&nbsp;If so, RCU-sched is the only choice that will work for you.</P>
<P>e.&nbsp;Do you need RCU grace periods to complete even in the face<BR>&nbsp;of softirq monopolization of one or more of the CPUs?&nbsp; For<BR>&nbsp;example, is your code subject to network-based denial-of-service<BR>&nbsp;attacks?&nbsp; If so, you need RCU-bh.</P>
<P>f.&nbsp;Is your workload too update-intensive for normal use of<BR>&nbsp;RCU, but inappropriate for other synchronization mechanisms?<BR>&nbsp;If so, consider SLAB_DESTROY_BY_RCU.&nbsp; But please be careful!</P>
<P>g.&nbsp;Do you need read-side critical sections that are respected<BR>&nbsp;even though they are in the middle of the idle loop, during<BR>&nbsp;user-mode execution, or on an offlined CPU?&nbsp; If so, SRCU is the<BR>&nbsp;only choice that will work for you.</P>
<P>h.&nbsp;Otherwise, use RCU.</P>
<P>Of course, this all assumes that you have determined that RCU is in fact<BR>the right tool for your job.</P>
<P><BR>8.&nbsp; ANSWERS TO QUICK QUIZZES</P>
<P>Quick Quiz #1:&nbsp;Why is this argument naive?&nbsp; How could a deadlock<BR>&nbsp;&nbsp;occur when using this algorithm in a real-world Linux<BR>&nbsp;&nbsp;kernel?&nbsp; [Referring to the lock-based "toy" RCU<BR>&nbsp;&nbsp;algorithm.]</P>
<P>Answer:&nbsp;&nbsp;Consider the following sequence of events:</P>
<P>&nbsp;&nbsp;1.&nbsp;CPU 0 acquires some unrelated lock, call it<BR>&nbsp;&nbsp;&nbsp;"problematic_lock", disabling irq via<BR>&nbsp;&nbsp;&nbsp;spin_lock_irqsave().</P>
<P>&nbsp;&nbsp;2.&nbsp;CPU 1 enters synchronize_rcu(), write-acquiring<BR>&nbsp;&nbsp;&nbsp;rcu_gp_mutex.</P>
<P>&nbsp;&nbsp;3.&nbsp;CPU 0 enters rcu_read_lock(), but must wait<BR>&nbsp;&nbsp;&nbsp;because CPU 1 holds rcu_gp_mutex.</P>
<P>&nbsp;&nbsp;4.&nbsp;CPU 1 is interrupted, and the irq handler<BR>&nbsp;&nbsp;&nbsp;attempts to acquire problematic_lock.</P>
<P>&nbsp;&nbsp;The system is now deadlocked.</P>
<P>&nbsp;&nbsp;One way to avoid this deadlock is to use an approach like<BR>&nbsp;&nbsp;that of CONFIG_PREEMPT_RT, where all normal spinlocks<BR>&nbsp;&nbsp;become blocking locks, and all irq handlers execute in<BR>&nbsp;&nbsp;the context of special tasks.&nbsp; In this case, in step 4<BR>&nbsp;&nbsp;above, the irq handler would block, allowing CPU 1 to<BR>&nbsp;&nbsp;release rcu_gp_mutex, avoiding the deadlock.</P>
<P>&nbsp;&nbsp;Even in the absence of deadlock, this RCU implementation<BR>&nbsp;&nbsp;allows latency to "bleed" from readers to other<BR>&nbsp;&nbsp;readers through synchronize_rcu().&nbsp; To see this,<BR>&nbsp;&nbsp;consider task A in an RCU read-side critical section<BR>&nbsp;&nbsp;(thus read-holding rcu_gp_mutex), task B blocked<BR>&nbsp;&nbsp;attempting to write-acquire rcu_gp_mutex, and<BR>&nbsp;&nbsp;task C blocked in rcu_read_lock() attempting to<BR>&nbsp;&nbsp;read_acquire rcu_gp_mutex.&nbsp; Task A's RCU read-side<BR>&nbsp;&nbsp;latency is holding up task C, albeit indirectly via<BR>&nbsp;&nbsp;task B.</P>
<P>&nbsp;&nbsp;Realtime RCU implementations therefore use a counter-based<BR>&nbsp;&nbsp;approach where tasks in RCU read-side critical sections<BR>&nbsp;&nbsp;cannot be blocked by tasks executing synchronize_rcu().</P>
<P>Quick Quiz #2:&nbsp;Give an example where Classic RCU's read-side<BR>&nbsp;&nbsp;overhead is -negative-.</P>
<P>Answer:&nbsp;&nbsp;Imagine a single-CPU system with a non-CONFIG_PREEMPT<BR>&nbsp;&nbsp;kernel where a routing table is used by process-context<BR>&nbsp;&nbsp;code, but can be updated by irq-context code (for example,<BR>&nbsp;&nbsp;by an "ICMP REDIRECT" packet).&nbsp;The usual way of handling<BR>&nbsp;&nbsp;this would be to have the process-context code disable<BR>&nbsp;&nbsp;interrupts while searching the routing table.&nbsp; Use of<BR>&nbsp;&nbsp;RCU allows such interrupt-disabling to be dispensed with.<BR>&nbsp;&nbsp;Thus, without RCU, you pay the cost of disabling interrupts,<BR>&nbsp;&nbsp;and with RCU you don't.</P>
<P>&nbsp;&nbsp;One can argue that the overhead of RCU in this<BR>&nbsp;&nbsp;case is negative with respect to the single-CPU<BR>&nbsp;&nbsp;interrupt-disabling approach.&nbsp; Others might argue that<BR>&nbsp;&nbsp;the overhead of RCU is merely zero, and that replacing<BR>&nbsp;&nbsp;the positive overhead of the interrupt-disabling scheme<BR>&nbsp;&nbsp;with the zero-overhead RCU scheme does not constitute<BR>&nbsp;&nbsp;negative overhead.</P>
<P>&nbsp;&nbsp;In real life, of course, things are more complex.&nbsp; But<BR>&nbsp;&nbsp;even the theoretical possibility of negative overhead for<BR>&nbsp;&nbsp;a synchronization primitive is a bit unexpected.&nbsp; ;-)</P>
<P>Quick Quiz #3:&nbsp; If it is illegal to block in an RCU read-side<BR>&nbsp;&nbsp;critical section, what the heck do you do in<BR>&nbsp;&nbsp;PREEMPT_RT, where normal spinlocks can block???</P>
<P>Answer:&nbsp;&nbsp;Just as PREEMPT_RT permits preemption of spinlock<BR>&nbsp;&nbsp;critical sections, it permits preemption of RCU<BR>&nbsp;&nbsp;read-side critical sections.&nbsp; It also permits<BR>&nbsp;&nbsp;spinlocks blocking while in RCU read-side critical<BR>&nbsp;&nbsp;sections.</P>
<P>&nbsp;&nbsp;Why the apparent inconsistency?&nbsp; Because it is it<BR>&nbsp;&nbsp;possible to use priority boosting to keep the RCU<BR>&nbsp;&nbsp;grace periods short if need be (for example, if running<BR>&nbsp;&nbsp;short of memory).&nbsp; In contrast, if blocking waiting<BR>&nbsp;&nbsp;for (say) network reception, there is no way to know<BR>&nbsp;&nbsp;what should be boosted.&nbsp; Especially given that the<BR>&nbsp;&nbsp;process we need to boost might well be a human being<BR>&nbsp;&nbsp;who just went out for a pizza or something.&nbsp; And although<BR>&nbsp;&nbsp;a computer-operated cattle prod might arouse serious<BR>&nbsp;&nbsp;interest, it might also provoke serious objections.<BR>&nbsp;&nbsp;Besides, how does the computer know what pizza parlor<BR>&nbsp;&nbsp;the human being went to???</P>
<P><BR>ACKNOWLEDGEMENTS</P>
<P>My thanks to the people who helped make this human-readable, including<BR>Jon Walpole, Josh Triplett, Serge Hallyn, Suzanne Wood, and Alan Stern.</P>
<P><BR>For more information, see <A href="http://www.rdrop.com/users/paulmck/RCU">http://www.rdrop.com/users/paulmck/RCU</A>.