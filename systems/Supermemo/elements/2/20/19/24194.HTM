# Documentation/RCU/rcubarrier.txt&nbsp; 
<P></P>
<P>RCU and Unloadable Modules</P>
<P></P>
<P>[Originally published in LWN Jan. 14, 2007: <A href="http://lwn.net/Articles/217484/">http://lwn.net/Articles/217484/</A>]</P>
<P>RCU (read-copy update) is a synchronization mechanism that can be thought<BR>of as a replacement for read-writer locking (among other things), but with<BR>very low-overhead readers that are immune to deadlock, priority inversion,<BR>and unbounded latency. RCU read-side critical sections are delimited<BR>by rcu_read_lock() and rcu_read_unlock(), which, in non-CONFIG_PREEMPT<BR>kernels, generate no code whatsoever.</P>
<P>This means that RCU writers are unaware of the presence of concurrent<BR>readers, so that RCU updates to shared data must be undertaken quite<BR>carefully, leaving an old version of the data structure in place until all<BR>pre-existing readers have finished. These old versions are needed because<BR>such readers might hold a reference to them. RCU updates can therefore be<BR>rather expensive, and RCU is thus best suited for read-mostly situations.</P>
<P>How can an RCU writer possibly determine when all readers are finished,<BR>given that readers might well leave absolutely no trace of their<BR>presence? There is a synchronize_rcu() primitive that blocks until all<BR>pre-existing readers have completed. An updater wishing to delete an<BR>element p from a linked list might do the following, while holding an<BR>appropriate lock, of course:</P>
<P>&nbsp;list_del_rcu(p);<BR>&nbsp;synchronize_rcu();<BR>&nbsp;kfree(p);</P>
<P>But the above code cannot be used in IRQ context -- the call_rcu()<BR>primitive must be used instead. This primitive takes a pointer to an<BR>rcu_head struct placed within the RCU-protected data structure and<BR>another pointer to a function that may be invoked later to free that<BR>structure. Code to delete an element p from the linked list from IRQ<BR>context might then be as follows:</P>
<P>&nbsp;list_del_rcu(p);<BR>&nbsp;call_rcu(&amp;p-&gt;rcu, p_callback);</P>
<P>Since call_rcu() never blocks, this code can safely be used from within<BR>IRQ context. The function p_callback() might be defined as follows:</P>
<P>&nbsp;static void p_callback(struct rcu_head *rp)<BR>&nbsp;{<BR>&nbsp;&nbsp;struct pstruct *p = container_of(rp, struct pstruct, rcu);</P>
<P>&nbsp;&nbsp;kfree(p);<BR>&nbsp;}</P>
<P><BR>Unloading Modules That Use call_rcu()</P>
<P>But what if p_callback is defined in an unloadable module?</P>
<P>If we unload the module while some RCU callbacks are pending,<BR>the CPUs executing these callbacks are going to be severely<BR>disappointed when they are later invoked, as fancifully depicted at<BR><A href="http://lwn.net/images/ns/kernel/rcu-drop.jpg">http://lwn.net/images/ns/kernel/rcu-drop.jpg</A>.</P>
<P>We could try placing a synchronize_rcu() in the module-exit code path,<BR>but this is not sufficient. Although synchronize_rcu() does wait for a<BR>grace period to elapse, it does not wait for the callbacks to complete.</P>
<P>One might be tempted to try several back-to-back synchronize_rcu()<BR>calls, but this is still not guaranteed to work. If there is a very<BR>heavy RCU-callback load, then some of the callbacks might be deferred<BR>in order to allow other processing to proceed. Such deferral is required<BR>in realtime kernels in order to avoid excessive scheduling latencies.</P>
<P><BR>rcu_barrier()</P>
<P>We instead need the rcu_barrier() primitive. This primitive is similar<BR>to synchronize_rcu(), but instead of waiting solely for a grace<BR>period to elapse, it also waits for all outstanding RCU callbacks to<BR>complete. Pseudo-code using rcu_barrier() is as follows:</P>
<P>&nbsp;&nbsp; 1. Prevent any new RCU callbacks from being posted.<BR>&nbsp;&nbsp; 2. Execute rcu_barrier().<BR>&nbsp;&nbsp; 3. Allow the module to be unloaded.</P>
<P>There are also rcu_barrier_bh(), rcu_barrier_sched(), and srcu_barrier()<BR>functions for the other flavors of RCU, and you of course must match<BR>the flavor of rcu_barrier() with that of call_rcu().&nbsp; If your module<BR>uses multiple flavors of call_rcu(), then it must also use multiple<BR>flavors of rcu_barrier() when unloading that module.&nbsp; For example, if<BR>it uses call_rcu_bh(), call_srcu() on srcu_struct_1, and call_srcu() on<BR>srcu_struct_2(), then the following three lines of code will be required<BR>when unloading:</P>
<P>&nbsp;1 rcu_barrier_bh();<BR>&nbsp;2 srcu_barrier(&amp;srcu_struct_1);<BR>&nbsp;3 srcu_barrier(&amp;srcu_struct_2);</P>
<P>The rcutorture module makes use of rcu_barrier() in its exit function<BR>as follows:</P>
<P>&nbsp;1 static void<BR>&nbsp;2 rcu_torture_cleanup(void)<BR>&nbsp;3 {<BR>&nbsp;4&nbsp;&nbsp; int i;<BR>&nbsp;5<BR>&nbsp;6&nbsp;&nbsp; fullstop = 1;<BR>&nbsp;7&nbsp;&nbsp; if (shuffler_task != NULL) {<BR>&nbsp;8&nbsp;&nbsp;&nbsp;&nbsp; VERBOSE_PRINTK_STRING("Stopping rcu_torture_shuffle task");<BR>&nbsp;9&nbsp;&nbsp;&nbsp;&nbsp; kthread_stop(shuffler_task);<BR>10&nbsp;&nbsp; }<BR>11&nbsp;&nbsp; shuffler_task = NULL;<BR>12<BR>13&nbsp;&nbsp; if (writer_task != NULL) {<BR>14&nbsp;&nbsp;&nbsp;&nbsp; VERBOSE_PRINTK_STRING("Stopping rcu_torture_writer task");<BR>15&nbsp;&nbsp;&nbsp;&nbsp; kthread_stop(writer_task);<BR>16&nbsp;&nbsp; }<BR>17&nbsp;&nbsp; writer_task = NULL;<BR>18<BR>19&nbsp;&nbsp; if (reader_tasks != NULL) {<BR>20&nbsp;&nbsp;&nbsp;&nbsp; for (i = 0; i &lt; nrealreaders; i++) {<BR>21&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (reader_tasks[i] != NULL) {<BR>22&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; VERBOSE_PRINTK_STRING(<BR>23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "Stopping rcu_torture_reader task");<BR>24&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; kthread_stop(reader_tasks[i]);<BR>25&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<BR>26&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; reader_tasks[i] = NULL;<BR>27&nbsp;&nbsp;&nbsp;&nbsp; }<BR>28&nbsp;&nbsp;&nbsp;&nbsp; kfree(reader_tasks);<BR>29&nbsp;&nbsp;&nbsp;&nbsp; reader_tasks = NULL;<BR>30&nbsp;&nbsp; }<BR>31&nbsp;&nbsp; rcu_torture_current = NULL;<BR>32<BR>33&nbsp;&nbsp; if (fakewriter_tasks != NULL) {<BR>34&nbsp;&nbsp;&nbsp;&nbsp; for (i = 0; i &lt; nfakewriters; i++) {<BR>35&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (fakewriter_tasks[i] != NULL) {<BR>36&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; VERBOSE_PRINTK_STRING(<BR>37&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "Stopping rcu_torture_fakewriter task");<BR>38&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; kthread_stop(fakewriter_tasks[i]);<BR>39&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<BR>40&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; fakewriter_tasks[i] = NULL;<BR>41&nbsp;&nbsp;&nbsp;&nbsp; }<BR>42&nbsp;&nbsp;&nbsp;&nbsp; kfree(fakewriter_tasks);<BR>43&nbsp;&nbsp;&nbsp;&nbsp; fakewriter_tasks = NULL;<BR>44&nbsp;&nbsp; }<BR>45<BR>46&nbsp;&nbsp; if (stats_task != NULL) {<BR>47&nbsp;&nbsp;&nbsp;&nbsp; VERBOSE_PRINTK_STRING("Stopping rcu_torture_stats task");<BR>48&nbsp;&nbsp;&nbsp;&nbsp; kthread_stop(stats_task);<BR>49&nbsp;&nbsp; }<BR>50&nbsp;&nbsp; stats_task = NULL;<BR>51<BR>52&nbsp;&nbsp; /* Wait for all RCU callbacks to fire. */<BR>53&nbsp;&nbsp; rcu_barrier();<BR>54<BR>55&nbsp;&nbsp; rcu_torture_stats_print(); /* -After- the stats thread is stopped! */<BR>56<BR>57&nbsp;&nbsp; if (cur_ops-&gt;cleanup != NULL)<BR>58&nbsp;&nbsp;&nbsp;&nbsp; cur_ops-&gt;cleanup();<BR>59&nbsp;&nbsp; if (atomic_read(&amp;n_rcu_torture_error))<BR>60&nbsp;&nbsp;&nbsp;&nbsp; rcu_torture_print_module_parms("End of test: FAILURE");<BR>61&nbsp;&nbsp; else<BR>62&nbsp;&nbsp;&nbsp;&nbsp; rcu_torture_print_module_parms("End of test: SUCCESS");<BR>63 }</P>
<P>Line 6 sets a global variable that prevents any RCU callbacks from<BR>re-posting themselves. This will not be necessary in most cases, since<BR>RCU callbacks rarely include calls to call_rcu(). However, the rcutorture<BR>module is an exception to this rule, and therefore needs to set this<BR>global variable.</P>
<P>Lines 7-50 stop all the kernel tasks associated with the rcutorture<BR>module. Therefore, once execution reaches line 53, no more rcutorture<BR>RCU callbacks will be posted. The rcu_barrier() call on line 53 waits<BR>for any pre-existing callbacks to complete.</P>
<P>Then lines 55-62 print status and do operation-specific cleanup, and<BR>then return, permitting the module-unload operation to be completed.</P>
<P>Quick Quiz #1: Is there any other situation where rcu_barrier() might<BR>&nbsp;be required?</P>
<P>Your module might have additional complications. For example, if your<BR>module invokes call_rcu() from timers, you will need to first cancel all<BR>the timers, and only then invoke rcu_barrier() to wait for any remaining<BR>RCU callbacks to complete.</P>
<P>Of course, if you module uses call_rcu_bh(), you will need to invoke<BR>rcu_barrier_bh() before unloading.&nbsp; Similarly, if your module uses<BR>call_rcu_sched(), you will need to invoke rcu_barrier_sched() before<BR>unloading.&nbsp; If your module uses call_rcu(), call_rcu_bh(), -and-<BR>call_rcu_sched(), then you will need to invoke each of rcu_barrier(),<BR>rcu_barrier_bh(), and rcu_barrier_sched().</P>
<P><BR>Implementing rcu_barrier()</P>
<P>Dipankar Sarma's implementation of rcu_barrier() makes use of the fact<BR>that RCU callbacks are never reordered once queued on one of the per-CPU<BR>queues. His implementation queues an RCU callback on each of the per-CPU<BR>callback queues, and then waits until they have all started executing, at<BR>which point, all earlier RCU callbacks are guaranteed to have completed.</P>
<P>The original code for rcu_barrier() was as follows:</P>
<P>&nbsp;1 void rcu_barrier(void)<BR>&nbsp;2 {<BR>&nbsp;3&nbsp;&nbsp; BUG_ON(in_interrupt());<BR>&nbsp;4&nbsp;&nbsp; /* Take cpucontrol mutex to protect against CPU hotplug */<BR>&nbsp;5&nbsp;&nbsp; mutex_lock(&amp;rcu_barrier_mutex);<BR>&nbsp;6&nbsp;&nbsp; init_completion(&amp;rcu_barrier_completion);<BR>&nbsp;7&nbsp;&nbsp; atomic_set(&amp;rcu_barrier_cpu_count, 0);<BR>&nbsp;8&nbsp;&nbsp; on_each_cpu(rcu_barrier_func, NULL, 0, 1);<BR>&nbsp;9&nbsp;&nbsp; wait_for_completion(&amp;rcu_barrier_completion);<BR>10&nbsp;&nbsp; mutex_unlock(&amp;rcu_barrier_mutex);<BR>11 }</P>
<P>Line 3 verifies that the caller is in process context, and lines 5 and 10<BR>use rcu_barrier_mutex to ensure that only one rcu_barrier() is using the<BR>global completion and counters at a time, which are initialized on lines<BR>6 and 7. Line 8 causes each CPU to invoke rcu_barrier_func(), which is<BR>shown below. Note that the final "1" in on_each_cpu()'s argument list<BR>ensures that all the calls to rcu_barrier_func() will have completed<BR>before on_each_cpu() returns. Line 9 then waits for the completion.</P>
<P>This code was rewritten in 2008 to support rcu_barrier_bh() and<BR>rcu_barrier_sched() in addition to the original rcu_barrier().</P>
<P>The rcu_barrier_func() runs on each CPU, where it invokes call_rcu()<BR>to post an RCU callback, as follows:</P>
<P>&nbsp;1 static void rcu_barrier_func(void *notused)<BR>&nbsp;2 {<BR>&nbsp;3 int cpu = smp_processor_id();<BR>&nbsp;4 struct rcu_data *rdp = &amp;per_cpu(rcu_data, cpu);<BR>&nbsp;5 struct rcu_head *head;<BR>&nbsp;6<BR>&nbsp;7 head = &amp;rdp-&gt;barrier;<BR>&nbsp;8 atomic_inc(&amp;rcu_barrier_cpu_count);<BR>&nbsp;9 call_rcu(head, rcu_barrier_callback);<BR>10 }</P>
<P>Lines 3 and 4 locate RCU's internal per-CPU rcu_data structure,<BR>which contains the struct rcu_head that needed for the later call to<BR>call_rcu(). Line 7 picks up a pointer to this struct rcu_head, and line<BR>8 increments a global counter. This counter will later be decremented<BR>by the callback. Line 9 then registers the rcu_barrier_callback() on<BR>the current CPU's queue.</P>
<P>The rcu_barrier_callback() function simply atomically decrements the<BR>rcu_barrier_cpu_count variable and finalizes the completion when it<BR>reaches zero, as follows:</P>
<P>&nbsp;1 static void rcu_barrier_callback(struct rcu_head *notused)<BR>&nbsp;2 {<BR>&nbsp;3 if (atomic_dec_and_test(&amp;rcu_barrier_cpu_count))<BR>&nbsp;4 complete(&amp;rcu_barrier_completion);<BR>&nbsp;5 }</P>
<P>Quick Quiz #2: What happens if CPU 0's rcu_barrier_func() executes<BR>&nbsp;immediately (thus incrementing rcu_barrier_cpu_count to the<BR>&nbsp;value one), but the other CPU's rcu_barrier_func() invocations<BR>&nbsp;are delayed for a full grace period? Couldn't this result in<BR>&nbsp;rcu_barrier() returning prematurely?</P>
<P><BR>rcu_barrier() Summary</P>
<P>The rcu_barrier() primitive has seen relatively little use, since most<BR>code using RCU is in the core kernel rather than in modules. However, if<BR>you are using RCU from an unloadable module, you need to use rcu_barrier()<BR>so that your module may be safely unloaded.</P>
<P><BR>Answers to Quick Quizzes</P>
<P>Quick Quiz #1: Is there any other situation where rcu_barrier() might<BR>&nbsp;be required?</P>
<P>Answer: Interestingly enough, rcu_barrier() was not originally<BR>&nbsp;implemented for module unloading. Nikita Danilov was using<BR>&nbsp;RCU in a filesystem, which resulted in a similar situation at<BR>&nbsp;filesystem-unmount time. Dipankar Sarma coded up rcu_barrier()<BR>&nbsp;in response, so that Nikita could invoke it during the<BR>&nbsp;filesystem-unmount process.</P>
<P>&nbsp;Much later, yours truly hit the RCU module-unload problem when<BR>&nbsp;implementing rcutorture, and found that rcu_barrier() solves<BR>&nbsp;this problem as well.</P>
<P>Quick Quiz #2: What happens if CPU 0's rcu_barrier_func() executes<BR>&nbsp;immediately (thus incrementing rcu_barrier_cpu_count to the<BR>&nbsp;value one), but the other CPU's rcu_barrier_func() invocations<BR>&nbsp;are delayed for a full grace period? Couldn't this result in<BR>&nbsp;rcu_barrier() returning prematurely?</P>
<P>Answer: This cannot happen. The reason is that on_each_cpu() has its last<BR>&nbsp;argument, the wait flag, set to "1". This flag is passed through<BR>&nbsp;to smp_call_function() and further to smp_call_function_on_cpu(),<BR>&nbsp;causing this latter to spin until the cross-CPU invocation of<BR>&nbsp;rcu_barrier_func() has completed. This by itself would prevent<BR>&nbsp;a grace period from completing on non-CONFIG_PREEMPT kernels,<BR>&nbsp;since each CPU must undergo a context switch (or other quiescent<BR>&nbsp;state) before the grace period can complete. However, this is<BR>&nbsp;of no use in CONFIG_PREEMPT kernels.</P>
<P>&nbsp;Therefore, on_each_cpu() disables preemption across its call<BR>&nbsp;to smp_call_function() and also across the local call to<BR>&nbsp;rcu_barrier_func(). This prevents the local CPU from context<BR>&nbsp;switching, again preventing grace periods from completing. This<BR>&nbsp;means that all CPUs have executed rcu_barrier_func() before<BR>&nbsp;the first rcu_barrier_callback() can possibly execute, in turn<BR>&nbsp;preventing rcu_barrier_cpu_count from prematurely reaching zero.</P>
<P>&nbsp;Currently, -rt implementations of RCU keep but a single global<BR>&nbsp;queue for RCU callbacks, and thus do not suffer from this<BR>&nbsp;problem. However, when the -rt RCU eventually does have per-CPU<BR>&nbsp;callback queues, things will have to change. One simple change<BR>&nbsp;is to add an rcu_read_lock() before line 8 of rcu_barrier()<BR>&nbsp;and an rcu_read_unlock() after line 8 of this same function. If<BR>&nbsp;you can think of a better change, please let me know!