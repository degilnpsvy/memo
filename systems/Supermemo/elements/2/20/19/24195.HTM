# Documentation/RCU/rculist_nulls.txt
<P></P>
<P>Using hlist_nulls to protect read-mostly linked lists and<BR>objects using SLAB_DESTROY_BY_RCU allocations.</P>
<P></P>
<P>Please read the basics in Documentation/RCU/listRCU.txt</P>
<P>Using special makers (called 'nulls') is a convenient way<BR>to solve following problem :</P>
<P>A typical RCU linked list managing objects which are<BR>allocated with SLAB_DESTROY_BY_RCU kmem_cache can<BR>use following algos :</P>
<P>1) Lookup algo<BR>--------------<BR>rcu_read_lock()<BR>begin:<BR>obj = lockless_lookup(key);<BR>if (obj) {<BR>&nbsp; if (!try_get_ref(obj)) // might fail for free objects<BR>&nbsp;&nbsp;&nbsp; goto begin;<BR>&nbsp; /*<BR>&nbsp;&nbsp; * Because a writer could delete object, and a writer could<BR>&nbsp;&nbsp; * reuse these object before the RCU grace period, we<BR>&nbsp;&nbsp; * must check key after getting the reference on object<BR>&nbsp;&nbsp; */<BR>&nbsp; if (obj-&gt;key != key) { // not the object we expected<BR>&nbsp;&nbsp;&nbsp;&nbsp; put_ref(obj);<BR>&nbsp;&nbsp;&nbsp;&nbsp; goto begin;<BR>&nbsp;&nbsp; }<BR>}<BR>rcu_read_unlock();</P>
<P>Beware that lockless_lookup(key) cannot use traditional hlist_for_each_entry_rcu()<BR>but a version with an additional memory barrier (smp_rmb())</P>
<P>lockless_lookup(key)<BR>{<BR>&nbsp;&nbsp; struct hlist_node *node, *next;<BR>&nbsp;&nbsp; for (pos = rcu_dereference((head)-&gt;first);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; pos &amp;&amp; ({ next = pos-&gt;next; smp_rmb(); prefetch(next); 1; }) &amp;&amp;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ({ tpos = hlist_entry(pos, typeof(*tpos), member); 1; });<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; pos = rcu_dereference(next))<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (obj-&gt;key == key)<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return obj;<BR>&nbsp;&nbsp; return NULL;</P>
<P>And note the traditional hlist_for_each_entry_rcu() misses this smp_rmb() :</P>
<P>&nbsp;&nbsp; struct hlist_node *node;<BR>&nbsp;&nbsp; for (pos = rcu_dereference((head)-&gt;first);<BR>&nbsp;&nbsp;pos &amp;&amp; ({ prefetch(pos-&gt;next); 1; }) &amp;&amp;<BR>&nbsp;&nbsp;({ tpos = hlist_entry(pos, typeof(*tpos), member); 1; });<BR>&nbsp;&nbsp;pos = rcu_dereference(pos-&gt;next))<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (obj-&gt;key == key)<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return obj;<BR>&nbsp;&nbsp; return NULL;<BR>}</P>
<P>Quoting Corey Minyard :</P>
<P>"If the object is moved from one list to another list in-between the<BR>&nbsp;time the hash is calculated and the next field is accessed, and the<BR>&nbsp;object has moved to the end of a new list, the traversal will not<BR>&nbsp;complete properly on the list it should have, since the object will<BR>&nbsp;be on the end of the new list and there's not a way to tell it's on a<BR>&nbsp;new list and restart the list traversal.&nbsp; I think that this can be<BR>&nbsp;solved by pre-fetching the "next" field (with proper barriers) before<BR>&nbsp;checking the key."</P>
<P>2) Insert algo :<BR>----------------</P>
<P>We need to make sure a reader cannot read the new 'obj-&gt;obj_next' value<BR>and previous value of 'obj-&gt;key'. Or else, an item could be deleted<BR>from a chain, and inserted into another chain. If new chain was empty<BR>before the move, 'next' pointer is NULL, and lockless reader can<BR>not detect it missed following items in original chain.</P>
<P>/*<BR>&nbsp;* Please note that new inserts are done at the head of list,<BR>&nbsp;* not in the middle or end.<BR>&nbsp;*/<BR>obj = kmem_cache_alloc(...);<BR>lock_chain(); // typically a spin_lock()<BR>obj-&gt;key = key;<BR>/*<BR>&nbsp;* we need to make sure obj-&gt;key is updated before obj-&gt;next<BR>&nbsp;* or obj-&gt;refcnt<BR>&nbsp;*/<BR>smp_wmb();<BR>atomic_set(&amp;obj-&gt;refcnt, 1);<BR>hlist_add_head_rcu(&amp;obj-&gt;obj_node, list);<BR>unlock_chain(); // typically a spin_unlock()</P>
<P><BR>3) Remove algo<BR>--------------<BR>Nothing special here, we can use a standard RCU hlist deletion.<BR>But thanks to SLAB_DESTROY_BY_RCU, beware a deleted object can be reused<BR>very very fast (before the end of RCU grace period)</P>
<P>if (put_last_reference_on(obj) {<BR>&nbsp;&nbsp; lock_chain(); // typically a spin_lock()<BR>&nbsp;&nbsp; hlist_del_init_rcu(&amp;obj-&gt;obj_node);<BR>&nbsp;&nbsp; unlock_chain(); // typically a spin_unlock()<BR>&nbsp;&nbsp; kmem_cache_free(cachep, obj);<BR>}</P>
<P>&nbsp;</P>
<P>--------------------------------------------------------------------------<BR>With hlist_nulls we can avoid extra smp_rmb() in lockless_lookup()<BR>and extra smp_wmb() in insert function.</P>
<P>For example, if we choose to store the slot number as the 'nulls'<BR>end-of-list marker for each slot of the hash table, we can detect<BR>a race (some writer did a delete and/or a move of an object<BR>to another chain) checking the final 'nulls' value if<BR>the lookup met the end of chain. If final 'nulls' value<BR>is not the slot number, then we must restart the lookup at<BR>the beginning. If the object was moved to the same chain,<BR>then the reader doesn't care : It might eventually<BR>scan the list again without harm.</P>
<P><BR>1) lookup algo</P>
<P>&nbsp;head = &amp;table[slot];<BR>&nbsp;rcu_read_lock();<BR>begin:<BR>&nbsp;hlist_nulls_for_each_entry_rcu(obj, node, head, member) {<BR>&nbsp;&nbsp; if (obj-&gt;key == key) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (!try_get_ref(obj)) // might fail for free objects<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; goto begin;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (obj-&gt;key != key) { // not the object we expected<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; put_ref(obj);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; goto begin;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<BR>&nbsp; goto out;<BR>&nbsp;}<BR>/*<BR>&nbsp;* if the nulls value we got at the end of this lookup is<BR>&nbsp;* not the expected one, we must restart lookup.<BR>&nbsp;* We probably met an item that was moved to another chain.<BR>&nbsp;*/<BR>&nbsp;if (get_nulls_value(node) != slot)<BR>&nbsp;&nbsp; goto begin;<BR>&nbsp;obj = NULL;</P>
<P>out:<BR>&nbsp;rcu_read_unlock();</P>
<P>2) Insert function :<BR>--------------------</P>
<P>/*<BR>&nbsp;* Please note that new inserts are done at the head of list,<BR>&nbsp;* not in the middle or end.<BR>&nbsp;*/<BR>obj = kmem_cache_alloc(cachep);<BR>lock_chain(); // typically a spin_lock()<BR>obj-&gt;key = key;<BR>/*<BR>&nbsp;* changes to obj-&gt;key must be visible before refcnt one<BR>&nbsp;*/<BR>smp_wmb();<BR>atomic_set(&amp;obj-&gt;refcnt, 1);<BR>/*<BR>&nbsp;* insert obj in RCU way (readers might be traversing chain)<BR>&nbsp;*/<BR>hlist_nulls_add_head_rcu(&amp;obj-&gt;obj_node, list);<BR>unlock_chain(); // typically a spin_unlock()