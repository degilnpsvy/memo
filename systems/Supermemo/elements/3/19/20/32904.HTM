# qemu:exec.c 
<P></P>
<P>/*<BR>&nbsp;*&nbsp; Virtual page mapping<BR>&nbsp;*<BR>&nbsp;*&nbsp; Copyright (c) 2003 Fabrice Bellard<BR>&nbsp;*<BR>&nbsp;* This library is free software; you can redistribute it and/or<BR>&nbsp;* modify it under the terms of the GNU Lesser General Public<BR>&nbsp;* License as published by the Free Software Foundation; either<BR>&nbsp;* version 2 of the License, or (at your option) any later version.<BR>&nbsp;*<BR>&nbsp;* This library is distributed in the hope that it will be useful,<BR>&nbsp;* but WITHOUT ANY WARRANTY; without even the implied warranty of<BR>&nbsp;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.&nbsp; See the GNU<BR>&nbsp;* Lesser General Public License for more details.<BR>&nbsp;*<BR>&nbsp;* You should have received a copy of the GNU Lesser General Public<BR>&nbsp;* License along with this library; if not, see &lt;<A href="http://www.gnu.org/licenses/">http://www.gnu.org/licenses/</A>&gt;.<BR>&nbsp;*/<BR>#include "qemu/osdep.h"<BR>#include "qapi/error.h"<BR>#ifndef _WIN32<BR>#endif</P>
<P></P>
<P>#include "qemu/cutils.h"<BR>#include "cpu.h"<BR>#include "exec/exec-all.h"<BR>#include "tcg.h"<BR>#include "hw/qdev-core.h"<BR>#if !defined(CONFIG_USER_ONLY)<BR>#include "hw/boards.h"<BR>#include "hw/xen/xen.h"<BR>#endif<BR>#include "sysemu/kvm.h"<BR>#include "sysemu/sysemu.h"<BR>#include "qemu/timer.h"<BR>#include "qemu/config-file.h"<BR>#include "qemu/error-report.h"<BR>#if defined(CONFIG_USER_ONLY)<BR>#include "qemu.h"<BR>#else /* !CONFIG_USER_ONLY */<BR>#include "hw/hw.h"<BR>#include "exec/memory.h"<BR>#include "exec/ioport.h"<BR>#include "sysemu/dma.h"<BR>#include "exec/address-spaces.h"<BR>#include "sysemu/xen-mapcache.h"<BR>#include "trace.h"<BR>#endif<BR>#include "exec/cpu-all.h"<BR>#include "qemu/rcu_queue.h"<BR>#include "qemu/main-loop.h"<BR>#include "translate-all.h"<BR>#include "sysemu/replay.h"</P>
<P>#include "exec/memory-internal.h"<BR>#include "exec/ram_addr.h"<BR>#include "exec/log.h"</P>
<P>#include "migration/vmstate.h"</P>
<P>#include "qemu/range.h"<BR>#ifndef _WIN32<BR>#include "qemu/mmap-alloc.h"<BR>#endif</P>
<P>//#define DEBUG_SUBPAGE</P>
<P>#if !defined(CONFIG_USER_ONLY)<BR>/* ram_list is read under rcu_read_lock()/rcu_read_unlock().&nbsp; Writes<BR>&nbsp;* are protected by the ramlist lock.<BR>&nbsp;*/<BR>RAMList ram_list = { .blocks = QLIST_HEAD_INITIALIZER(ram_list.blocks) };</P>
<P>static MemoryRegion *system_memory;<BR>static MemoryRegion *system_io;</P>
<P>AddressSpace address_space_io;<BR>AddressSpace address_space_memory;</P>
<P>MemoryRegion io_mem_rom, io_mem_notdirty;<BR>static MemoryRegion io_mem_unassigned;</P>
<P>/* RAM is pre-allocated and passed into qemu_ram_alloc_from_ptr */<BR>#define RAM_PREALLOC&nbsp;&nbsp; (1 &lt;&lt; 0)</P>
<P>/* RAM is mmap-ed with MAP_SHARED */<BR>#define RAM_SHARED&nbsp;&nbsp;&nbsp;&nbsp; (1 &lt;&lt; 1)</P>
<P>/* Only a portion of RAM (used_length) is actually used, and migrated.<BR>&nbsp;* This used_length size can change across reboots.<BR>&nbsp;*/<BR>#define RAM_RESIZEABLE (1 &lt;&lt; 2)</P>
<P>#endif</P>
<P>#ifdef TARGET_PAGE_BITS_VARY<BR>int target_page_bits;<BR>bool target_page_bits_decided;<BR>#endif</P>
<P>struct CPUTailQ cpus = QTAILQ_HEAD_INITIALIZER(cpus);<BR>/* current CPU in the current thread. It is only valid inside<BR>&nbsp;&nbsp; cpu_exec() */<BR>__thread CPUState *current_cpu;<BR>/* 0 = Do not count executed instructions.<BR>&nbsp;&nbsp; 1 = Precise instruction counting.<BR>&nbsp;&nbsp; 2 = Adaptive rate instruction counting.&nbsp; */<BR>int use_icount;</P>
<P>bool set_preferred_target_page_bits(int bits)<BR>{<BR>&nbsp;&nbsp;&nbsp; /* The target page size is the lowest common denominator for all<BR>&nbsp;&nbsp;&nbsp;&nbsp; * the CPUs in the system, so we can only make it smaller, never<BR>&nbsp;&nbsp;&nbsp;&nbsp; * larger. And we can't make it smaller once we've committed to<BR>&nbsp;&nbsp;&nbsp;&nbsp; * a particular size.<BR>&nbsp;&nbsp;&nbsp;&nbsp; */<BR>#ifdef TARGET_PAGE_BITS_VARY<BR>&nbsp;&nbsp;&nbsp; assert(bits &gt;= TARGET_PAGE_BITS_MIN);<BR>&nbsp;&nbsp;&nbsp; if (target_page_bits == 0 || target_page_bits &gt; bits) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (target_page_bits_decided) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return false;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; target_page_bits = bits;<BR>&nbsp;&nbsp;&nbsp; }<BR>#endif<BR>&nbsp;&nbsp;&nbsp; return true;<BR>}</P>
<P>#if !defined(CONFIG_USER_ONLY)</P>
<P>static void finalize_target_page_bits(void)<BR>{<BR>#ifdef TARGET_PAGE_BITS_VARY<BR>&nbsp;&nbsp;&nbsp; if (target_page_bits == 0) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; target_page_bits = TARGET_PAGE_BITS_MIN;<BR>&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; target_page_bits_decided = true;<BR>#endif<BR>}</P>
<P>typedef struct PhysPageEntry PhysPageEntry;</P>
<P>struct PhysPageEntry {<BR>&nbsp;&nbsp;&nbsp; /* How many bits skip to next level (in units of L2_SIZE). 0 for a leaf. */<BR>&nbsp;&nbsp;&nbsp; uint32_t skip : 6;<BR>&nbsp;&nbsp;&nbsp;&nbsp; /* index into phys_sections (!skip) or phys_map_nodes (skip) */<BR>&nbsp;&nbsp;&nbsp; uint32_t ptr : 26;<BR>};</P>
<P>#define PHYS_MAP_NODE_NIL (((uint32_t)~0) &gt;&gt; 6)</P>
<P>/* Size of the L2 (and L3, etc) page tables.&nbsp; */<BR>#define ADDR_SPACE_BITS 64</P>
<P>#define P_L2_BITS 9<BR>#define P_L2_SIZE (1 &lt;&lt; P_L2_BITS)</P>
<P>#define P_L2_LEVELS (((ADDR_SPACE_BITS - TARGET_PAGE_BITS - 1) / P_L2_BITS) + 1)</P>
<P>typedef PhysPageEntry Node[P_L2_SIZE];</P>
<P>typedef struct PhysPageMap {<BR>&nbsp;&nbsp;&nbsp; struct rcu_head rcu;</P>
<P>&nbsp;&nbsp;&nbsp; unsigned sections_nb;<BR>&nbsp;&nbsp;&nbsp; unsigned sections_nb_alloc;<BR>&nbsp;&nbsp;&nbsp; unsigned nodes_nb;<BR>&nbsp;&nbsp;&nbsp; unsigned nodes_nb_alloc;<BR>&nbsp;&nbsp;&nbsp; Node *nodes;<BR>&nbsp;&nbsp;&nbsp; MemoryRegionSection *sections;<BR>} PhysPageMap;</P>
<P>struct AddressSpaceDispatch {<BR>&nbsp;&nbsp;&nbsp; struct rcu_head rcu;</P>
<P>&nbsp;&nbsp;&nbsp; MemoryRegionSection *mru_section;<BR>&nbsp;&nbsp;&nbsp; /* This is a multi-level map on the physical address space.<BR>&nbsp;&nbsp;&nbsp;&nbsp; * The bottom level has pointers to MemoryRegionSections.<BR>&nbsp;&nbsp;&nbsp;&nbsp; */<BR>&nbsp;&nbsp;&nbsp; PhysPageEntry phys_map;<BR>&nbsp;&nbsp;&nbsp; PhysPageMap map;<BR>&nbsp;&nbsp;&nbsp; AddressSpace *as;<BR>};</P>
<P>#define SUBPAGE_IDX(addr) ((addr) &amp; ~TARGET_PAGE_MASK)<BR>typedef struct subpage_t {<BR>&nbsp;&nbsp;&nbsp; MemoryRegion iomem;<BR>&nbsp;&nbsp;&nbsp; AddressSpace *as;<BR>&nbsp;&nbsp;&nbsp; hwaddr base;<BR>&nbsp;&nbsp;&nbsp; uint16_t sub_section[];<BR>} subpage_t;</P>
<P>#define PHYS_SECTION_UNASSIGNED 0<BR>#define PHYS_SECTION_NOTDIRTY 1<BR>#define PHYS_SECTION_ROM 2<BR>#define PHYS_SECTION_WATCH 3</P>
<P>static void io_mem_init(void);<BR>static void memory_map_init(void);<BR>static void tcg_commit(MemoryListener *listener);</P>
<P>static MemoryRegion io_mem_watch;</P>
<P>/**<BR>&nbsp;* CPUAddressSpace: all the information a CPU needs about an AddressSpace<BR>&nbsp;* @cpu: the CPU whose AddressSpace this is<BR>&nbsp;* @as: the AddressSpace itself<BR>&nbsp;* @memory_dispatch: its dispatch pointer (cached, RCU protected)<BR>&nbsp;* @tcg_as_listener: listener for tracking changes to the AddressSpace<BR>&nbsp;*/<BR>struct CPUAddressSpace {<BR>&nbsp;&nbsp;&nbsp; CPUState *cpu;<BR>&nbsp;&nbsp;&nbsp; AddressSpace *as;<BR>&nbsp;&nbsp;&nbsp; struct AddressSpaceDispatch *memory_dispatch;<BR>&nbsp;&nbsp;&nbsp; MemoryListener tcg_as_listener;<BR>};</P>
<P>#endif</P>
<P>#if !defined(CONFIG_USER_ONLY)</P>
<P>static void phys_map_node_reserve(PhysPageMap *map, unsigned nodes)<BR>{<BR>&nbsp;&nbsp;&nbsp; static unsigned alloc_hint = 16;<BR>&nbsp;&nbsp;&nbsp; if (map-&gt;nodes_nb + nodes &gt; map-&gt;nodes_nb_alloc) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; map-&gt;nodes_nb_alloc = MAX(map-&gt;nodes_nb_alloc, alloc_hint);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; map-&gt;nodes_nb_alloc = MAX(map-&gt;nodes_nb_alloc, map-&gt;nodes_nb + nodes);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; map-&gt;nodes = g_renew(Node, map-&gt;nodes, map-&gt;nodes_nb_alloc);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; alloc_hint = map-&gt;nodes_nb_alloc;<BR>&nbsp;&nbsp;&nbsp; }<BR>}</P>
<P>static uint32_t phys_map_node_alloc(PhysPageMap *map, bool leaf)<BR>{<BR>&nbsp;&nbsp;&nbsp; unsigned i;<BR>&nbsp;&nbsp;&nbsp; uint32_t ret;<BR>&nbsp;&nbsp;&nbsp; PhysPageEntry e;<BR>&nbsp;&nbsp;&nbsp; PhysPageEntry *p;</P>
<P>&nbsp;&nbsp;&nbsp; ret = map-&gt;nodes_nb++;<BR>&nbsp;&nbsp;&nbsp; p = map-&gt;nodes[ret];<BR>&nbsp;&nbsp;&nbsp; assert(ret != PHYS_MAP_NODE_NIL);<BR>&nbsp;&nbsp;&nbsp; assert(ret != map-&gt;nodes_nb_alloc);</P>
<P>&nbsp;&nbsp;&nbsp; e.skip = leaf ? 0 : 1;<BR>&nbsp;&nbsp;&nbsp; e.ptr = leaf ? PHYS_SECTION_UNASSIGNED : PHYS_MAP_NODE_NIL;<BR>&nbsp;&nbsp;&nbsp; for (i = 0; i &lt; P_L2_SIZE; ++i) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; memcpy(&amp;p[i], &amp;e, sizeof(e));<BR>&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; return ret;<BR>}</P>
<P>static void phys_page_set_level(PhysPageMap *map, PhysPageEntry *lp,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; hwaddr *index, hwaddr *nb, uint16_t leaf,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; int level)<BR>{<BR>&nbsp;&nbsp;&nbsp; PhysPageEntry *p;<BR>&nbsp;&nbsp;&nbsp; hwaddr step = (hwaddr)1 &lt;&lt; (level * P_L2_BITS);</P>
<P>&nbsp;&nbsp;&nbsp; if (lp-&gt;skip &amp;&amp; lp-&gt;ptr == PHYS_MAP_NODE_NIL) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; lp-&gt;ptr = phys_map_node_alloc(map, level == 0);<BR>&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; p = map-&gt;nodes[lp-&gt;ptr];<BR>&nbsp;&nbsp;&nbsp; lp = &amp;p[(*index &gt;&gt; (level * P_L2_BITS)) &amp; (P_L2_SIZE - 1)];</P>
<P>&nbsp;&nbsp;&nbsp; while (*nb &amp;&amp; lp &lt; &amp;p[P_L2_SIZE]) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if ((*index &amp; (step - 1)) == 0 &amp;&amp; *nb &gt;= step) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; lp-&gt;skip = 0;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; lp-&gt;ptr = leaf;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; *index += step;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; *nb -= step;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; } else {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; phys_page_set_level(map, lp, index, nb, leaf, level - 1);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ++lp;<BR>&nbsp;&nbsp;&nbsp; }<BR>}</P>
<P>static void phys_page_set(AddressSpaceDispatch *d,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; hwaddr index, hwaddr nb,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; uint16_t leaf)<BR>{<BR>&nbsp;&nbsp;&nbsp; /* Wildly overreserve - it doesn't matter much. */<BR>&nbsp;&nbsp;&nbsp; phys_map_node_reserve(&amp;d-&gt;map, 3 * P_L2_LEVELS);</P>
<P>&nbsp;&nbsp;&nbsp; phys_page_set_level(&amp;d-&gt;map, &amp;d-&gt;phys_map, &amp;index, &amp;nb, leaf, P_L2_LEVELS - 1);<BR>}</P>
<P>/* Compact a non leaf page entry. Simply detect that the entry has a single child,<BR>&nbsp;* and update our entry so we can skip it and go directly to the destination.<BR>&nbsp;*/<BR>static void phys_page_compact(PhysPageEntry *lp, Node *nodes)<BR>{<BR>&nbsp;&nbsp;&nbsp; unsigned valid_ptr = P_L2_SIZE;<BR>&nbsp;&nbsp;&nbsp; int valid = 0;<BR>&nbsp;&nbsp;&nbsp; PhysPageEntry *p;<BR>&nbsp;&nbsp;&nbsp; int i;</P>
<P>&nbsp;&nbsp;&nbsp; if (lp-&gt;ptr == PHYS_MAP_NODE_NIL) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return;<BR>&nbsp;&nbsp;&nbsp; }</P>
<P>&nbsp;&nbsp;&nbsp; p = nodes[lp-&gt;ptr];<BR>&nbsp;&nbsp;&nbsp; for (i = 0; i &lt; P_L2_SIZE; i++) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (p[i].ptr == PHYS_MAP_NODE_NIL) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; continue;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; valid_ptr = i;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; valid++;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (p[i].skip) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; phys_page_compact(&amp;p[i], nodes);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; }</P>
<P>&nbsp;&nbsp;&nbsp; /* We can only compress if there's only one child. */<BR>&nbsp;&nbsp;&nbsp; if (valid != 1) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return;<BR>&nbsp;&nbsp;&nbsp; }</P>
<P>&nbsp;&nbsp;&nbsp; assert(valid_ptr &lt; P_L2_SIZE);</P>
<P>&nbsp;&nbsp;&nbsp; /* Don't compress if it won't fit in the # of bits we have. */<BR>&nbsp;&nbsp;&nbsp; if (lp-&gt;skip + p[valid_ptr].skip &gt;= (1 &lt;&lt; 3)) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return;<BR>&nbsp;&nbsp;&nbsp; }</P>
<P>&nbsp;&nbsp;&nbsp; lp-&gt;ptr = p[valid_ptr].ptr;<BR>&nbsp;&nbsp;&nbsp; if (!p[valid_ptr].skip) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; /* If our only child is a leaf, make this a leaf. */<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; /* By design, we should have made this node a leaf to begin with so we<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; * should never reach here.<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; * But since it's so simple to handle this, let's do it just in case we<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; * change this rule.<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; */<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; lp-&gt;skip = 0;<BR>&nbsp;&nbsp;&nbsp; } else {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; lp-&gt;skip += p[valid_ptr].skip;<BR>&nbsp;&nbsp;&nbsp; }<BR>}</P>
<P>static void phys_page_compact_all(AddressSpaceDispatch *d, int nodes_nb)<BR>{<BR>&nbsp;&nbsp;&nbsp; if (d-&gt;phys_map.skip) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; phys_page_compact(&amp;d-&gt;phys_map, d-&gt;map.nodes);<BR>&nbsp;&nbsp;&nbsp; }<BR>}</P>
<P>static inline bool section_covers_addr(const MemoryRegionSection *section,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; hwaddr addr)<BR>{<BR>&nbsp;&nbsp;&nbsp; /* Memory topology clips a memory region to [0, 2^64); size.hi &gt; 0 means<BR>&nbsp;&nbsp;&nbsp;&nbsp; * the section must cover the entire address space.<BR>&nbsp;&nbsp;&nbsp;&nbsp; */<BR>&nbsp;&nbsp;&nbsp; return int128_gethi(section-&gt;size) ||<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; range_covers_byte(section-&gt;offset_within_address_space,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; int128_getlo(section-&gt;size), addr);<BR>}</P>
<P>static MemoryRegionSection *phys_page_find(PhysPageEntry lp, hwaddr addr,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Node *nodes, MemoryRegionSection *sections)<BR>{<BR>&nbsp;&nbsp;&nbsp; PhysPageEntry *p;<BR>&nbsp;&nbsp;&nbsp; hwaddr index = addr &gt;&gt; TARGET_PAGE_BITS;<BR>&nbsp;&nbsp;&nbsp; int i;</P>
<P>&nbsp;&nbsp;&nbsp; for (i = P_L2_LEVELS; lp.skip &amp;&amp; (i -= lp.skip) &gt;= 0;) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (lp.ptr == PHYS_MAP_NODE_NIL) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return &amp;sections[PHYS_SECTION_UNASSIGNED];<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; p = nodes[lp.ptr];<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; lp = p[(index &gt;&gt; (i * P_L2_BITS)) &amp; (P_L2_SIZE - 1)];<BR>&nbsp;&nbsp;&nbsp; }</P>
<P>&nbsp;&nbsp;&nbsp; if (section_covers_addr(&amp;sections[lp.ptr], addr)) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return &amp;sections[lp.ptr];<BR>&nbsp;&nbsp;&nbsp; } else {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return &amp;sections[PHYS_SECTION_UNASSIGNED];<BR>&nbsp;&nbsp;&nbsp; }<BR>}</P>
<P>bool memory_region_is_unassigned(MemoryRegion *mr)<BR>{<BR>&nbsp;&nbsp;&nbsp; return mr != &amp;io_mem_rom &amp;&amp; mr != &amp;io_mem_notdirty &amp;&amp; !mr-&gt;rom_device<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;&amp; mr != &amp;io_mem_watch;<BR>}</P>
<P>/* Called from RCU critical section */<BR>static MemoryRegionSection *address_space_lookup_region(AddressSpaceDispatch *d,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; hwaddr addr,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; bool resolve_subpage)<BR>{<BR>&nbsp;&nbsp;&nbsp; MemoryRegionSection *section = atomic_read(&amp;d-&gt;mru_section);<BR>&nbsp;&nbsp;&nbsp; subpage_t *subpage;<BR>&nbsp;&nbsp;&nbsp; bool update;</P>
<P>&nbsp;&nbsp;&nbsp; if (section &amp;&amp; section != &amp;d-&gt;map.sections[PHYS_SECTION_UNASSIGNED] &amp;&amp;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; section_covers_addr(section, addr)) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; update = false;<BR>&nbsp;&nbsp;&nbsp; } else {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; section = phys_page_find(d-&gt;phys_map, addr, d-&gt;map.nodes,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; d-&gt;map.sections);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; update = true;<BR>&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; if (resolve_subpage &amp;&amp; section-&gt;mr-&gt;subpage) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; subpage = container_of(section-&gt;mr, subpage_t, iomem);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; section = &amp;d-&gt;map.sections[subpage-&gt;sub_section[SUBPAGE_IDX(addr)]];<BR>&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; if (update) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; atomic_set(&amp;d-&gt;mru_section, section);<BR>&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; return section;<BR>}</P>
<P>/* Called from RCU critical section */<BR>static MemoryRegionSection *<BR>address_space_translate_internal(AddressSpaceDispatch *d, hwaddr addr, hwaddr *xlat,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; hwaddr *plen, bool resolve_subpage)<BR>{<BR>&nbsp;&nbsp;&nbsp; MemoryRegionSection *section;<BR>&nbsp;&nbsp;&nbsp; MemoryRegion *mr;<BR>&nbsp;&nbsp;&nbsp; Int128 diff;</P>
<P>&nbsp;&nbsp;&nbsp; section = address_space_lookup_region(d, addr, resolve_subpage);<BR>&nbsp;&nbsp;&nbsp; /* Compute offset within MemoryRegionSection */<BR>&nbsp;&nbsp;&nbsp; addr -= section-&gt;offset_within_address_space;</P>
<P>&nbsp;&nbsp;&nbsp; /* Compute offset within MemoryRegion */<BR>&nbsp;&nbsp;&nbsp; *xlat = addr + section-&gt;offset_within_region;</P>
<P>&nbsp;&nbsp;&nbsp; mr = section-&gt;mr;</P>
<P>&nbsp;&nbsp;&nbsp; /* MMIO registers can be expected to perform full-width accesses based only<BR>&nbsp;&nbsp;&nbsp;&nbsp; * on their address, without considering adjacent registers that could<BR>&nbsp;&nbsp;&nbsp;&nbsp; * decode to completely different MemoryRegions.&nbsp; When such registers<BR>&nbsp;&nbsp;&nbsp;&nbsp; * exist (e.g. I/O ports 0xcf8 and 0xcf9 on most PC chipsets), MMIO<BR>&nbsp;&nbsp;&nbsp;&nbsp; * regions overlap wildly.&nbsp; For this reason we cannot clamp the accesses<BR>&nbsp;&nbsp;&nbsp;&nbsp; * here.<BR>&nbsp;&nbsp;&nbsp;&nbsp; *<BR>&nbsp;&nbsp;&nbsp;&nbsp; * If the length is small (as is the case for address_space_ldl/stl),<BR>&nbsp;&nbsp;&nbsp;&nbsp; * everything works fine.&nbsp; If the incoming length is large, however,<BR>&nbsp;&nbsp;&nbsp;&nbsp; * the caller really has to do the clamping through memory_access_size.<BR>&nbsp;&nbsp;&nbsp;&nbsp; */<BR>&nbsp;&nbsp;&nbsp; if (memory_region_is_ram(mr)) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; diff = int128_sub(section-&gt;size, int128_make64(addr));<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; *plen = int128_get64(int128_min(diff, int128_make64(*plen)));<BR>&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; return section;<BR>}</P>
<P>/* Called from RCU critical section */<BR>IOMMUTLBEntry address_space_get_iotlb_entry(AddressSpace *as, hwaddr addr,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; bool is_write)<BR>{<BR>&nbsp;&nbsp;&nbsp; IOMMUTLBEntry iotlb = {0};<BR>&nbsp;&nbsp;&nbsp; MemoryRegionSection *section;<BR>&nbsp;&nbsp;&nbsp; MemoryRegion *mr;</P>
<P>&nbsp;&nbsp;&nbsp; for (;;) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; AddressSpaceDispatch *d = atomic_rcu_read(&amp;as-&gt;dispatch);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; section = address_space_lookup_region(d, addr, false);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; addr = addr - section-&gt;offset_within_address_space<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; + section-&gt;offset_within_region;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; mr = section-&gt;mr;</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (!mr-&gt;iommu_ops) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; break;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; iotlb = mr-&gt;iommu_ops-&gt;translate(mr, addr, is_write);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (!(iotlb.perm &amp; (1 &lt;&lt; is_write))) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; iotlb.target_as = NULL;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; break;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; addr = ((iotlb.translated_addr &amp; ~iotlb.addr_mask)<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | (addr &amp; iotlb.addr_mask));<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; as = iotlb.target_as;<BR>&nbsp;&nbsp;&nbsp; }</P>
<P>&nbsp;&nbsp;&nbsp; return iotlb;<BR>}</P>
<P>/* Called from RCU critical section */<BR>MemoryRegion *address_space_translate(AddressSpace *as, hwaddr addr,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; hwaddr *xlat, hwaddr *plen,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; bool is_write)<BR>{<BR>&nbsp;&nbsp;&nbsp; IOMMUTLBEntry iotlb;<BR>&nbsp;&nbsp;&nbsp; MemoryRegionSection *section;<BR>&nbsp;&nbsp;&nbsp; MemoryRegion *mr;</P>
<P>&nbsp;&nbsp;&nbsp; for (;;) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; AddressSpaceDispatch *d = atomic_rcu_read(&amp;as-&gt;dispatch);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; section = address_space_translate_internal(d, addr, &amp;addr, plen, true);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; mr = section-&gt;mr;</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (!mr-&gt;iommu_ops) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; break;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; iotlb = mr-&gt;iommu_ops-&gt;translate(mr, addr, is_write);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; addr = ((iotlb.translated_addr &amp; ~iotlb.addr_mask)<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | (addr &amp; iotlb.addr_mask));<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; *plen = MIN(*plen, (addr | iotlb.addr_mask) - addr + 1);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (!(iotlb.perm &amp; (1 &lt;&lt; is_write))) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; mr = &amp;io_mem_unassigned;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; break;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; as = iotlb.target_as;<BR>&nbsp;&nbsp;&nbsp; }</P>
<P>&nbsp;&nbsp;&nbsp; if (xen_enabled() &amp;&amp; memory_access_is_direct(mr, is_write)) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; hwaddr page = ((addr &amp; TARGET_PAGE_MASK) + TARGET_PAGE_SIZE) - addr;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; *plen = MIN(page, *plen);<BR>&nbsp;&nbsp;&nbsp; }</P>
<P>&nbsp;&nbsp;&nbsp; *xlat = addr;<BR>&nbsp;&nbsp;&nbsp; return mr;<BR>}</P>
<P>/* Called from RCU critical section */<BR>MemoryRegionSection *<BR>address_space_translate_for_iotlb(CPUState *cpu, int asidx, hwaddr addr,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; hwaddr *xlat, hwaddr *plen)<BR>{<BR>&nbsp;&nbsp;&nbsp; MemoryRegionSection *section;<BR>&nbsp;&nbsp;&nbsp; AddressSpaceDispatch *d = atomic_rcu_read(&amp;cpu-&gt;cpu_ases[asidx].memory_dispatch);</P>
<P>&nbsp;&nbsp;&nbsp; section = address_space_translate_internal(d, addr, xlat, plen, false);</P>
<P>&nbsp;&nbsp;&nbsp; assert(!section-&gt;mr-&gt;iommu_ops);<BR>&nbsp;&nbsp;&nbsp; return section;<BR>}<BR>#endif</P>
<P>#if !defined(CONFIG_USER_ONLY)</P>
<P>static int cpu_common_post_load(void *opaque, int version_id)<BR>{<BR>&nbsp;&nbsp;&nbsp; CPUState *cpu = opaque;</P>
<P>&nbsp;&nbsp;&nbsp; /* 0x01 was CPU_INTERRUPT_EXIT. This line can be removed when the<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; version_id is increased. */<BR>&nbsp;&nbsp;&nbsp; cpu-&gt;interrupt_request &amp;= ~0x01;<BR>&nbsp;&nbsp;&nbsp; tlb_flush(cpu);</P>
<P>&nbsp;&nbsp;&nbsp; return 0;<BR>}</P>
<P>static int cpu_common_pre_load(void *opaque)<BR>{<BR>&nbsp;&nbsp;&nbsp; CPUState *cpu = opaque;</P>
<P>&nbsp;&nbsp;&nbsp; cpu-&gt;exception_index = -1;</P>
<P>&nbsp;&nbsp;&nbsp; return 0;<BR>}</P>
<P>static bool cpu_common_exception_index_needed(void *opaque)<BR>{<BR>&nbsp;&nbsp;&nbsp; CPUState *cpu = opaque;</P>
<P>&nbsp;&nbsp;&nbsp; return tcg_enabled() &amp;&amp; cpu-&gt;exception_index != -1;<BR>}</P>
<P>static const VMStateDescription vmstate_cpu_common_exception_index = {<BR>&nbsp;&nbsp;&nbsp; .name = "cpu_common/exception_index",<BR>&nbsp;&nbsp;&nbsp; .version_id = 1,<BR>&nbsp;&nbsp;&nbsp; .minimum_version_id = 1,<BR>&nbsp;&nbsp;&nbsp; .needed = cpu_common_exception_index_needed,<BR>&nbsp;&nbsp;&nbsp; .fields = (VMStateField[]) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; VMSTATE_INT32(exception_index, CPUState),<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; VMSTATE_END_OF_LIST()<BR>&nbsp;&nbsp;&nbsp; }<BR>};</P>
<P>static bool cpu_common_crash_occurred_needed(void *opaque)<BR>{<BR>&nbsp;&nbsp;&nbsp; CPUState *cpu = opaque;</P>
<P>&nbsp;&nbsp;&nbsp; return cpu-&gt;crash_occurred;<BR>}</P>
<P>static const VMStateDescription vmstate_cpu_common_crash_occurred = {<BR>&nbsp;&nbsp;&nbsp; .name = "cpu_common/crash_occurred",<BR>&nbsp;&nbsp;&nbsp; .version_id = 1,<BR>&nbsp;&nbsp;&nbsp; .minimum_version_id = 1,<BR>&nbsp;&nbsp;&nbsp; .needed = cpu_common_crash_occurred_needed,<BR>&nbsp;&nbsp;&nbsp; .fields = (VMStateField[]) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; VMSTATE_BOOL(crash_occurred, CPUState),<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; VMSTATE_END_OF_LIST()<BR>&nbsp;&nbsp;&nbsp; }<BR>};</P>
<P>const VMStateDescription vmstate_cpu_common = {<BR>&nbsp;&nbsp;&nbsp; .name = "cpu_common",<BR>&nbsp;&nbsp;&nbsp; .version_id = 1,<BR>&nbsp;&nbsp;&nbsp; .minimum_version_id = 1,<BR>&nbsp;&nbsp;&nbsp; .pre_load = cpu_common_pre_load,<BR>&nbsp;&nbsp;&nbsp; .post_load = cpu_common_post_load,<BR>&nbsp;&nbsp;&nbsp; .fields = (VMStateField[]) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; VMSTATE_UINT32(halted, CPUState),<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; VMSTATE_UINT32(interrupt_request, CPUState),<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; VMSTATE_END_OF_LIST()<BR>&nbsp;&nbsp;&nbsp; },<BR>&nbsp;&nbsp;&nbsp; .subsections = (const VMStateDescription*[]) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;vmstate_cpu_common_exception_index,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;vmstate_cpu_common_crash_occurred,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; NULL<BR>&nbsp;&nbsp;&nbsp; }<BR>};</P>
<P>#endif</P>
<P>CPUState *qemu_get_cpu(int index)<BR>{<BR>&nbsp;&nbsp;&nbsp; CPUState *cpu;</P>
<P>&nbsp;&nbsp;&nbsp; CPU_FOREACH(cpu) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (cpu-&gt;cpu_index == index) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return cpu;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; }</P>
<P>&nbsp;&nbsp;&nbsp; return NULL;<BR>}</P>
<P>#if !defined(CONFIG_USER_ONLY)<BR>void cpu_address_space_init(CPUState *cpu, AddressSpace *as, int asidx)<BR>{<BR>&nbsp;&nbsp;&nbsp; CPUAddressSpace *newas;</P>
<P>&nbsp;&nbsp;&nbsp; /* Target code should have set num_ases before calling us */<BR>&nbsp;&nbsp;&nbsp; assert(asidx &lt; cpu-&gt;num_ases);</P>
<P>&nbsp;&nbsp;&nbsp; if (asidx == 0) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; /* address space 0 gets the convenience alias */<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; cpu-&gt;as = as;<BR>&nbsp;&nbsp;&nbsp; }</P>
<P>&nbsp;&nbsp;&nbsp; /* KVM cannot currently support multiple address spaces. */<BR>&nbsp;&nbsp;&nbsp; assert(asidx == 0 || !kvm_enabled());</P>
<P>&nbsp;&nbsp;&nbsp; if (!cpu-&gt;cpu_ases) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; cpu-&gt;cpu_ases = g_new0(CPUAddressSpace, cpu-&gt;num_ases);<BR>&nbsp;&nbsp;&nbsp; }</P>
<P>&nbsp;&nbsp;&nbsp; newas = &amp;cpu-&gt;cpu_ases[asidx];<BR>&nbsp;&nbsp;&nbsp; newas-&gt;cpu = cpu;<BR>&nbsp;&nbsp;&nbsp; newas-&gt;as = as;<BR>&nbsp;&nbsp;&nbsp; if (tcg_enabled()) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; newas-&gt;tcg_as_listener.commit = tcg_commit;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; memory_listener_register(&amp;newas-&gt;tcg_as_listener, as);<BR>&nbsp;&nbsp;&nbsp; }<BR>}</P>
<P>AddressSpace *cpu_get_address_space(CPUState *cpu, int asidx)<BR>{<BR>&nbsp;&nbsp;&nbsp; /* Return the AddressSpace corresponding to the specified index */<BR>&nbsp;&nbsp;&nbsp; return cpu-&gt;cpu_ases[asidx].as;<BR>}<BR>#endif</P>
<P>void cpu_exec_unrealizefn(CPUState *cpu)<BR>{<BR>&nbsp;&nbsp;&nbsp; CPUClass *cc = CPU_GET_CLASS(cpu);</P>
<P>&nbsp;&nbsp;&nbsp; cpu_list_remove(cpu);</P>
<P>&nbsp;&nbsp;&nbsp; if (cc-&gt;vmsd != NULL) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; vmstate_unregister(NULL, cc-&gt;vmsd, cpu);<BR>&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; if (qdev_get_vmsd(DEVICE(cpu)) == NULL) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; vmstate_unregister(NULL, &amp;vmstate_cpu_common, cpu);<BR>&nbsp;&nbsp;&nbsp; }<BR>}</P>
<P>void cpu_exec_initfn(CPUState *cpu)<BR>{<BR>&nbsp;&nbsp;&nbsp; cpu-&gt;as = NULL;<BR>&nbsp;&nbsp;&nbsp; cpu-&gt;num_ases = 0;</P>
<P>#ifndef CONFIG_USER_ONLY<BR>&nbsp;&nbsp;&nbsp; cpu-&gt;thread_id = qemu_get_thread_id();</P>
<P>&nbsp;&nbsp;&nbsp; /* This is a softmmu CPU object, so create a property for it<BR>&nbsp;&nbsp;&nbsp;&nbsp; * so users can wire up its memory. (This can't go in qom/cpu.c<BR>&nbsp;&nbsp;&nbsp;&nbsp; * because that file is compiled only once for both user-mode<BR>&nbsp;&nbsp;&nbsp;&nbsp; * and system builds.) The default if no link is set up is to use<BR>&nbsp;&nbsp;&nbsp;&nbsp; * the system address space.<BR>&nbsp;&nbsp;&nbsp;&nbsp; */<BR>&nbsp;&nbsp;&nbsp; object_property_add_link(OBJECT(cpu), "memory", TYPE_MEMORY_REGION,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (Object **)&amp;cpu-&gt;memory,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; qdev_prop_allow_set_link_before_realize,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; OBJ_PROP_LINK_UNREF_ON_RELEASE,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;error_abort);<BR>&nbsp;&nbsp;&nbsp; cpu-&gt;memory = system_memory;<BR>&nbsp;&nbsp;&nbsp; object_ref(OBJECT(cpu-&gt;memory));<BR>#endif<BR>}</P>
<P>void cpu_exec_realizefn(CPUState *cpu, Error **errp)<BR>{<BR>&nbsp;&nbsp;&nbsp; CPUClass *cc ATTRIBUTE_UNUSED = CPU_GET_CLASS(cpu);</P>
<P>&nbsp;&nbsp;&nbsp; cpu_list_add(cpu);</P>
<P>#ifndef CONFIG_USER_ONLY<BR>&nbsp;&nbsp;&nbsp; if (qdev_get_vmsd(DEVICE(cpu)) == NULL) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; vmstate_register(NULL, cpu-&gt;cpu_index, &amp;vmstate_cpu_common, cpu);<BR>&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; if (cc-&gt;vmsd != NULL) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; vmstate_register(NULL, cpu-&gt;cpu_index, cc-&gt;vmsd, cpu);<BR>&nbsp;&nbsp;&nbsp; }<BR>#endif<BR>}</P>
<P>static void breakpoint_invalidate(CPUState *cpu, target_ulong pc)<BR>{<BR>&nbsp;&nbsp;&nbsp; /* Flush the whole TB as this will not have race conditions<BR>&nbsp;&nbsp;&nbsp;&nbsp; * even if we don't have proper locking yet.<BR>&nbsp;&nbsp;&nbsp;&nbsp; * Ideally we would just invalidate the TBs for the<BR>&nbsp;&nbsp;&nbsp;&nbsp; * specified PC.<BR>&nbsp;&nbsp;&nbsp;&nbsp; */<BR>&nbsp;&nbsp;&nbsp; tb_flush(cpu);<BR>}</P>
<P>#if defined(CONFIG_USER_ONLY)<BR>void cpu_watchpoint_remove_all(CPUState *cpu, int mask)</P>
<P>{<BR>}</P>
<P>int cpu_watchpoint_remove(CPUState *cpu, vaddr addr, vaddr len,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; int flags)<BR>{<BR>&nbsp;&nbsp;&nbsp; return -ENOSYS;<BR>}</P>
<P>void cpu_watchpoint_remove_by_ref(CPUState *cpu, CPUWatchpoint *watchpoint)<BR>{<BR>}</P>
<P>int cpu_watchpoint_insert(CPUState *cpu, vaddr addr, vaddr len,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; int flags, CPUWatchpoint **watchpoint)<BR>{<BR>&nbsp;&nbsp;&nbsp; return -ENOSYS;<BR>}<BR>#else<BR>/* Add a watchpoint.&nbsp; */<BR>int cpu_watchpoint_insert(CPUState *cpu, vaddr addr, vaddr len,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; int flags, CPUWatchpoint **watchpoint)<BR>{<BR>&nbsp;&nbsp;&nbsp; CPUWatchpoint *wp;</P>
<P>&nbsp;&nbsp;&nbsp; /* forbid ranges which are empty or run off the end of the address space */<BR>&nbsp;&nbsp;&nbsp; if (len == 0 || (addr + len - 1) &lt; addr) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; error_report("tried to set invalid watchpoint at %"<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; VADDR_PRIx ", len=%" VADDR_PRIu, addr, len);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return -EINVAL;<BR>&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; wp = g_malloc(sizeof(*wp));</P>
<P>&nbsp;&nbsp;&nbsp; wp-&gt;vaddr = addr;<BR>&nbsp;&nbsp;&nbsp; wp-&gt;len = len;<BR>&nbsp;&nbsp;&nbsp; wp-&gt;flags = flags;</P>
<P>&nbsp;&nbsp;&nbsp; /* keep all GDB-injected watchpoints in front */<BR>&nbsp;&nbsp;&nbsp; if (flags &amp; BP_GDB) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; QTAILQ_INSERT_HEAD(&amp;cpu-&gt;watchpoints, wp, entry);<BR>&nbsp;&nbsp;&nbsp; } else {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; QTAILQ_INSERT_TAIL(&amp;cpu-&gt;watchpoints, wp, entry);<BR>&nbsp;&nbsp;&nbsp; }</P>
<P>&nbsp;&nbsp;&nbsp; tlb_flush_page(cpu, addr);</P>
<P>&nbsp;&nbsp;&nbsp; if (watchpoint)<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; *watchpoint = wp;<BR>&nbsp;&nbsp;&nbsp; return 0;<BR>}</P>
<P>/* Remove a specific watchpoint.&nbsp; */<BR>int cpu_watchpoint_remove(CPUState *cpu, vaddr addr, vaddr len,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; int flags)<BR>{<BR>&nbsp;&nbsp;&nbsp; CPUWatchpoint *wp;</P>
<P>&nbsp;&nbsp;&nbsp; QTAILQ_FOREACH(wp, &amp;cpu-&gt;watchpoints, entry) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (addr == wp-&gt;vaddr &amp;&amp; len == wp-&gt;len<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;&amp; flags == (wp-&gt;flags &amp; ~BP_WATCHPOINT_HIT)) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; cpu_watchpoint_remove_by_ref(cpu, wp);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return 0;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; return -ENOENT;<BR>}</P>
<P>/* Remove a specific watchpoint by reference.&nbsp; */<BR>void cpu_watchpoint_remove_by_ref(CPUState *cpu, CPUWatchpoint *watchpoint)<BR>{<BR>&nbsp;&nbsp;&nbsp; QTAILQ_REMOVE(&amp;cpu-&gt;watchpoints, watchpoint, entry);</P>
<P>&nbsp;&nbsp;&nbsp; tlb_flush_page(cpu, watchpoint-&gt;vaddr);</P>
<P>&nbsp;&nbsp;&nbsp; g_free(watchpoint);<BR>}</P>
<P>/* Remove all matching watchpoints.&nbsp; */<BR>void cpu_watchpoint_remove_all(CPUState *cpu, int mask)<BR>{<BR>&nbsp;&nbsp;&nbsp; CPUWatchpoint *wp, *next;</P>
<P>&nbsp;&nbsp;&nbsp; QTAILQ_FOREACH_SAFE(wp, &amp;cpu-&gt;watchpoints, entry, next) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (wp-&gt;flags &amp; mask) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; cpu_watchpoint_remove_by_ref(cpu, wp);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; }<BR>}</P>
<P>/* Return true if this watchpoint address matches the specified<BR>&nbsp;* access (ie the address range covered by the watchpoint overlaps<BR>&nbsp;* partially or completely with the address range covered by the<BR>&nbsp;* access).<BR>&nbsp;*/<BR>static inline bool cpu_watchpoint_address_matches(CPUWatchpoint *wp,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; vaddr addr,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; vaddr len)<BR>{<BR>&nbsp;&nbsp;&nbsp; /* We know the lengths are non-zero, but a little caution is<BR>&nbsp;&nbsp;&nbsp;&nbsp; * required to avoid errors in the case where the range ends<BR>&nbsp;&nbsp;&nbsp;&nbsp; * exactly at the top of the address space and so addr + len<BR>&nbsp;&nbsp;&nbsp;&nbsp; * wraps round to zero.<BR>&nbsp;&nbsp;&nbsp;&nbsp; */<BR>&nbsp;&nbsp;&nbsp; vaddr wpend = wp-&gt;vaddr + wp-&gt;len - 1;<BR>&nbsp;&nbsp;&nbsp; vaddr addrend = addr + len - 1;</P>
<P>&nbsp;&nbsp;&nbsp; return !(addr &gt; wpend || wp-&gt;vaddr &gt; addrend);<BR>}</P>
<P>#endif</P>
<P>/* Add a breakpoint.&nbsp; */<BR>int cpu_breakpoint_insert(CPUState *cpu, vaddr pc, int flags,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; CPUBreakpoint **breakpoint)<BR>{<BR>&nbsp;&nbsp;&nbsp; CPUBreakpoint *bp;</P>
<P>&nbsp;&nbsp;&nbsp; bp = g_malloc(sizeof(*bp));</P>
<P>&nbsp;&nbsp;&nbsp; bp-&gt;pc = pc;<BR>&nbsp;&nbsp;&nbsp; bp-&gt;flags = flags;</P>
<P>&nbsp;&nbsp;&nbsp; /* keep all GDB-injected breakpoints in front */<BR>&nbsp;&nbsp;&nbsp; if (flags &amp; BP_GDB) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; QTAILQ_INSERT_HEAD(&amp;cpu-&gt;breakpoints, bp, entry);<BR>&nbsp;&nbsp;&nbsp; } else {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; QTAILQ_INSERT_TAIL(&amp;cpu-&gt;breakpoints, bp, entry);<BR>&nbsp;&nbsp;&nbsp; }</P>
<P>&nbsp;&nbsp;&nbsp; breakpoint_invalidate(cpu, pc);</P>
<P>&nbsp;&nbsp;&nbsp; if (breakpoint) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; *breakpoint = bp;<BR>&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; return 0;<BR>}</P>
<P>/* Remove a specific breakpoint.&nbsp; */<BR>int cpu_breakpoint_remove(CPUState *cpu, vaddr pc, int flags)<BR>{<BR>&nbsp;&nbsp;&nbsp; CPUBreakpoint *bp;</P>
<P>&nbsp;&nbsp;&nbsp; QTAILQ_FOREACH(bp, &amp;cpu-&gt;breakpoints, entry) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (bp-&gt;pc == pc &amp;&amp; bp-&gt;flags == flags) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; cpu_breakpoint_remove_by_ref(cpu, bp);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return 0;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; return -ENOENT;<BR>}</P>
<P>/* Remove a specific breakpoint by reference.&nbsp; */<BR>void cpu_breakpoint_remove_by_ref(CPUState *cpu, CPUBreakpoint *breakpoint)<BR>{<BR>&nbsp;&nbsp;&nbsp; QTAILQ_REMOVE(&amp;cpu-&gt;breakpoints, breakpoint, entry);</P>
<P>&nbsp;&nbsp;&nbsp; breakpoint_invalidate(cpu, breakpoint-&gt;pc);</P>
<P>&nbsp;&nbsp;&nbsp; g_free(breakpoint);<BR>}</P>
<P>/* Remove all matching breakpoints. */<BR>void cpu_breakpoint_remove_all(CPUState *cpu, int mask)<BR>{<BR>&nbsp;&nbsp;&nbsp; CPUBreakpoint *bp, *next;</P>
<P>&nbsp;&nbsp;&nbsp; QTAILQ_FOREACH_SAFE(bp, &amp;cpu-&gt;breakpoints, entry, next) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (bp-&gt;flags &amp; mask) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; cpu_breakpoint_remove_by_ref(cpu, bp);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; }<BR>}</P>
<P>/* enable or disable single step mode. EXCP_DEBUG is returned by the<BR>&nbsp;&nbsp; CPU loop after each instruction */<BR>void cpu_single_step(CPUState *cpu, int enabled)<BR>{<BR>&nbsp;&nbsp;&nbsp; if (cpu-&gt;singlestep_enabled != enabled) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; cpu-&gt;singlestep_enabled = enabled;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (kvm_enabled()) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; kvm_update_guest_debug(cpu, 0);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; } else {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; /* must flush all the translated code to avoid inconsistencies */<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; /* XXX: only flush what is necessary */<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; tb_flush(cpu);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; }<BR>}</P>
<P>void cpu_abort(CPUState *cpu, const char *fmt, ...)<BR>{<BR>&nbsp;&nbsp;&nbsp; va_list ap;<BR>&nbsp;&nbsp;&nbsp; va_list ap2;</P>
<P>&nbsp;&nbsp;&nbsp; va_start(ap, fmt);<BR>&nbsp;&nbsp;&nbsp; va_copy(ap2, ap);<BR>&nbsp;&nbsp;&nbsp; fprintf(stderr, "qemu: fatal: ");<BR>&nbsp;&nbsp;&nbsp; vfprintf(stderr, fmt, ap);<BR>&nbsp;&nbsp;&nbsp; fprintf(stderr, "\n");<BR>&nbsp;&nbsp;&nbsp; cpu_dump_state(cpu, stderr, fprintf, CPU_DUMP_FPU | CPU_DUMP_CCOP);<BR>&nbsp;&nbsp;&nbsp; if (qemu_log_separate()) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; qemu_log_lock();<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; qemu_log("qemu: fatal: ");<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; qemu_log_vprintf(fmt, ap2);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; qemu_log("\n");<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; log_cpu_state(cpu, CPU_DUMP_FPU | CPU_DUMP_CCOP);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; qemu_log_flush();<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; qemu_log_unlock();<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; qemu_log_close();<BR>&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; va_end(ap2);<BR>&nbsp;&nbsp;&nbsp; va_end(ap);<BR>&nbsp;&nbsp;&nbsp; replay_finish();<BR>#if defined(CONFIG_USER_ONLY)<BR>&nbsp;&nbsp;&nbsp; {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; struct sigaction act;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; sigfillset(&amp;act.sa_mask);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; act.sa_handler = SIG_DFL;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; sigaction(SIGABRT, &amp;act, NULL);<BR>&nbsp;&nbsp;&nbsp; }<BR>#endif<BR>&nbsp;&nbsp;&nbsp; abort();<BR>}</P>
<P>#if !defined(CONFIG_USER_ONLY)<BR>/* Called from RCU critical section */<BR>static RAMBlock *qemu_get_ram_block(ram_addr_t addr)<BR>{<BR>&nbsp;&nbsp;&nbsp; RAMBlock *block;</P>
<P>&nbsp;&nbsp;&nbsp; block = atomic_rcu_read(&amp;ram_list.mru_block);<BR>&nbsp;&nbsp;&nbsp; if (block &amp;&amp; addr - block-&gt;offset &lt; block-&gt;max_length) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return block;<BR>&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; QLIST_FOREACH_RCU(block, &amp;ram_list.blocks, next) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (addr - block-&gt;offset &lt; block-&gt;max_length) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; goto found;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; }</P>
<P>&nbsp;&nbsp;&nbsp; fprintf(stderr, "Bad ram offset %" PRIx64 "\n", (uint64_t)addr);<BR>&nbsp;&nbsp;&nbsp; abort();</P>
<P>found:<BR>&nbsp;&nbsp;&nbsp; /* It is safe to write mru_block outside the iothread lock.&nbsp; This<BR>&nbsp;&nbsp;&nbsp;&nbsp; * is what happens:<BR>&nbsp;&nbsp;&nbsp;&nbsp; *<BR>&nbsp;&nbsp;&nbsp;&nbsp; *&nbsp;&nbsp;&nbsp;&nbsp; mru_block = xxx<BR>&nbsp;&nbsp;&nbsp;&nbsp; *&nbsp;&nbsp;&nbsp;&nbsp; rcu_read_unlock()<BR>&nbsp;&nbsp;&nbsp;&nbsp; *&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; xxx removed from list<BR>&nbsp;&nbsp;&nbsp;&nbsp; *&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; rcu_read_lock()<BR>&nbsp;&nbsp;&nbsp;&nbsp; *&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; read mru_block<BR>&nbsp;&nbsp;&nbsp;&nbsp; *&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; mru_block = NULL;<BR>&nbsp;&nbsp;&nbsp;&nbsp; *&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; call_rcu(reclaim_ramblock, xxx);<BR>&nbsp;&nbsp;&nbsp;&nbsp; *&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; rcu_read_unlock()<BR>&nbsp;&nbsp;&nbsp;&nbsp; *<BR>&nbsp;&nbsp;&nbsp;&nbsp; * atomic_rcu_set is not needed here.&nbsp; The block was already published<BR>&nbsp;&nbsp;&nbsp;&nbsp; * when it was placed into the list.&nbsp; Here we're just making an extra<BR>&nbsp;&nbsp;&nbsp;&nbsp; * copy of the pointer.<BR>&nbsp;&nbsp;&nbsp;&nbsp; */<BR>&nbsp;&nbsp;&nbsp; ram_list.mru_block = block;<BR>&nbsp;&nbsp;&nbsp; return block;<BR>}</P>
<P>static void tlb_reset_dirty_range_all(ram_addr_t start, ram_addr_t length)<BR>{<BR>&nbsp;&nbsp;&nbsp; CPUState *cpu;<BR>&nbsp;&nbsp;&nbsp; ram_addr_t start1;<BR>&nbsp;&nbsp;&nbsp; RAMBlock *block;<BR>&nbsp;&nbsp;&nbsp; ram_addr_t end;</P>
<P>&nbsp;&nbsp;&nbsp; end = TARGET_PAGE_ALIGN(start + length);<BR>&nbsp;&nbsp;&nbsp; start &amp;= TARGET_PAGE_MASK;</P>
<P>&nbsp;&nbsp;&nbsp; rcu_read_lock();<BR>&nbsp;&nbsp;&nbsp; block = qemu_get_ram_block(start);<BR>&nbsp;&nbsp;&nbsp; assert(block == qemu_get_ram_block(end - 1));<BR>&nbsp;&nbsp;&nbsp; start1 = (uintptr_t)ramblock_ptr(block, start - block-&gt;offset);<BR>&nbsp;&nbsp;&nbsp; CPU_FOREACH(cpu) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; tlb_reset_dirty(cpu, start1, length);<BR>&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; rcu_read_unlock();<BR>}</P>
<P>/* Note: start and end must be within the same ram block.&nbsp; */<BR>bool cpu_physical_memory_test_and_clear_dirty(ram_addr_t start,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ram_addr_t length,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; unsigned client)<BR>{<BR>&nbsp;&nbsp;&nbsp; DirtyMemoryBlocks *blocks;<BR>&nbsp;&nbsp;&nbsp; unsigned long end, page;<BR>&nbsp;&nbsp;&nbsp; bool dirty = false;</P>
<P>&nbsp;&nbsp;&nbsp; if (length == 0) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return false;<BR>&nbsp;&nbsp;&nbsp; }</P>
<P>&nbsp;&nbsp;&nbsp; end = TARGET_PAGE_ALIGN(start + length) &gt;&gt; TARGET_PAGE_BITS;<BR>&nbsp;&nbsp;&nbsp; page = start &gt;&gt; TARGET_PAGE_BITS;</P>
<P>&nbsp;&nbsp;&nbsp; rcu_read_lock();</P>
<P>&nbsp;&nbsp;&nbsp; blocks = atomic_rcu_read(&amp;ram_list.dirty_memory[client]);</P>
<P>&nbsp;&nbsp;&nbsp; while (page &lt; end) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; unsigned long idx = page / DIRTY_MEMORY_BLOCK_SIZE;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; unsigned long offset = page % DIRTY_MEMORY_BLOCK_SIZE;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; unsigned long num = MIN(end - page, DIRTY_MEMORY_BLOCK_SIZE - offset);</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; dirty |= bitmap_test_and_clear_atomic(blocks-&gt;blocks[idx],<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; offset, num);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; page += num;<BR>&nbsp;&nbsp;&nbsp; }</P>
<P>&nbsp;&nbsp;&nbsp; rcu_read_unlock();</P>
<P>&nbsp;&nbsp;&nbsp; if (dirty &amp;&amp; tcg_enabled()) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; tlb_reset_dirty_range_all(start, length);<BR>&nbsp;&nbsp;&nbsp; }</P>
<P>&nbsp;&nbsp;&nbsp; return dirty;<BR>}</P>
<P>/* Called from RCU critical section */<BR>hwaddr memory_region_section_get_iotlb(CPUState *cpu,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; MemoryRegionSection *section,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; target_ulong vaddr,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; hwaddr paddr, hwaddr xlat,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; int prot,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; target_ulong *address)<BR>{<BR>&nbsp;&nbsp;&nbsp; hwaddr iotlb;<BR>&nbsp;&nbsp;&nbsp; CPUWatchpoint *wp;</P>
<P>&nbsp;&nbsp;&nbsp; if (memory_region_is_ram(section-&gt;mr)) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; /* Normal RAM.&nbsp; */<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; iotlb = memory_region_get_ram_addr(section-&gt;mr) + xlat;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (!section-&gt;readonly) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; iotlb |= PHYS_SECTION_NOTDIRTY;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; } else {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; iotlb |= PHYS_SECTION_ROM;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; } else {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; AddressSpaceDispatch *d;</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; d = atomic_rcu_read(&amp;section-&gt;address_space-&gt;dispatch);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; iotlb = section - d-&gt;map.sections;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; iotlb += xlat;<BR>&nbsp;&nbsp;&nbsp; }</P>
<P>&nbsp;&nbsp;&nbsp; /* Make accesses to pages with watchpoints go via the<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; watchpoint trap routines.&nbsp; */<BR>&nbsp;&nbsp;&nbsp; QTAILQ_FOREACH(wp, &amp;cpu-&gt;watchpoints, entry) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (cpu_watchpoint_address_matches(wp, vaddr, TARGET_PAGE_SIZE)) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; /* Avoid trapping reads of pages with a write breakpoint. */<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if ((prot &amp; PAGE_WRITE) || (wp-&gt;flags &amp; BP_MEM_READ)) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; iotlb = PHYS_SECTION_WATCH + paddr;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; *address |= TLB_MMIO;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; break;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; }</P>
<P>&nbsp;&nbsp;&nbsp; return iotlb;<BR>}<BR>#endif /* defined(CONFIG_USER_ONLY) */</P>
<P>#if !defined(CONFIG_USER_ONLY)</P>
<P>static int subpage_register (subpage_t *mmio, uint32_t start, uint32_t end,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; uint16_t section);<BR>static subpage_t *subpage_init(AddressSpace *as, hwaddr base);</P>
<P>static void *(*phys_mem_alloc)(size_t size, uint64_t *align) =<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; qemu_anon_ram_alloc;</P>
<P>/*<BR>&nbsp;* Set a custom physical guest memory alloator.<BR>&nbsp;* Accelerators with unusual needs may need this.&nbsp; Hopefully, we can<BR>&nbsp;* get rid of it eventually.<BR>&nbsp;*/<BR>void phys_mem_set_alloc(void *(*alloc)(size_t, uint64_t *align))<BR>{<BR>&nbsp;&nbsp;&nbsp; phys_mem_alloc = alloc;<BR>}</P>
<P>static uint16_t phys_section_add(PhysPageMap *map,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; MemoryRegionSection *section)<BR>{<BR>&nbsp;&nbsp;&nbsp; /* The physical section number is ORed with a page-aligned<BR>&nbsp;&nbsp;&nbsp;&nbsp; * pointer to produce the iotlb entries.&nbsp; Thus it should<BR>&nbsp;&nbsp;&nbsp;&nbsp; * never overflow into the page-aligned value.<BR>&nbsp;&nbsp;&nbsp;&nbsp; */<BR>&nbsp;&nbsp;&nbsp; assert(map-&gt;sections_nb &lt; TARGET_PAGE_SIZE);</P>
<P>&nbsp;&nbsp;&nbsp; if (map-&gt;sections_nb == map-&gt;sections_nb_alloc) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; map-&gt;sections_nb_alloc = MAX(map-&gt;sections_nb_alloc * 2, 16);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; map-&gt;sections = g_renew(MemoryRegionSection, map-&gt;sections,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; map-&gt;sections_nb_alloc);<BR>&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; map-&gt;sections[map-&gt;sections_nb] = *section;<BR>&nbsp;&nbsp;&nbsp; memory_region_ref(section-&gt;mr);<BR>&nbsp;&nbsp;&nbsp; return map-&gt;sections_nb++;<BR>}</P>
<P>static void phys_section_destroy(MemoryRegion *mr)<BR>{<BR>&nbsp;&nbsp;&nbsp; bool have_sub_page = mr-&gt;subpage;</P>
<P>&nbsp;&nbsp;&nbsp; memory_region_unref(mr);</P>
<P>&nbsp;&nbsp;&nbsp; if (have_sub_page) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; subpage_t *subpage = container_of(mr, subpage_t, iomem);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; object_unref(OBJECT(&amp;subpage-&gt;iomem));<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; g_free(subpage);<BR>&nbsp;&nbsp;&nbsp; }<BR>}</P>
<P>static void phys_sections_free(PhysPageMap *map)<BR>{<BR>&nbsp;&nbsp;&nbsp; while (map-&gt;sections_nb &gt; 0) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; MemoryRegionSection *section = &amp;map-&gt;sections[--map-&gt;sections_nb];<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; phys_section_destroy(section-&gt;mr);<BR>&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; g_free(map-&gt;sections);<BR>&nbsp;&nbsp;&nbsp; g_free(map-&gt;nodes);<BR>}</P>
<P>static void register_subpage(AddressSpaceDispatch *d, MemoryRegionSection *section)<BR>{<BR>&nbsp;&nbsp;&nbsp; subpage_t *subpage;<BR>&nbsp;&nbsp;&nbsp; hwaddr base = section-&gt;offset_within_address_space<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp; TARGET_PAGE_MASK;<BR>&nbsp;&nbsp;&nbsp; MemoryRegionSection *existing = phys_page_find(d-&gt;phys_map, base,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; d-&gt;map.nodes, d-&gt;map.sections);<BR>&nbsp;&nbsp;&nbsp; MemoryRegionSection subsection = {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; .offset_within_address_space = base,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; .size = int128_make64(TARGET_PAGE_SIZE),<BR>&nbsp;&nbsp;&nbsp; };<BR>&nbsp;&nbsp;&nbsp; hwaddr start, end;</P>
<P>&nbsp;&nbsp;&nbsp; assert(existing-&gt;mr-&gt;subpage || existing-&gt;mr == &amp;io_mem_unassigned);</P>
<P>&nbsp;&nbsp;&nbsp; if (!(existing-&gt;mr-&gt;subpage)) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; subpage = subpage_init(d-&gt;as, base);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; subsection.address_space = d-&gt;as;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; subsection.mr = &amp;subpage-&gt;iomem;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; phys_page_set(d, base &gt;&gt; TARGET_PAGE_BITS, 1,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; phys_section_add(&amp;d-&gt;map, &amp;subsection));<BR>&nbsp;&nbsp;&nbsp; } else {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; subpage = container_of(existing-&gt;mr, subpage_t, iomem);<BR>&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; start = section-&gt;offset_within_address_space &amp; ~TARGET_PAGE_MASK;<BR>&nbsp;&nbsp;&nbsp; end = start + int128_get64(section-&gt;size) - 1;<BR>&nbsp;&nbsp;&nbsp; subpage_register(subpage, start, end,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; phys_section_add(&amp;d-&gt;map, section));<BR>}</P>
<P><BR>static void register_multipage(AddressSpaceDispatch *d,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; MemoryRegionSection *section)<BR>{<BR>&nbsp;&nbsp;&nbsp; hwaddr start_addr = section-&gt;offset_within_address_space;<BR>&nbsp;&nbsp;&nbsp; uint16_t section_index = phys_section_add(&amp;d-&gt;map, section);<BR>&nbsp;&nbsp;&nbsp; uint64_t num_pages = int128_get64(int128_rshift(section-&gt;size,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; TARGET_PAGE_BITS));</P>
<P>&nbsp;&nbsp;&nbsp; assert(num_pages);<BR>&nbsp;&nbsp;&nbsp; phys_page_set(d, start_addr &gt;&gt; TARGET_PAGE_BITS, num_pages, section_index);<BR>}</P>
<P>static void mem_add(MemoryListener *listener, MemoryRegionSection *section)<BR>{<BR>&nbsp;&nbsp;&nbsp; AddressSpace *as = container_of(listener, AddressSpace, dispatch_listener);<BR>&nbsp;&nbsp;&nbsp; AddressSpaceDispatch *d = as-&gt;next_dispatch;<BR>&nbsp;&nbsp;&nbsp; MemoryRegionSection now = *section, remain = *section;<BR>&nbsp;&nbsp;&nbsp; Int128 page_size = int128_make64(TARGET_PAGE_SIZE);</P>
<P>&nbsp;&nbsp;&nbsp; if (now.offset_within_address_space &amp; ~TARGET_PAGE_MASK) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; uint64_t left = TARGET_PAGE_ALIGN(now.offset_within_address_space)<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - now.offset_within_address_space;</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; now.size = int128_min(int128_make64(left), now.size);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; register_subpage(d, &amp;now);<BR>&nbsp;&nbsp;&nbsp; } else {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; now.size = int128_zero();<BR>&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; while (int128_ne(remain.size, now.size)) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; remain.size = int128_sub(remain.size, now.size);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; remain.offset_within_address_space += int128_get64(now.size);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; remain.offset_within_region += int128_get64(now.size);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; now = remain;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (int128_lt(remain.size, page_size)) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; register_subpage(d, &amp;now);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; } else if (remain.offset_within_address_space &amp; ~TARGET_PAGE_MASK) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; now.size = page_size;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; register_subpage(d, &amp;now);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; } else {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; now.size = int128_and(now.size, int128_neg(page_size));<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; register_multipage(d, &amp;now);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; }<BR>}</P>
<P>void qemu_flush_coalesced_mmio_buffer(void)<BR>{<BR>&nbsp;&nbsp;&nbsp; if (kvm_enabled())<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; kvm_flush_coalesced_mmio_buffer();<BR>}</P>
<P>void qemu_mutex_lock_ramlist(void)<BR>{<BR>&nbsp;&nbsp;&nbsp; qemu_mutex_lock(&amp;ram_list.mutex);<BR>}</P>
<P>void qemu_mutex_unlock_ramlist(void)<BR>{<BR>&nbsp;&nbsp;&nbsp; qemu_mutex_unlock(&amp;ram_list.mutex);<BR>}</P>
<P>#ifdef __linux__<BR>static int64_t get_file_size(int fd)<BR>{<BR>&nbsp;&nbsp;&nbsp; int64_t size = lseek(fd, 0, SEEK_END);<BR>&nbsp;&nbsp;&nbsp; if (size &lt; 0) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return -errno;<BR>&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; return size;<BR>}</P>
<P>static void *file_ram_alloc(RAMBlock *block,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ram_addr_t memory,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; const char *path,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Error **errp)<BR>{<BR>&nbsp;&nbsp;&nbsp; bool unlink_on_error = false;<BR>&nbsp;&nbsp;&nbsp; char *filename;<BR>&nbsp;&nbsp;&nbsp; char *sanitized_name;<BR>&nbsp;&nbsp;&nbsp; char *c;<BR>&nbsp;&nbsp;&nbsp; void *area = MAP_FAILED;<BR>&nbsp;&nbsp;&nbsp; int fd = -1;<BR>&nbsp;&nbsp;&nbsp; int64_t file_size;</P>
<P>&nbsp;&nbsp;&nbsp; if (kvm_enabled() &amp;&amp; !kvm_has_sync_mmu()) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; error_setg(errp,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "host lacks kvm mmu notifiers, -mem-path unsupported");<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return NULL;<BR>&nbsp;&nbsp;&nbsp; }</P>
<P>&nbsp;&nbsp;&nbsp; for (;;) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; fd = open(path, O_RDWR);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (fd &gt;= 0) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; /* @path names an existing file, use it */<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; break;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (errno == ENOENT) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; /* @path names a file that doesn't exist, create it */<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; fd = open(path, O_RDWR | O_CREAT | O_EXCL, 0644);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (fd &gt;= 0) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; unlink_on_error = true;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; break;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; } else if (errno == EISDIR) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; /* @path names a directory, create a file there */<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; /* Make name safe to use with mkstemp by replacing '/' with '_'. */<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; sanitized_name = g_strdup(memory_region_name(block-&gt;mr));<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for (c = sanitized_name; *c != '\0'; c++) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (*c == '/') {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; *c = '_';<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; filename = g_strdup_printf("%s/qemu_back_mem.%s.XXXXXX", path,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; sanitized_name);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; g_free(sanitized_name);</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; fd = mkstemp(filename);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (fd &gt;= 0) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; unlink(filename);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; g_free(filename);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; break;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; g_free(filename);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (errno != EEXIST &amp;&amp; errno != EINTR) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; error_setg_errno(errp, errno,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "can't open backing store %s for guest RAM",<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; path);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; goto error;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; /*<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; * Try again on EINTR and EEXIST.&nbsp; The latter happens when<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; * something else creates the file between our two open().<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; */<BR>&nbsp;&nbsp;&nbsp; }</P>
<P>&nbsp;&nbsp;&nbsp; block-&gt;page_size = qemu_fd_getpagesize(fd);<BR>&nbsp;&nbsp;&nbsp; block-&gt;mr-&gt;align = block-&gt;page_size;<BR>#if defined(__s390x__)<BR>&nbsp;&nbsp;&nbsp; if (kvm_enabled()) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; block-&gt;mr-&gt;align = MAX(block-&gt;mr-&gt;align, QEMU_VMALLOC_ALIGN);<BR>&nbsp;&nbsp;&nbsp; }<BR>#endif</P>
<P>&nbsp;&nbsp;&nbsp; file_size = get_file_size(fd);</P>
<P>&nbsp;&nbsp;&nbsp; if (memory &lt; block-&gt;page_size) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; error_setg(errp, "memory size 0x" RAM_ADDR_FMT " must be equal to "<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "or larger than page size 0x%zx",<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; memory, block-&gt;page_size);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; goto error;<BR>&nbsp;&nbsp;&nbsp; }</P>
<P>&nbsp;&nbsp;&nbsp; if (file_size &gt; 0 &amp;&amp; file_size &lt; memory) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; error_setg(errp, "backing store %s size 0x%" PRIx64<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; " does not match 'size' option 0x" RAM_ADDR_FMT,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; path, file_size, memory);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; goto error;<BR>&nbsp;&nbsp;&nbsp; }</P>
<P>&nbsp;&nbsp;&nbsp; memory = ROUND_UP(memory, block-&gt;page_size);</P>
<P>&nbsp;&nbsp;&nbsp; /*<BR>&nbsp;&nbsp;&nbsp;&nbsp; * ftruncate is not supported by hugetlbfs in older<BR>&nbsp;&nbsp;&nbsp;&nbsp; * hosts, so don't bother bailing out on errors.<BR>&nbsp;&nbsp;&nbsp;&nbsp; * If anything goes wrong with it under other filesystems,<BR>&nbsp;&nbsp;&nbsp;&nbsp; * mmap will fail.<BR>&nbsp;&nbsp;&nbsp;&nbsp; *<BR>&nbsp;&nbsp;&nbsp;&nbsp; * Do not truncate the non-empty backend file to avoid corrupting<BR>&nbsp;&nbsp;&nbsp;&nbsp; * the existing data in the file. Disabling shrinking is not<BR>&nbsp;&nbsp;&nbsp;&nbsp; * enough. For example, the current vNVDIMM implementation stores<BR>&nbsp;&nbsp;&nbsp;&nbsp; * the guest NVDIMM labels at the end of the backend file. If the<BR>&nbsp;&nbsp;&nbsp;&nbsp; * backend file is later extended, QEMU will not be able to find<BR>&nbsp;&nbsp;&nbsp;&nbsp; * those labels. Therefore, extending the non-empty backend file<BR>&nbsp;&nbsp;&nbsp;&nbsp; * is disabled as well.<BR>&nbsp;&nbsp;&nbsp;&nbsp; */<BR>&nbsp;&nbsp;&nbsp; if (!file_size &amp;&amp; ftruncate(fd, memory)) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; perror("ftruncate");<BR>&nbsp;&nbsp;&nbsp; }</P>
<P>&nbsp;&nbsp;&nbsp; area = qemu_ram_mmap(fd, memory, block-&gt;mr-&gt;align,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; block-&gt;flags &amp; RAM_SHARED);<BR>&nbsp;&nbsp;&nbsp; if (area == MAP_FAILED) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; error_setg_errno(errp, errno,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "unable to map backing store for guest RAM");<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; goto error;<BR>&nbsp;&nbsp;&nbsp; }</P>
<P>&nbsp;&nbsp;&nbsp; if (mem_prealloc) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; os_mem_prealloc(fd, area, memory, errp);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (errp &amp;&amp; *errp) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; goto error;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; }</P>
<P>&nbsp;&nbsp;&nbsp; block-&gt;fd = fd;<BR>&nbsp;&nbsp;&nbsp; return area;</P>
<P>error:<BR>&nbsp;&nbsp;&nbsp; if (area != MAP_FAILED) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; qemu_ram_munmap(area, memory);<BR>&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; if (unlink_on_error) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; unlink(path);<BR>&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; if (fd != -1) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; close(fd);<BR>&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; return NULL;<BR>}<BR>#endif</P>
<P>/* Called with the ramlist lock held.&nbsp; */<BR>static ram_addr_t find_ram_offset(ram_addr_t size)<BR>{<BR>&nbsp;&nbsp;&nbsp; RAMBlock *block, *next_block;<BR>&nbsp;&nbsp;&nbsp; ram_addr_t offset = RAM_ADDR_MAX, mingap = RAM_ADDR_MAX;</P>
<P>&nbsp;&nbsp;&nbsp; assert(size != 0); /* it would hand out same offset multiple times */</P>
<P>&nbsp;&nbsp;&nbsp; if (QLIST_EMPTY_RCU(&amp;ram_list.blocks)) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return 0;<BR>&nbsp;&nbsp;&nbsp; }</P>
<P>&nbsp;&nbsp;&nbsp; QLIST_FOREACH_RCU(block, &amp;ram_list.blocks, next) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ram_addr_t end, next = RAM_ADDR_MAX;</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; end = block-&gt;offset + block-&gt;max_length;</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; QLIST_FOREACH_RCU(next_block, &amp;ram_list.blocks, next) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (next_block-&gt;offset &gt;= end) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; next = MIN(next, next_block-&gt;offset);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (next - end &gt;= size &amp;&amp; next - end &lt; mingap) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; offset = end;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; mingap = next - end;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; }</P>
<P>&nbsp;&nbsp;&nbsp; if (offset == RAM_ADDR_MAX) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; fprintf(stderr, "Failed to find gap of requested size: %" PRIu64 "\n",<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (uint64_t)size);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; abort();<BR>&nbsp;&nbsp;&nbsp; }</P>
<P>&nbsp;&nbsp;&nbsp; return offset;<BR>}</P>
<P>ram_addr_t last_ram_offset(void)<BR>{<BR>&nbsp;&nbsp;&nbsp; RAMBlock *block;<BR>&nbsp;&nbsp;&nbsp; ram_addr_t last = 0;</P>
<P>&nbsp;&nbsp;&nbsp; rcu_read_lock();<BR>&nbsp;&nbsp;&nbsp; QLIST_FOREACH_RCU(block, &amp;ram_list.blocks, next) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; last = MAX(last, block-&gt;offset + block-&gt;max_length);<BR>&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; rcu_read_unlock();<BR>&nbsp;&nbsp;&nbsp; return last;<BR>}</P>
<P>static void qemu_ram_setup_dump(void *addr, ram_addr_t size)<BR>{<BR>&nbsp;&nbsp;&nbsp; int ret;</P>
<P>&nbsp;&nbsp;&nbsp; /* Use MADV_DONTDUMP, if user doesn't want the guest memory in the core */<BR>&nbsp;&nbsp;&nbsp; if (!machine_dump_guest_core(current_machine)) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ret = qemu_madvise(addr, size, QEMU_MADV_DONTDUMP);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (ret) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; perror("qemu_madvise");<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; fprintf(stderr, "madvise doesn't support MADV_DONTDUMP, "<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "but dump_guest_core=off specified\n");<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; }<BR>}</P>
<P>const char *qemu_ram_get_idstr(RAMBlock *rb)<BR>{<BR>&nbsp;&nbsp;&nbsp; return rb-&gt;idstr;<BR>}</P>
<P>/* Called with iothread lock held.&nbsp; */<BR>void qemu_ram_set_idstr(RAMBlock *new_block, const char *name, DeviceState *dev)<BR>{<BR>&nbsp;&nbsp;&nbsp; RAMBlock *block;</P>
<P>&nbsp;&nbsp;&nbsp; assert(new_block);<BR>&nbsp;&nbsp;&nbsp; assert(!new_block-&gt;idstr[0]);</P>
<P>&nbsp;&nbsp;&nbsp; if (dev) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; char *id = qdev_get_dev_path(dev);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (id) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; snprintf(new_block-&gt;idstr, sizeof(new_block-&gt;idstr), "%s/", id);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; g_free(id);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; pstrcat(new_block-&gt;idstr, sizeof(new_block-&gt;idstr), name);</P>
<P>&nbsp;&nbsp;&nbsp; rcu_read_lock();<BR>&nbsp;&nbsp;&nbsp; QLIST_FOREACH_RCU(block, &amp;ram_list.blocks, next) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (block != new_block &amp;&amp;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; !strcmp(block-&gt;idstr, new_block-&gt;idstr)) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; fprintf(stderr, "RAMBlock \"%s\" already registered, abort!\n",<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; new_block-&gt;idstr);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; abort();<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; rcu_read_unlock();<BR>}</P>
<P>/* Called with iothread lock held.&nbsp; */<BR>void qemu_ram_unset_idstr(RAMBlock *block)<BR>{<BR>&nbsp;&nbsp;&nbsp; /* FIXME: arch_init.c assumes that this is not called throughout<BR>&nbsp;&nbsp;&nbsp;&nbsp; * migration.&nbsp; Ignore the problem since hot-unplug during migration<BR>&nbsp;&nbsp;&nbsp;&nbsp; * does not work anyway.<BR>&nbsp;&nbsp;&nbsp;&nbsp; */<BR>&nbsp;&nbsp;&nbsp; if (block) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; memset(block-&gt;idstr, 0, sizeof(block-&gt;idstr));<BR>&nbsp;&nbsp;&nbsp; }<BR>}</P>
<P>size_t qemu_ram_pagesize(RAMBlock *rb)<BR>{<BR>&nbsp;&nbsp;&nbsp; return rb-&gt;page_size;<BR>}</P>
<P>static int memory_try_enable_merging(void *addr, size_t len)<BR>{<BR>&nbsp;&nbsp;&nbsp; if (!machine_mem_merge(current_machine)) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; /* disabled by the user */<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return 0;<BR>&nbsp;&nbsp;&nbsp; }</P>
<P>&nbsp;&nbsp;&nbsp; return qemu_madvise(addr, len, QEMU_MADV_MERGEABLE);<BR>}</P>
<P>/* Only legal before guest might have detected the memory size: e.g. on<BR>&nbsp;* incoming migration, or right after reset.<BR>&nbsp;*<BR>&nbsp;* As memory core doesn't know how is memory accessed, it is up to<BR>&nbsp;* resize callback to update device state and/or add assertions to detect<BR>&nbsp;* misuse, if necessary.<BR>&nbsp;*/<BR>int qemu_ram_resize(RAMBlock *block, ram_addr_t newsize, Error **errp)<BR>{<BR>&nbsp;&nbsp;&nbsp; assert(block);</P>
<P>&nbsp;&nbsp;&nbsp; newsize = HOST_PAGE_ALIGN(newsize);</P>
<P>&nbsp;&nbsp;&nbsp; if (block-&gt;used_length == newsize) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return 0;<BR>&nbsp;&nbsp;&nbsp; }</P>
<P>&nbsp;&nbsp;&nbsp; if (!(block-&gt;flags &amp; RAM_RESIZEABLE)) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; error_setg_errno(errp, EINVAL,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "Length mismatch: %s: 0x" RAM_ADDR_FMT<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; " in != 0x" RAM_ADDR_FMT, block-&gt;idstr,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; newsize, block-&gt;used_length);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return -EINVAL;<BR>&nbsp;&nbsp;&nbsp; }</P>
<P>&nbsp;&nbsp;&nbsp; if (block-&gt;max_length &lt; newsize) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; error_setg_errno(errp, EINVAL,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "Length too large: %s: 0x" RAM_ADDR_FMT<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; " &gt; 0x" RAM_ADDR_FMT, block-&gt;idstr,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; newsize, block-&gt;max_length);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return -EINVAL;<BR>&nbsp;&nbsp;&nbsp; }</P>
<P>&nbsp;&nbsp;&nbsp; cpu_physical_memory_clear_dirty_range(block-&gt;offset, block-&gt;used_length);<BR>&nbsp;&nbsp;&nbsp; block-&gt;used_length = newsize;<BR>&nbsp;&nbsp;&nbsp; cpu_physical_memory_set_dirty_range(block-&gt;offset, block-&gt;used_length,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; DIRTY_CLIENTS_ALL);<BR>&nbsp;&nbsp;&nbsp; memory_region_set_size(block-&gt;mr, newsize);<BR>&nbsp;&nbsp;&nbsp; if (block-&gt;resized) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; block-&gt;resized(block-&gt;idstr, newsize, block-&gt;host);<BR>&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; return 0;<BR>}</P>
<P>/* Called with ram_list.mutex held */<BR>static void dirty_memory_extend(ram_addr_t old_ram_size,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ram_addr_t new_ram_size)<BR>{<BR>&nbsp;&nbsp;&nbsp; ram_addr_t old_num_blocks = DIV_ROUND_UP(old_ram_size,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; DIRTY_MEMORY_BLOCK_SIZE);<BR>&nbsp;&nbsp;&nbsp; ram_addr_t new_num_blocks = DIV_ROUND_UP(new_ram_size,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; DIRTY_MEMORY_BLOCK_SIZE);<BR>&nbsp;&nbsp;&nbsp; int i;</P>
<P>&nbsp;&nbsp;&nbsp; /* Only need to extend if block count increased */<BR>&nbsp;&nbsp;&nbsp; if (new_num_blocks &lt;= old_num_blocks) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return;<BR>&nbsp;&nbsp;&nbsp; }</P>
<P>&nbsp;&nbsp;&nbsp; for (i = 0; i &lt; DIRTY_MEMORY_NUM; i++) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; DirtyMemoryBlocks *old_blocks;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; DirtyMemoryBlocks *new_blocks;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; int j;</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; old_blocks = atomic_rcu_read(&amp;ram_list.dirty_memory[i]);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; new_blocks = g_malloc(sizeof(*new_blocks) +<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; sizeof(new_blocks-&gt;blocks[0]) * new_num_blocks);</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (old_num_blocks) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; memcpy(new_blocks-&gt;blocks, old_blocks-&gt;blocks,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; old_num_blocks * sizeof(old_blocks-&gt;blocks[0]));<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for (j = old_num_blocks; j &lt; new_num_blocks; j++) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; new_blocks-&gt;blocks[j] = bitmap_new(DIRTY_MEMORY_BLOCK_SIZE);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; atomic_rcu_set(&amp;ram_list.dirty_memory[i], new_blocks);</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (old_blocks) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; g_free_rcu(old_blocks, rcu);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; }<BR>}</P>
<P>static void ram_block_add(RAMBlock *new_block, Error **errp)<BR>{<BR>&nbsp;&nbsp;&nbsp; RAMBlock *block;<BR>&nbsp;&nbsp;&nbsp; RAMBlock *last_block = NULL;<BR>&nbsp;&nbsp;&nbsp; ram_addr_t old_ram_size, new_ram_size;<BR>&nbsp;&nbsp;&nbsp; Error *err = NULL;</P>
<P>&nbsp;&nbsp;&nbsp; old_ram_size = last_ram_offset() &gt;&gt; TARGET_PAGE_BITS;</P>
<P>&nbsp;&nbsp;&nbsp; qemu_mutex_lock_ramlist();<BR>&nbsp;&nbsp;&nbsp; new_block-&gt;offset = find_ram_offset(new_block-&gt;max_length);</P>
<P>&nbsp;&nbsp;&nbsp; if (!new_block-&gt;host) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (xen_enabled()) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; xen_ram_alloc(new_block-&gt;offset, new_block-&gt;max_length,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; new_block-&gt;mr, &amp;err);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (err) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; error_propagate(errp, err);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; qemu_mutex_unlock_ramlist();<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; } else {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; new_block-&gt;host = phys_mem_alloc(new_block-&gt;max_length,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;new_block-&gt;mr-&gt;align);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (!new_block-&gt;host) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; error_setg_errno(errp, errno,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "cannot set up guest memory '%s'",<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; memory_region_name(new_block-&gt;mr));<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; qemu_mutex_unlock_ramlist();<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; memory_try_enable_merging(new_block-&gt;host, new_block-&gt;max_length);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; }</P>
<P>&nbsp;&nbsp;&nbsp; new_ram_size = MAX(old_ram_size,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (new_block-&gt;offset + new_block-&gt;max_length) &gt;&gt; TARGET_PAGE_BITS);<BR>&nbsp;&nbsp;&nbsp; if (new_ram_size &gt; old_ram_size) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; migration_bitmap_extend(old_ram_size, new_ram_size);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; dirty_memory_extend(old_ram_size, new_ram_size);<BR>&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; /* Keep the list sorted from biggest to smallest block.&nbsp; Unlike QTAILQ,<BR>&nbsp;&nbsp;&nbsp;&nbsp; * QLIST (which has an RCU-friendly variant) does not have insertion at<BR>&nbsp;&nbsp;&nbsp;&nbsp; * tail, so save the last element in last_block.<BR>&nbsp;&nbsp;&nbsp;&nbsp; */<BR>&nbsp;&nbsp;&nbsp; QLIST_FOREACH_RCU(block, &amp;ram_list.blocks, next) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; last_block = block;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (block-&gt;max_length &lt; new_block-&gt;max_length) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; break;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; if (block) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; QLIST_INSERT_BEFORE_RCU(block, new_block, next);<BR>&nbsp;&nbsp;&nbsp; } else if (last_block) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; QLIST_INSERT_AFTER_RCU(last_block, new_block, next);<BR>&nbsp;&nbsp;&nbsp; } else { /* list is empty */<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; QLIST_INSERT_HEAD_RCU(&amp;ram_list.blocks, new_block, next);<BR>&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; ram_list.mru_block = NULL;</P>
<P>&nbsp;&nbsp;&nbsp; /* Write list before version */<BR>&nbsp;&nbsp;&nbsp; smp_wmb();<BR>&nbsp;&nbsp;&nbsp; ram_list.version++;<BR>&nbsp;&nbsp;&nbsp; qemu_mutex_unlock_ramlist();</P>
<P>&nbsp;&nbsp;&nbsp; cpu_physical_memory_set_dirty_range(new_block-&gt;offset,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; new_block-&gt;used_length,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; DIRTY_CLIENTS_ALL);</P>
<P>&nbsp;&nbsp;&nbsp; if (new_block-&gt;host) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; qemu_ram_setup_dump(new_block-&gt;host, new_block-&gt;max_length);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; qemu_madvise(new_block-&gt;host, new_block-&gt;max_length, QEMU_MADV_HUGEPAGE);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; /* MADV_DONTFORK is also needed by KVM in absence of synchronous MMU */<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; qemu_madvise(new_block-&gt;host, new_block-&gt;max_length, QEMU_MADV_DONTFORK);<BR>&nbsp;&nbsp;&nbsp; }<BR>}</P>
<P>#ifdef __linux__<BR>RAMBlock *qemu_ram_alloc_from_file(ram_addr_t size, MemoryRegion *mr,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; bool share, const char *mem_path,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Error **errp)<BR>{<BR>&nbsp;&nbsp;&nbsp; RAMBlock *new_block;<BR>&nbsp;&nbsp;&nbsp; Error *local_err = NULL;</P>
<P>&nbsp;&nbsp;&nbsp; if (xen_enabled()) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; error_setg(errp, "-mem-path not supported with Xen");<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return NULL;<BR>&nbsp;&nbsp;&nbsp; }</P>
<P>&nbsp;&nbsp;&nbsp; if (phys_mem_alloc != qemu_anon_ram_alloc) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; /*<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; * file_ram_alloc() needs to allocate just like<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; * phys_mem_alloc, but we haven't bothered to provide<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; * a hook there.<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; */<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; error_setg(errp,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "-mem-path not supported with this accelerator");<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return NULL;<BR>&nbsp;&nbsp;&nbsp; }</P>
<P>&nbsp;&nbsp;&nbsp; size = HOST_PAGE_ALIGN(size);<BR>&nbsp;&nbsp;&nbsp; new_block = g_malloc0(sizeof(*new_block));<BR>&nbsp;&nbsp;&nbsp; new_block-&gt;mr = mr;<BR>&nbsp;&nbsp;&nbsp; new_block-&gt;used_length = size;<BR>&nbsp;&nbsp;&nbsp; new_block-&gt;max_length = size;<BR>&nbsp;&nbsp;&nbsp; new_block-&gt;flags = share ? RAM_SHARED : 0;<BR>&nbsp;&nbsp;&nbsp; new_block-&gt;host = file_ram_alloc(new_block, size,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; mem_path, errp);<BR>&nbsp;&nbsp;&nbsp; if (!new_block-&gt;host) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; g_free(new_block);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return NULL;<BR>&nbsp;&nbsp;&nbsp; }</P>
<P>&nbsp;&nbsp;&nbsp; ram_block_add(new_block, &amp;local_err);<BR>&nbsp;&nbsp;&nbsp; if (local_err) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; g_free(new_block);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; error_propagate(errp, local_err);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return NULL;<BR>&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; return new_block;<BR>}<BR>#endif</P>
<P>static<BR>RAMBlock *qemu_ram_alloc_internal(ram_addr_t size, ram_addr_t max_size,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; void (*resized)(const char*,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; uint64_t length,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; void *host),<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; void *host, bool resizeable,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; MemoryRegion *mr, Error **errp)<BR>{<BR>&nbsp;&nbsp;&nbsp; RAMBlock *new_block;<BR>&nbsp;&nbsp;&nbsp; Error *local_err = NULL;</P>
<P>&nbsp;&nbsp;&nbsp; size = HOST_PAGE_ALIGN(size);<BR>&nbsp;&nbsp;&nbsp; max_size = HOST_PAGE_ALIGN(max_size);<BR>&nbsp;&nbsp;&nbsp; new_block = g_malloc0(sizeof(*new_block));<BR>&nbsp;&nbsp;&nbsp; new_block-&gt;mr = mr;<BR>&nbsp;&nbsp;&nbsp; new_block-&gt;resized = resized;<BR>&nbsp;&nbsp;&nbsp; new_block-&gt;used_length = size;<BR>&nbsp;&nbsp;&nbsp; new_block-&gt;max_length = max_size;<BR>&nbsp;&nbsp;&nbsp; assert(max_size &gt;= size);<BR>&nbsp;&nbsp;&nbsp; new_block-&gt;fd = -1;<BR>&nbsp;&nbsp;&nbsp; new_block-&gt;page_size = getpagesize();<BR>&nbsp;&nbsp;&nbsp; new_block-&gt;host = host;<BR>&nbsp;&nbsp;&nbsp; if (host) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; new_block-&gt;flags |= RAM_PREALLOC;<BR>&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; if (resizeable) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; new_block-&gt;flags |= RAM_RESIZEABLE;<BR>&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; ram_block_add(new_block, &amp;local_err);<BR>&nbsp;&nbsp;&nbsp; if (local_err) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; g_free(new_block);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; error_propagate(errp, local_err);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return NULL;<BR>&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; return new_block;<BR>}</P>
<P>RAMBlock *qemu_ram_alloc_from_ptr(ram_addr_t size, void *host,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; MemoryRegion *mr, Error **errp)<BR>{<BR>&nbsp;&nbsp;&nbsp; return qemu_ram_alloc_internal(size, size, NULL, host, false, mr, errp);<BR>}</P>
<P>RAMBlock *qemu_ram_alloc(ram_addr_t size, MemoryRegion *mr, Error **errp)<BR>{<BR>&nbsp;&nbsp;&nbsp; return qemu_ram_alloc_internal(size, size, NULL, NULL, false, mr, errp);<BR>}</P>
<P>RAMBlock *qemu_ram_alloc_resizeable(ram_addr_t size, ram_addr_t maxsz,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; void (*resized)(const char*,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; uint64_t length,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; void *host),<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; MemoryRegion *mr, Error **errp)<BR>{<BR>&nbsp;&nbsp;&nbsp; return qemu_ram_alloc_internal(size, maxsz, resized, NULL, true, mr, errp);<BR>}</P>
<P>static void reclaim_ramblock(RAMBlock *block)<BR>{<BR>&nbsp;&nbsp;&nbsp; if (block-&gt;flags &amp; RAM_PREALLOC) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ;<BR>&nbsp;&nbsp;&nbsp; } else if (xen_enabled()) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; xen_invalidate_map_cache_entry(block-&gt;host);<BR>#ifndef _WIN32<BR>&nbsp;&nbsp;&nbsp; } else if (block-&gt;fd &gt;= 0) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; qemu_ram_munmap(block-&gt;host, block-&gt;max_length);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; close(block-&gt;fd);<BR>#endif<BR>&nbsp;&nbsp;&nbsp; } else {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; qemu_anon_ram_free(block-&gt;host, block-&gt;max_length);<BR>&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; g_free(block);<BR>}</P>
<P>void qemu_ram_free(RAMBlock *block)<BR>{<BR>&nbsp;&nbsp;&nbsp; if (!block) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return;<BR>&nbsp;&nbsp;&nbsp; }</P>
<P>&nbsp;&nbsp;&nbsp; qemu_mutex_lock_ramlist();<BR>&nbsp;&nbsp;&nbsp; QLIST_REMOVE_RCU(block, next);<BR>&nbsp;&nbsp;&nbsp; ram_list.mru_block = NULL;<BR>&nbsp;&nbsp;&nbsp; /* Write list before version */<BR>&nbsp;&nbsp;&nbsp; smp_wmb();<BR>&nbsp;&nbsp;&nbsp; ram_list.version++;<BR>&nbsp;&nbsp;&nbsp; call_rcu(block, reclaim_ramblock, rcu);<BR>&nbsp;&nbsp;&nbsp; qemu_mutex_unlock_ramlist();<BR>}</P>
<P>#ifndef _WIN32<BR>void qemu_ram_remap(ram_addr_t addr, ram_addr_t length)<BR>{<BR>&nbsp;&nbsp;&nbsp; RAMBlock *block;<BR>&nbsp;&nbsp;&nbsp; ram_addr_t offset;<BR>&nbsp;&nbsp;&nbsp; int flags;<BR>&nbsp;&nbsp;&nbsp; void *area, *vaddr;</P>
<P>&nbsp;&nbsp;&nbsp; QLIST_FOREACH_RCU(block, &amp;ram_list.blocks, next) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; offset = addr - block-&gt;offset;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (offset &lt; block-&gt;max_length) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; vaddr = ramblock_ptr(block, offset);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (block-&gt;flags &amp; RAM_PREALLOC) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; } else if (xen_enabled()) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; abort();<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; } else {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; flags = MAP_FIXED;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (block-&gt;fd &gt;= 0) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; flags |= (block-&gt;flags &amp; RAM_SHARED ?<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; MAP_SHARED : MAP_PRIVATE);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; area = mmap(vaddr, length, PROT_READ | PROT_WRITE,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; flags, block-&gt;fd, offset);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; } else {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; /*<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; * Remap needs to match alloc.&nbsp; Accelerators that<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; * set phys_mem_alloc never remap.&nbsp; If they did,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; * we'd need a remap hook here.<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; */<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; assert(phys_mem_alloc == qemu_anon_ram_alloc);</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; flags |= MAP_PRIVATE | MAP_ANONYMOUS;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; area = mmap(vaddr, length, PROT_READ | PROT_WRITE,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; flags, -1, 0);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (area != vaddr) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; fprintf(stderr, "Could not remap addr: "<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; RAM_ADDR_FMT "@" RAM_ADDR_FMT "\n",<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; length, addr);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; exit(1);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; memory_try_enable_merging(vaddr, length);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; qemu_ram_setup_dump(vaddr, length);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; }<BR>}<BR>#endif /* !_WIN32 */</P>
<P>/* Return a host pointer to ram allocated with qemu_ram_alloc.<BR>&nbsp;* This should not be used for general purpose DMA.&nbsp; Use address_space_map<BR>&nbsp;* or address_space_rw instead. For local memory (e.g. video ram) that the<BR>&nbsp;* device owns, use memory_region_get_ram_ptr.<BR>&nbsp;*<BR>&nbsp;* Called within RCU critical section.<BR>&nbsp;*/<BR>void *qemu_map_ram_ptr(RAMBlock *ram_block, ram_addr_t addr)<BR>{<BR>&nbsp;&nbsp;&nbsp; RAMBlock *block = ram_block;</P>
<P>&nbsp;&nbsp;&nbsp; if (block == NULL) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; block = qemu_get_ram_block(addr);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; addr -= block-&gt;offset;<BR>&nbsp;&nbsp;&nbsp; }</P>
<P>&nbsp;&nbsp;&nbsp; if (xen_enabled() &amp;&amp; block-&gt;host == NULL) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; /* We need to check if the requested address is in the RAM<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; * because we don't want to map the entire memory in QEMU.<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; * In that case just map until the end of the page.<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; */<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (block-&gt;offset == 0) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return xen_map_cache(addr, 0, 0);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; block-&gt;host = xen_map_cache(block-&gt;offset, block-&gt;max_length, 1);<BR>&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; return ramblock_ptr(block, addr);<BR>}</P>
<P>/* Return a host pointer to guest's ram. Similar to qemu_map_ram_ptr<BR>&nbsp;* but takes a size argument.<BR>&nbsp;*<BR>&nbsp;* Called within RCU critical section.<BR>&nbsp;*/<BR>static void *qemu_ram_ptr_length(RAMBlock *ram_block, ram_addr_t addr,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; hwaddr *size)<BR>{<BR>&nbsp;&nbsp;&nbsp; RAMBlock *block = ram_block;<BR>&nbsp;&nbsp;&nbsp; if (*size == 0) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return NULL;<BR>&nbsp;&nbsp;&nbsp; }</P>
<P>&nbsp;&nbsp;&nbsp; if (block == NULL) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; block = qemu_get_ram_block(addr);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; addr -= block-&gt;offset;<BR>&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; *size = MIN(*size, block-&gt;max_length - addr);</P>
<P>&nbsp;&nbsp;&nbsp; if (xen_enabled() &amp;&amp; block-&gt;host == NULL) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; /* We need to check if the requested address is in the RAM<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; * because we don't want to map the entire memory in QEMU.<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; * In that case just map the requested area.<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; */<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (block-&gt;offset == 0) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return xen_map_cache(addr, *size, 1);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; block-&gt;host = xen_map_cache(block-&gt;offset, block-&gt;max_length, 1);<BR>&nbsp;&nbsp;&nbsp; }</P>
<P>&nbsp;&nbsp;&nbsp; return ramblock_ptr(block, addr);<BR>}</P>
<P>/*<BR>&nbsp;* Translates a host ptr back to a RAMBlock, a ram_addr and an offset<BR>&nbsp;* in that RAMBlock.<BR>&nbsp;*<BR>&nbsp;* ptr: Host pointer to look up<BR>&nbsp;* round_offset: If true round the result offset down to a page boundary<BR>&nbsp;* *ram_addr: set to result ram_addr<BR>&nbsp;* *offset: set to result offset within the RAMBlock<BR>&nbsp;*<BR>&nbsp;* Returns: RAMBlock (or NULL if not found)<BR>&nbsp;*<BR>&nbsp;* By the time this function returns, the returned pointer is not protected<BR>&nbsp;* by RCU anymore.&nbsp; If the caller is not within an RCU critical section and<BR>&nbsp;* does not hold the iothread lock, it must have other means of protecting the<BR>&nbsp;* pointer, such as a reference to the region that includes the incoming<BR>&nbsp;* ram_addr_t.<BR>&nbsp;*/<BR>RAMBlock *qemu_ram_block_from_host(void *ptr, bool round_offset,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ram_addr_t *offset)<BR>{<BR>&nbsp;&nbsp;&nbsp; RAMBlock *block;<BR>&nbsp;&nbsp;&nbsp; uint8_t *host = ptr;</P>
<P>&nbsp;&nbsp;&nbsp; if (xen_enabled()) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ram_addr_t ram_addr;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; rcu_read_lock();<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ram_addr = xen_ram_addr_from_mapcache(ptr);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; block = qemu_get_ram_block(ram_addr);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (block) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; *offset = ram_addr - block-&gt;offset;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; rcu_read_unlock();<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return block;<BR>&nbsp;&nbsp;&nbsp; }</P>
<P>&nbsp;&nbsp;&nbsp; rcu_read_lock();<BR>&nbsp;&nbsp;&nbsp; block = atomic_rcu_read(&amp;ram_list.mru_block);<BR>&nbsp;&nbsp;&nbsp; if (block &amp;&amp; block-&gt;host &amp;&amp; host - block-&gt;host &lt; block-&gt;max_length) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; goto found;<BR>&nbsp;&nbsp;&nbsp; }</P>
<P>&nbsp;&nbsp;&nbsp; QLIST_FOREACH_RCU(block, &amp;ram_list.blocks, next) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; /* This case append when the block is not mapped. */<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (block-&gt;host == NULL) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; continue;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (host - block-&gt;host &lt; block-&gt;max_length) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; goto found;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; }</P>
<P>&nbsp;&nbsp;&nbsp; rcu_read_unlock();<BR>&nbsp;&nbsp;&nbsp; return NULL;</P>
<P>found:<BR>&nbsp;&nbsp;&nbsp; *offset = (host - block-&gt;host);<BR>&nbsp;&nbsp;&nbsp; if (round_offset) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; *offset &amp;= TARGET_PAGE_MASK;<BR>&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; rcu_read_unlock();<BR>&nbsp;&nbsp;&nbsp; return block;<BR>}</P>
<P>/*<BR>&nbsp;* Finds the named RAMBlock<BR>&nbsp;*<BR>&nbsp;* name: The name of RAMBlock to find<BR>&nbsp;*<BR>&nbsp;* Returns: RAMBlock (or NULL if not found)<BR>&nbsp;*/<BR>RAMBlock *qemu_ram_block_by_name(const char *name)<BR>{<BR>&nbsp;&nbsp;&nbsp; RAMBlock *block;</P>
<P>&nbsp;&nbsp;&nbsp; QLIST_FOREACH_RCU(block, &amp;ram_list.blocks, next) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (!strcmp(name, block-&gt;idstr)) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return block;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; }</P>
<P>&nbsp;&nbsp;&nbsp; return NULL;<BR>}</P>
<P>/* Some of the softmmu routines need to translate from a host pointer<BR>&nbsp;&nbsp; (typically a TLB entry) back to a ram offset.&nbsp; */<BR>ram_addr_t qemu_ram_addr_from_host(void *ptr)<BR>{<BR>&nbsp;&nbsp;&nbsp; RAMBlock *block;<BR>&nbsp;&nbsp;&nbsp; ram_addr_t offset;</P>
<P>&nbsp;&nbsp;&nbsp; block = qemu_ram_block_from_host(ptr, false, &amp;offset);<BR>&nbsp;&nbsp;&nbsp; if (!block) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return RAM_ADDR_INVALID;<BR>&nbsp;&nbsp;&nbsp; }</P>
<P>&nbsp;&nbsp;&nbsp; return block-&gt;offset + offset;<BR>}</P>
<P>/* Called within RCU critical section.&nbsp; */<BR>static void notdirty_mem_write(void *opaque, hwaddr ram_addr,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; uint64_t val, unsigned size)<BR>{<BR>&nbsp;&nbsp;&nbsp; bool locked = false;</P>
<P>&nbsp;&nbsp;&nbsp; if (!cpu_physical_memory_get_dirty_flag(ram_addr, DIRTY_MEMORY_CODE)) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; locked = true;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; tb_lock();<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; tb_invalidate_phys_page_fast(ram_addr, size);<BR>&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; switch (size) {<BR>&nbsp;&nbsp;&nbsp; case 1:<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; stb_p(qemu_map_ram_ptr(NULL, ram_addr), val);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; break;<BR>&nbsp;&nbsp;&nbsp; case 2:<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; stw_p(qemu_map_ram_ptr(NULL, ram_addr), val);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; break;<BR>&nbsp;&nbsp;&nbsp; case 4:<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; stl_p(qemu_map_ram_ptr(NULL, ram_addr), val);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; break;<BR>&nbsp;&nbsp;&nbsp; default:<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; abort();<BR>&nbsp;&nbsp;&nbsp; }</P>
<P>&nbsp;&nbsp;&nbsp; if (locked) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; tb_unlock();<BR>&nbsp;&nbsp;&nbsp; }</P>
<P>&nbsp;&nbsp;&nbsp; /* Set both VGA and migration bits for simplicity and to remove<BR>&nbsp;&nbsp;&nbsp;&nbsp; * the notdirty callback faster.<BR>&nbsp;&nbsp;&nbsp;&nbsp; */<BR>&nbsp;&nbsp;&nbsp; cpu_physical_memory_set_dirty_range(ram_addr, size,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; DIRTY_CLIENTS_NOCODE);<BR>&nbsp;&nbsp;&nbsp; /* we remove the notdirty callback only if the code has been<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; flushed */<BR>&nbsp;&nbsp;&nbsp; if (!cpu_physical_memory_is_clean(ram_addr)) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; tlb_set_dirty(current_cpu, current_cpu-&gt;mem_io_vaddr);<BR>&nbsp;&nbsp;&nbsp; }<BR>}</P>
<P>static bool notdirty_mem_accepts(void *opaque, hwaddr addr,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; unsigned size, bool is_write)<BR>{<BR>&nbsp;&nbsp;&nbsp; return is_write;<BR>}</P>
<P>static const MemoryRegionOps notdirty_mem_ops = {<BR>&nbsp;&nbsp;&nbsp; .write = notdirty_mem_write,<BR>&nbsp;&nbsp;&nbsp; .valid.accepts = notdirty_mem_accepts,<BR>&nbsp;&nbsp;&nbsp; .endianness = DEVICE_NATIVE_ENDIAN,<BR>};</P>
<P>/* Generate a debug exception if a watchpoint has been hit.&nbsp; */<BR>static void check_watchpoint(int offset, int len, MemTxAttrs attrs, int flags)<BR>{<BR>&nbsp;&nbsp;&nbsp; CPUState *cpu = current_cpu;<BR>&nbsp;&nbsp;&nbsp; CPUClass *cc = CPU_GET_CLASS(cpu);<BR>&nbsp;&nbsp;&nbsp; CPUArchState *env = cpu-&gt;env_ptr;<BR>&nbsp;&nbsp;&nbsp; target_ulong pc, cs_base;<BR>&nbsp;&nbsp;&nbsp; target_ulong vaddr;<BR>&nbsp;&nbsp;&nbsp; CPUWatchpoint *wp;<BR>&nbsp;&nbsp;&nbsp; uint32_t cpu_flags;</P>
<P>&nbsp;&nbsp;&nbsp; if (cpu-&gt;watchpoint_hit) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; /* We re-entered the check after replacing the TB. Now raise<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; * the debug interrupt so that is will trigger after the<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; * current instruction. */<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; cpu_interrupt(cpu, CPU_INTERRUPT_DEBUG);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return;<BR>&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; vaddr = (cpu-&gt;mem_io_vaddr &amp; TARGET_PAGE_MASK) + offset;<BR>&nbsp;&nbsp;&nbsp; QTAILQ_FOREACH(wp, &amp;cpu-&gt;watchpoints, entry) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (cpu_watchpoint_address_matches(wp, vaddr, len)<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;&amp; (wp-&gt;flags &amp; flags)) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (flags == BP_MEM_READ) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; wp-&gt;flags |= BP_WATCHPOINT_HIT_READ;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; } else {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; wp-&gt;flags |= BP_WATCHPOINT_HIT_WRITE;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; wp-&gt;hitaddr = vaddr;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; wp-&gt;hitattrs = attrs;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (!cpu-&gt;watchpoint_hit) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (wp-&gt;flags &amp; BP_CPU &amp;&amp;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; !cc-&gt;debug_check_watchpoint(cpu, wp)) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; wp-&gt;flags &amp;= ~BP_WATCHPOINT_HIT;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; continue;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; cpu-&gt;watchpoint_hit = wp;</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; /* The tb_lock will be reset when cpu_loop_exit or<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; * cpu_loop_exit_noexc longjmp back into the cpu_exec<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; * main loop.<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; */<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; tb_lock();<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; tb_check_watchpoint(cpu);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (wp-&gt;flags &amp; BP_STOP_BEFORE_ACCESS) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; cpu-&gt;exception_index = EXCP_DEBUG;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; cpu_loop_exit(cpu);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; } else {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; cpu_get_tb_cpu_state(env, &amp;pc, &amp;cs_base, &amp;cpu_flags);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; tb_gen_code(cpu, pc, cs_base, cpu_flags, 1);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; cpu_loop_exit_noexc(cpu);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; } else {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; wp-&gt;flags &amp;= ~BP_WATCHPOINT_HIT;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; }<BR>}</P>
<P>/* Watchpoint access routines.&nbsp; Watchpoints are inserted using TLB tricks,<BR>&nbsp;&nbsp; so these check for a hit then pass through to the normal out-of-line<BR>&nbsp;&nbsp; phys routines.&nbsp; */<BR>static MemTxResult watch_mem_read(void *opaque, hwaddr addr, uint64_t *pdata,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; unsigned size, MemTxAttrs attrs)<BR>{<BR>&nbsp;&nbsp;&nbsp; MemTxResult res;<BR>&nbsp;&nbsp;&nbsp; uint64_t data;<BR>&nbsp;&nbsp;&nbsp; int asidx = cpu_asidx_from_attrs(current_cpu, attrs);<BR>&nbsp;&nbsp;&nbsp; AddressSpace *as = current_cpu-&gt;cpu_ases[asidx].as;</P>
<P>&nbsp;&nbsp;&nbsp; check_watchpoint(addr &amp; ~TARGET_PAGE_MASK, size, attrs, BP_MEM_READ);<BR>&nbsp;&nbsp;&nbsp; switch (size) {<BR>&nbsp;&nbsp;&nbsp; case 1:<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; data = address_space_ldub(as, addr, attrs, &amp;res);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; break;<BR>&nbsp;&nbsp;&nbsp; case 2:<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; data = address_space_lduw(as, addr, attrs, &amp;res);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; break;<BR>&nbsp;&nbsp;&nbsp; case 4:<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; data = address_space_ldl(as, addr, attrs, &amp;res);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; break;<BR>&nbsp;&nbsp;&nbsp; default: abort();<BR>&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; *pdata = data;<BR>&nbsp;&nbsp;&nbsp; return res;<BR>}</P>
<P>static MemTxResult watch_mem_write(void *opaque, hwaddr addr,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; uint64_t val, unsigned size,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; MemTxAttrs attrs)<BR>{<BR>&nbsp;&nbsp;&nbsp; MemTxResult res;<BR>&nbsp;&nbsp;&nbsp; int asidx = cpu_asidx_from_attrs(current_cpu, attrs);<BR>&nbsp;&nbsp;&nbsp; AddressSpace *as = current_cpu-&gt;cpu_ases[asidx].as;</P>
<P>&nbsp;&nbsp;&nbsp; check_watchpoint(addr &amp; ~TARGET_PAGE_MASK, size, attrs, BP_MEM_WRITE);<BR>&nbsp;&nbsp;&nbsp; switch (size) {<BR>&nbsp;&nbsp;&nbsp; case 1:<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; address_space_stb(as, addr, val, attrs, &amp;res);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; break;<BR>&nbsp;&nbsp;&nbsp; case 2:<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; address_space_stw(as, addr, val, attrs, &amp;res);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; break;<BR>&nbsp;&nbsp;&nbsp; case 4:<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; address_space_stl(as, addr, val, attrs, &amp;res);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; break;<BR>&nbsp;&nbsp;&nbsp; default: abort();<BR>&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; return res;<BR>}</P>
<P>static const MemoryRegionOps watch_mem_ops = {<BR>&nbsp;&nbsp;&nbsp; .read_with_attrs = watch_mem_read,<BR>&nbsp;&nbsp;&nbsp; .write_with_attrs = watch_mem_write,<BR>&nbsp;&nbsp;&nbsp; .endianness = DEVICE_NATIVE_ENDIAN,<BR>};</P>
<P>static MemTxResult subpage_read(void *opaque, hwaddr addr, uint64_t *data,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; unsigned len, MemTxAttrs attrs)<BR>{<BR>&nbsp;&nbsp;&nbsp; subpage_t *subpage = opaque;<BR>&nbsp;&nbsp;&nbsp; uint8_t buf[8];<BR>&nbsp;&nbsp;&nbsp; MemTxResult res;</P>
<P>#if defined(DEBUG_SUBPAGE)<BR>&nbsp;&nbsp;&nbsp; printf("%s: subpage %p len %u addr " TARGET_FMT_plx "\n", __func__,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; subpage, len, addr);<BR>#endif<BR>&nbsp;&nbsp;&nbsp; res = address_space_read(subpage-&gt;as, addr + subpage-&gt;base,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; attrs, buf, len);<BR>&nbsp;&nbsp;&nbsp; if (res) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return res;<BR>&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; switch (len) {<BR>&nbsp;&nbsp;&nbsp; case 1:<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; *data = ldub_p(buf);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return MEMTX_OK;<BR>&nbsp;&nbsp;&nbsp; case 2:<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; *data = lduw_p(buf);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return MEMTX_OK;<BR>&nbsp;&nbsp;&nbsp; case 4:<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; *data = ldl_p(buf);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return MEMTX_OK;<BR>&nbsp;&nbsp;&nbsp; case 8:<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; *data = ldq_p(buf);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return MEMTX_OK;<BR>&nbsp;&nbsp;&nbsp; default:<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; abort();<BR>&nbsp;&nbsp;&nbsp; }<BR>}</P>
<P>static MemTxResult subpage_write(void *opaque, hwaddr addr,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; uint64_t value, unsigned len, MemTxAttrs attrs)<BR>{<BR>&nbsp;&nbsp;&nbsp; subpage_t *subpage = opaque;<BR>&nbsp;&nbsp;&nbsp; uint8_t buf[8];</P>
<P>#if defined(DEBUG_SUBPAGE)<BR>&nbsp;&nbsp;&nbsp; printf("%s: subpage %p len %u addr " TARGET_FMT_plx<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; " value %"PRIx64"\n",<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; __func__, subpage, len, addr, value);<BR>#endif<BR>&nbsp;&nbsp;&nbsp; switch (len) {<BR>&nbsp;&nbsp;&nbsp; case 1:<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; stb_p(buf, value);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; break;<BR>&nbsp;&nbsp;&nbsp; case 2:<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; stw_p(buf, value);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; break;<BR>&nbsp;&nbsp;&nbsp; case 4:<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; stl_p(buf, value);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; break;<BR>&nbsp;&nbsp;&nbsp; case 8:<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; stq_p(buf, value);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; break;<BR>&nbsp;&nbsp;&nbsp; default:<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; abort();<BR>&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; return address_space_write(subpage-&gt;as, addr + subpage-&gt;base,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; attrs, buf, len);<BR>}</P>
<P>static bool subpage_accepts(void *opaque, hwaddr addr,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; unsigned len, bool is_write)<BR>{<BR>&nbsp;&nbsp;&nbsp; subpage_t *subpage = opaque;<BR>#if defined(DEBUG_SUBPAGE)<BR>&nbsp;&nbsp;&nbsp; printf("%s: subpage %p %c len %u addr " TARGET_FMT_plx "\n",<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; __func__, subpage, is_write ? 'w' : 'r', len, addr);<BR>#endif</P>
<P>&nbsp;&nbsp;&nbsp; return address_space_access_valid(subpage-&gt;as, addr + subpage-&gt;base,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; len, is_write);<BR>}</P>
<P>static const MemoryRegionOps subpage_ops = {<BR>&nbsp;&nbsp;&nbsp; .read_with_attrs = subpage_read,<BR>&nbsp;&nbsp;&nbsp; .write_with_attrs = subpage_write,<BR>&nbsp;&nbsp;&nbsp; .impl.min_access_size = 1,<BR>&nbsp;&nbsp;&nbsp; .impl.max_access_size = 8,<BR>&nbsp;&nbsp;&nbsp; .valid.min_access_size = 1,<BR>&nbsp;&nbsp;&nbsp; .valid.max_access_size = 8,<BR>&nbsp;&nbsp;&nbsp; .valid.accepts = subpage_accepts,<BR>&nbsp;&nbsp;&nbsp; .endianness = DEVICE_NATIVE_ENDIAN,<BR>};</P>
<P>static int subpage_register (subpage_t *mmio, uint32_t start, uint32_t end,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; uint16_t section)<BR>{<BR>&nbsp;&nbsp;&nbsp; int idx, eidx;</P>
<P>&nbsp;&nbsp;&nbsp; if (start &gt;= TARGET_PAGE_SIZE || end &gt;= TARGET_PAGE_SIZE)<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return -1;<BR>&nbsp;&nbsp;&nbsp; idx = SUBPAGE_IDX(start);<BR>&nbsp;&nbsp;&nbsp; eidx = SUBPAGE_IDX(end);<BR>#if defined(DEBUG_SUBPAGE)<BR>&nbsp;&nbsp;&nbsp; printf("%s: %p start %08x end %08x idx %08x eidx %08x section %d\n",<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; __func__, mmio, start, end, idx, eidx, section);<BR>#endif<BR>&nbsp;&nbsp;&nbsp; for (; idx &lt;= eidx; idx++) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; mmio-&gt;sub_section[idx] = section;<BR>&nbsp;&nbsp;&nbsp; }</P>
<P>&nbsp;&nbsp;&nbsp; return 0;<BR>}</P>
<P>static subpage_t *subpage_init(AddressSpace *as, hwaddr base)<BR>{<BR>&nbsp;&nbsp;&nbsp; subpage_t *mmio;</P>
<P>&nbsp;&nbsp;&nbsp; mmio = g_malloc0(sizeof(subpage_t) + TARGET_PAGE_SIZE * sizeof(uint16_t));<BR>&nbsp;&nbsp;&nbsp; mmio-&gt;as = as;<BR>&nbsp;&nbsp;&nbsp; mmio-&gt;base = base;<BR>&nbsp;&nbsp;&nbsp; memory_region_init_io(&amp;mmio-&gt;iomem, NULL, &amp;subpage_ops, mmio,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; NULL, TARGET_PAGE_SIZE);<BR>&nbsp;&nbsp;&nbsp; mmio-&gt;iomem.subpage = true;<BR>#if defined(DEBUG_SUBPAGE)<BR>&nbsp;&nbsp;&nbsp; printf("%s: %p base " TARGET_FMT_plx " len %08x\n", __func__,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; mmio, base, TARGET_PAGE_SIZE);<BR>#endif<BR>&nbsp;&nbsp;&nbsp; subpage_register(mmio, 0, TARGET_PAGE_SIZE-1, PHYS_SECTION_UNASSIGNED);</P>
<P>&nbsp;&nbsp;&nbsp; return mmio;<BR>}</P>
<P>static uint16_t dummy_section(PhysPageMap *map, AddressSpace *as,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; MemoryRegion *mr)<BR>{<BR>&nbsp;&nbsp;&nbsp; assert(as);<BR>&nbsp;&nbsp;&nbsp; MemoryRegionSection section = {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; .address_space = as,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; .mr = mr,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; .offset_within_address_space = 0,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; .offset_within_region = 0,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; .size = int128_2_64(),<BR>&nbsp;&nbsp;&nbsp; };</P>
<P>&nbsp;&nbsp;&nbsp; return phys_section_add(map, &amp;section);<BR>}</P>
<P>MemoryRegion *iotlb_to_region(CPUState *cpu, hwaddr index, MemTxAttrs attrs)<BR>{<BR>&nbsp;&nbsp;&nbsp; int asidx = cpu_asidx_from_attrs(cpu, attrs);<BR>&nbsp;&nbsp;&nbsp; CPUAddressSpace *cpuas = &amp;cpu-&gt;cpu_ases[asidx];<BR>&nbsp;&nbsp;&nbsp; AddressSpaceDispatch *d = atomic_rcu_read(&amp;cpuas-&gt;memory_dispatch);<BR>&nbsp;&nbsp;&nbsp; MemoryRegionSection *sections = d-&gt;map.sections;</P>
<P>&nbsp;&nbsp;&nbsp; return sections[index &amp; ~TARGET_PAGE_MASK].mr;<BR>}</P>
<P>static void io_mem_init(void)<BR>{<BR>&nbsp;&nbsp;&nbsp; memory_region_init_io(&amp;io_mem_rom, NULL, &amp;unassigned_mem_ops, NULL, NULL, UINT64_MAX);<BR>&nbsp;&nbsp;&nbsp; memory_region_init_io(&amp;io_mem_unassigned, NULL, &amp;unassigned_mem_ops, NULL,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; NULL, UINT64_MAX);<BR>&nbsp;&nbsp;&nbsp; memory_region_init_io(&amp;io_mem_notdirty, NULL, &amp;notdirty_mem_ops, NULL,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; NULL, UINT64_MAX);<BR>&nbsp;&nbsp;&nbsp; memory_region_init_io(&amp;io_mem_watch, NULL, &amp;watch_mem_ops, NULL,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; NULL, UINT64_MAX);<BR>}</P>
<P>static void mem_begin(MemoryListener *listener)<BR>{<BR>&nbsp;&nbsp;&nbsp; AddressSpace *as = container_of(listener, AddressSpace, dispatch_listener);<BR>&nbsp;&nbsp;&nbsp; AddressSpaceDispatch *d = g_new0(AddressSpaceDispatch, 1);<BR>&nbsp;&nbsp;&nbsp; uint16_t n;</P>
<P>&nbsp;&nbsp;&nbsp; n = dummy_section(&amp;d-&gt;map, as, &amp;io_mem_unassigned);<BR>&nbsp;&nbsp;&nbsp; assert(n == PHYS_SECTION_UNASSIGNED);<BR>&nbsp;&nbsp;&nbsp; n = dummy_section(&amp;d-&gt;map, as, &amp;io_mem_notdirty);<BR>&nbsp;&nbsp;&nbsp; assert(n == PHYS_SECTION_NOTDIRTY);<BR>&nbsp;&nbsp;&nbsp; n = dummy_section(&amp;d-&gt;map, as, &amp;io_mem_rom);<BR>&nbsp;&nbsp;&nbsp; assert(n == PHYS_SECTION_ROM);<BR>&nbsp;&nbsp;&nbsp; n = dummy_section(&amp;d-&gt;map, as, &amp;io_mem_watch);<BR>&nbsp;&nbsp;&nbsp; assert(n == PHYS_SECTION_WATCH);</P>
<P>&nbsp;&nbsp;&nbsp; d-&gt;phys_map&nbsp; = (PhysPageEntry) { .ptr = PHYS_MAP_NODE_NIL, .skip = 1 };<BR>&nbsp;&nbsp;&nbsp; d-&gt;as = as;<BR>&nbsp;&nbsp;&nbsp; as-&gt;next_dispatch = d;<BR>}</P>
<P>static void address_space_dispatch_free(AddressSpaceDispatch *d)<BR>{<BR>&nbsp;&nbsp;&nbsp; phys_sections_free(&amp;d-&gt;map);<BR>&nbsp;&nbsp;&nbsp; g_free(d);<BR>}</P>
<P>static void mem_commit(MemoryListener *listener)<BR>{<BR>&nbsp;&nbsp;&nbsp; AddressSpace *as = container_of(listener, AddressSpace, dispatch_listener);<BR>&nbsp;&nbsp;&nbsp; AddressSpaceDispatch *cur = as-&gt;dispatch;<BR>&nbsp;&nbsp;&nbsp; AddressSpaceDispatch *next = as-&gt;next_dispatch;</P>
<P>&nbsp;&nbsp;&nbsp; phys_page_compact_all(next, next-&gt;map.nodes_nb);</P>
<P>&nbsp;&nbsp;&nbsp; atomic_rcu_set(&amp;as-&gt;dispatch, next);<BR>&nbsp;&nbsp;&nbsp; if (cur) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; call_rcu(cur, address_space_dispatch_free, rcu);<BR>&nbsp;&nbsp;&nbsp; }<BR>}</P>
<P>static void tcg_commit(MemoryListener *listener)<BR>{<BR>&nbsp;&nbsp;&nbsp; CPUAddressSpace *cpuas;<BR>&nbsp;&nbsp;&nbsp; AddressSpaceDispatch *d;</P>
<P>&nbsp;&nbsp;&nbsp; /* since each CPU stores ram addresses in its TLB cache, we must<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; reset the modified entries */<BR>&nbsp;&nbsp;&nbsp; cpuas = container_of(listener, CPUAddressSpace, tcg_as_listener);<BR>&nbsp;&nbsp;&nbsp; cpu_reloading_memory_map();<BR>&nbsp;&nbsp;&nbsp; /* The CPU and TLB are protected by the iothread lock.<BR>&nbsp;&nbsp;&nbsp;&nbsp; * We reload the dispatch pointer now because cpu_reloading_memory_map()<BR>&nbsp;&nbsp;&nbsp;&nbsp; * may have split the RCU critical section.<BR>&nbsp;&nbsp;&nbsp;&nbsp; */<BR>&nbsp;&nbsp;&nbsp; d = atomic_rcu_read(&amp;cpuas-&gt;as-&gt;dispatch);<BR>&nbsp;&nbsp;&nbsp; atomic_rcu_set(&amp;cpuas-&gt;memory_dispatch, d);<BR>&nbsp;&nbsp;&nbsp; tlb_flush(cpuas-&gt;cpu);<BR>}</P>
<P>void address_space_init_dispatch(AddressSpace *as)<BR>{<BR>&nbsp;&nbsp;&nbsp; as-&gt;dispatch = NULL;<BR>&nbsp;&nbsp;&nbsp; as-&gt;dispatch_listener = (MemoryListener) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; .begin = mem_begin,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; .commit = mem_commit,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; .region_add = mem_add,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; .region_nop = mem_add,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; .priority = 0,<BR>&nbsp;&nbsp;&nbsp; };<BR>&nbsp;&nbsp;&nbsp; memory_listener_register(&amp;as-&gt;dispatch_listener, as);<BR>}</P>
<P>void address_space_unregister(AddressSpace *as)<BR>{<BR>&nbsp;&nbsp;&nbsp; memory_listener_unregister(&amp;as-&gt;dispatch_listener);<BR>}</P>
<P>void address_space_destroy_dispatch(AddressSpace *as)<BR>{<BR>&nbsp;&nbsp;&nbsp; AddressSpaceDispatch *d = as-&gt;dispatch;</P>
<P>&nbsp;&nbsp;&nbsp; atomic_rcu_set(&amp;as-&gt;dispatch, NULL);<BR>&nbsp;&nbsp;&nbsp; if (d) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; call_rcu(d, address_space_dispatch_free, rcu);<BR>&nbsp;&nbsp;&nbsp; }<BR>}</P>
<P>static void memory_map_init(void)<BR>{<BR>&nbsp;&nbsp;&nbsp; system_memory = g_malloc(sizeof(*system_memory));</P>
<P>&nbsp;&nbsp;&nbsp; memory_region_init(system_memory, NULL, "system", UINT64_MAX);<BR>&nbsp;&nbsp;&nbsp; address_space_init(&amp;address_space_memory, system_memory, "memory");</P>
<P>&nbsp;&nbsp;&nbsp; system_io = g_malloc(sizeof(*system_io));<BR>&nbsp;&nbsp;&nbsp; memory_region_init_io(system_io, NULL, &amp;unassigned_io_ops, NULL, "io",<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 65536);<BR>&nbsp;&nbsp;&nbsp; address_space_init(&amp;address_space_io, system_io, "I/O");<BR>}</P>
<P>MemoryRegion *get_system_memory(void)<BR>{<BR>&nbsp;&nbsp;&nbsp; return system_memory;<BR>}</P>
<P>MemoryRegion *get_system_io(void)<BR>{<BR>&nbsp;&nbsp;&nbsp; return system_io;<BR>}</P>
<P>#endif /* !defined(CONFIG_USER_ONLY) */</P>
<P>/* physical memory access (slow version, mainly for debug) */<BR>#if defined(CONFIG_USER_ONLY)<BR>int cpu_memory_rw_debug(CPUState *cpu, target_ulong addr,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; uint8_t *buf, int len, int is_write)<BR>{<BR>&nbsp;&nbsp;&nbsp; int l, flags;<BR>&nbsp;&nbsp;&nbsp; target_ulong page;<BR>&nbsp;&nbsp;&nbsp; void * p;</P>
<P>&nbsp;&nbsp;&nbsp; while (len &gt; 0) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; page = addr &amp; TARGET_PAGE_MASK;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; l = (page + TARGET_PAGE_SIZE) - addr;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (l &gt; len)<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; l = len;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; flags = page_get_flags(page);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (!(flags &amp; PAGE_VALID))<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return -1;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (is_write) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (!(flags &amp; PAGE_WRITE))<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return -1;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; /* XXX: this code should not depend on lock_user */<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (!(p = lock_user(VERIFY_WRITE, addr, l, 0)))<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return -1;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; memcpy(p, buf, l);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; unlock_user(p, addr, l);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; } else {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (!(flags &amp; PAGE_READ))<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return -1;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; /* XXX: this code should not depend on lock_user */<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (!(p = lock_user(VERIFY_READ, addr, l, 1)))<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return -1;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; memcpy(buf, p, l);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; unlock_user(p, addr, 0);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; len -= l;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; buf += l;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; addr += l;<BR>&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; return 0;<BR>}</P>
<P>#else</P>
<P>static void invalidate_and_set_dirty(MemoryRegion *mr, hwaddr addr,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; hwaddr length)<BR>{<BR>&nbsp;&nbsp;&nbsp; uint8_t dirty_log_mask = memory_region_get_dirty_log_mask(mr);<BR>&nbsp;&nbsp;&nbsp; addr += memory_region_get_ram_addr(mr);</P>
<P>&nbsp;&nbsp;&nbsp; /* No early return if dirty_log_mask is or becomes 0, because<BR>&nbsp;&nbsp;&nbsp;&nbsp; * cpu_physical_memory_set_dirty_range will still call<BR>&nbsp;&nbsp;&nbsp;&nbsp; * xen_modified_memory.<BR>&nbsp;&nbsp;&nbsp;&nbsp; */<BR>&nbsp;&nbsp;&nbsp; if (dirty_log_mask) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; dirty_log_mask =<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; cpu_physical_memory_range_includes_clean(addr, length, dirty_log_mask);<BR>&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; if (dirty_log_mask &amp; (1 &lt;&lt; DIRTY_MEMORY_CODE)) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; tb_lock();<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; tb_invalidate_phys_range(addr, addr + length);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; tb_unlock();<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; dirty_log_mask &amp;= ~(1 &lt;&lt; DIRTY_MEMORY_CODE);<BR>&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; cpu_physical_memory_set_dirty_range(addr, length, dirty_log_mask);<BR>}</P>
<P>static int memory_access_size(MemoryRegion *mr, unsigned l, hwaddr addr)<BR>{<BR>&nbsp;&nbsp;&nbsp; unsigned access_size_max = mr-&gt;ops-&gt;valid.max_access_size;</P>
<P>&nbsp;&nbsp;&nbsp; /* Regions are assumed to support 1-4 byte accesses unless<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; otherwise specified.&nbsp; */<BR>&nbsp;&nbsp;&nbsp; if (access_size_max == 0) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; access_size_max = 4;<BR>&nbsp;&nbsp;&nbsp; }</P>
<P>&nbsp;&nbsp;&nbsp; /* Bound the maximum access by the alignment of the address.&nbsp; */<BR>&nbsp;&nbsp;&nbsp; if (!mr-&gt;ops-&gt;impl.unaligned) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; unsigned align_size_max = addr &amp; -addr;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (align_size_max != 0 &amp;&amp; align_size_max &lt; access_size_max) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; access_size_max = align_size_max;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; }</P>
<P>&nbsp;&nbsp;&nbsp; /* Don't attempt accesses larger than the maximum.&nbsp; */<BR>&nbsp;&nbsp;&nbsp; if (l &gt; access_size_max) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; l = access_size_max;<BR>&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; l = pow2floor(l);</P>
<P>&nbsp;&nbsp;&nbsp; return l;<BR>}</P>
<P>static bool prepare_mmio_access(MemoryRegion *mr)<BR>{<BR>&nbsp;&nbsp;&nbsp; bool unlocked = !qemu_mutex_iothread_locked();<BR>&nbsp;&nbsp;&nbsp; bool release_lock = false;</P>
<P>&nbsp;&nbsp;&nbsp; if (unlocked &amp;&amp; mr-&gt;global_locking) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; qemu_mutex_lock_iothread();<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; unlocked = false;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; release_lock = true;<BR>&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; if (mr-&gt;flush_coalesced_mmio) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (unlocked) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; qemu_mutex_lock_iothread();<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; qemu_flush_coalesced_mmio_buffer();<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (unlocked) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; qemu_mutex_unlock_iothread();<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; }</P>
<P>&nbsp;&nbsp;&nbsp; return release_lock;<BR>}</P>
<P>/* Called within RCU critical section.&nbsp; */<BR>static MemTxResult address_space_write_continue(AddressSpace *as, hwaddr addr,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; MemTxAttrs attrs,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; const uint8_t *buf,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; int len, hwaddr addr1,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; hwaddr l, MemoryRegion *mr)<BR>{<BR>&nbsp;&nbsp;&nbsp; uint8_t *ptr;<BR>&nbsp;&nbsp;&nbsp; uint64_t val;<BR>&nbsp;&nbsp;&nbsp; MemTxResult result = MEMTX_OK;<BR>&nbsp;&nbsp;&nbsp; bool release_lock = false;</P>
<P>&nbsp;&nbsp;&nbsp; for (;;) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (!memory_access_is_direct(mr, true)) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; release_lock |= prepare_mmio_access(mr);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; l = memory_access_size(mr, l, addr1);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; /* XXX: could force current_cpu to NULL to avoid<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; potential bugs */<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; switch (l) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; case 8:<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; /* 64 bit write access */<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; val = ldq_p(buf);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; result |= memory_region_dispatch_write(mr, addr1, val, 8,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; attrs);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; break;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; case 4:<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; /* 32 bit write access */<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; val = ldl_p(buf);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; result |= memory_region_dispatch_write(mr, addr1, val, 4,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; attrs);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; break;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; case 2:<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; /* 16 bit write access */<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; val = lduw_p(buf);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; result |= memory_region_dispatch_write(mr, addr1, val, 2,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; attrs);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; break;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; case 1:<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; /* 8 bit write access */<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; val = ldub_p(buf);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; result |= memory_region_dispatch_write(mr, addr1, val, 1,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; attrs);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; break;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; default:<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; abort();<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; } else {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; /* RAM case */<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ptr = qemu_map_ram_ptr(mr-&gt;ram_block, addr1);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; memcpy(ptr, buf, l);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; invalidate_and_set_dirty(mr, addr1, l);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (release_lock) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; qemu_mutex_unlock_iothread();<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; release_lock = false;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; len -= l;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; buf += l;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; addr += l;</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (!len) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; break;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; l = len;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; mr = address_space_translate(as, addr, &amp;addr1, &amp;l, true);<BR>&nbsp;&nbsp;&nbsp; }</P>
<P>&nbsp;&nbsp;&nbsp; return result;<BR>}</P>
<P>MemTxResult address_space_write(AddressSpace *as, hwaddr addr, MemTxAttrs attrs,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; const uint8_t *buf, int len)<BR>{<BR>&nbsp;&nbsp;&nbsp; hwaddr l;<BR>&nbsp;&nbsp;&nbsp; hwaddr addr1;<BR>&nbsp;&nbsp;&nbsp; MemoryRegion *mr;<BR>&nbsp;&nbsp;&nbsp; MemTxResult result = MEMTX_OK;</P>
<P>&nbsp;&nbsp;&nbsp; if (len &gt; 0) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; rcu_read_lock();<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; l = len;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; mr = address_space_translate(as, addr, &amp;addr1, &amp;l, true);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; result = address_space_write_continue(as, addr, attrs, buf, len,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; addr1, l, mr);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; rcu_read_unlock();<BR>&nbsp;&nbsp;&nbsp; }</P>
<P>&nbsp;&nbsp;&nbsp; return result;<BR>}</P>
<P>/* Called within RCU critical section.&nbsp; */<BR>MemTxResult address_space_read_continue(AddressSpace *as, hwaddr addr,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; MemTxAttrs attrs, uint8_t *buf,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; int len, hwaddr addr1, hwaddr l,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; MemoryRegion *mr)<BR>{<BR>&nbsp;&nbsp;&nbsp; uint8_t *ptr;<BR>&nbsp;&nbsp;&nbsp; uint64_t val;<BR>&nbsp;&nbsp;&nbsp; MemTxResult result = MEMTX_OK;<BR>&nbsp;&nbsp;&nbsp; bool release_lock = false;</P>
<P>&nbsp;&nbsp;&nbsp; for (;;) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (!memory_access_is_direct(mr, false)) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; /* I/O case */<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; release_lock |= prepare_mmio_access(mr);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; l = memory_access_size(mr, l, addr1);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; switch (l) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; case 8:<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; /* 64 bit read access */<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; result |= memory_region_dispatch_read(mr, addr1, &amp;val, 8,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; attrs);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; stq_p(buf, val);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; break;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; case 4:<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; /* 32 bit read access */<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; result |= memory_region_dispatch_read(mr, addr1, &amp;val, 4,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; attrs);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; stl_p(buf, val);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; break;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; case 2:<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; /* 16 bit read access */<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; result |= memory_region_dispatch_read(mr, addr1, &amp;val, 2,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; attrs);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; stw_p(buf, val);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; break;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; case 1:<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; /* 8 bit read access */<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; result |= memory_region_dispatch_read(mr, addr1, &amp;val, 1,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; attrs);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; stb_p(buf, val);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; break;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; default:<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; abort();<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; } else {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; /* RAM case */<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ptr = qemu_map_ram_ptr(mr-&gt;ram_block, addr1);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; memcpy(buf, ptr, l);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (release_lock) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; qemu_mutex_unlock_iothread();<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; release_lock = false;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; len -= l;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; buf += l;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; addr += l;</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (!len) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; break;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; l = len;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; mr = address_space_translate(as, addr, &amp;addr1, &amp;l, false);<BR>&nbsp;&nbsp;&nbsp; }</P>
<P>&nbsp;&nbsp;&nbsp; return result;<BR>}</P>
<P>MemTxResult address_space_read_full(AddressSpace *as, hwaddr addr,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; MemTxAttrs attrs, uint8_t *buf, int len)<BR>{<BR>&nbsp;&nbsp;&nbsp; hwaddr l;<BR>&nbsp;&nbsp;&nbsp; hwaddr addr1;<BR>&nbsp;&nbsp;&nbsp; MemoryRegion *mr;<BR>&nbsp;&nbsp;&nbsp; MemTxResult result = MEMTX_OK;</P>
<P>&nbsp;&nbsp;&nbsp; if (len &gt; 0) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; rcu_read_lock();<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; l = len;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; mr = address_space_translate(as, addr, &amp;addr1, &amp;l, false);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; result = address_space_read_continue(as, addr, attrs, buf, len,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; addr1, l, mr);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; rcu_read_unlock();<BR>&nbsp;&nbsp;&nbsp; }</P>
<P>&nbsp;&nbsp;&nbsp; return result;<BR>}</P>
<P>MemTxResult address_space_rw(AddressSpace *as, hwaddr addr, MemTxAttrs attrs,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; uint8_t *buf, int len, bool is_write)<BR>{<BR>&nbsp;&nbsp;&nbsp; if (is_write) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return address_space_write(as, addr, attrs, (uint8_t *)buf, len);<BR>&nbsp;&nbsp;&nbsp; } else {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return address_space_read(as, addr, attrs, (uint8_t *)buf, len);<BR>&nbsp;&nbsp;&nbsp; }<BR>}</P>
<P>void cpu_physical_memory_rw(hwaddr addr, uint8_t *buf,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; int len, int is_write)<BR>{<BR>&nbsp;&nbsp;&nbsp; address_space_rw(&amp;address_space_memory, addr, MEMTXATTRS_UNSPECIFIED,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; buf, len, is_write);<BR>}</P>
<P>enum write_rom_type {<BR>&nbsp;&nbsp;&nbsp; WRITE_DATA,<BR>&nbsp;&nbsp;&nbsp; FLUSH_CACHE,<BR>};</P>
<P>static inline void cpu_physical_memory_write_rom_internal(AddressSpace *as,<BR>&nbsp;&nbsp;&nbsp; hwaddr addr, const uint8_t *buf, int len, enum write_rom_type type)<BR>{<BR>&nbsp;&nbsp;&nbsp; hwaddr l;<BR>&nbsp;&nbsp;&nbsp; uint8_t *ptr;<BR>&nbsp;&nbsp;&nbsp; hwaddr addr1;<BR>&nbsp;&nbsp;&nbsp; MemoryRegion *mr;</P>
<P>&nbsp;&nbsp;&nbsp; rcu_read_lock();<BR>&nbsp;&nbsp;&nbsp; while (len &gt; 0) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; l = len;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; mr = address_space_translate(as, addr, &amp;addr1, &amp;l, true);</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (!(memory_region_is_ram(mr) ||<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; memory_region_is_romd(mr))) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; l = memory_access_size(mr, l, addr1);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; } else {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; /* ROM/RAM case */<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ptr = qemu_map_ram_ptr(mr-&gt;ram_block, addr1);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; switch (type) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; case WRITE_DATA:<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; memcpy(ptr, buf, l);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; invalidate_and_set_dirty(mr, addr1, l);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; break;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; case FLUSH_CACHE:<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; flush_icache_range((uintptr_t)ptr, (uintptr_t)ptr + l);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; break;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; len -= l;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; buf += l;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; addr += l;<BR>&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; rcu_read_unlock();<BR>}</P>
<P>/* used for ROM loading : can write in RAM and ROM */<BR>void cpu_physical_memory_write_rom(AddressSpace *as, hwaddr addr,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; const uint8_t *buf, int len)<BR>{<BR>&nbsp;&nbsp;&nbsp; cpu_physical_memory_write_rom_internal(as, addr, buf, len, WRITE_DATA);<BR>}</P>
<P>void cpu_flush_icache_range(hwaddr start, int len)<BR>{<BR>&nbsp;&nbsp;&nbsp; /*<BR>&nbsp;&nbsp;&nbsp;&nbsp; * This function should do the same thing as an icache flush that was<BR>&nbsp;&nbsp;&nbsp;&nbsp; * triggered from within the guest. For TCG we are always cache coherent,<BR>&nbsp;&nbsp;&nbsp;&nbsp; * so there is no need to flush anything. For KVM / Xen we need to flush<BR>&nbsp;&nbsp;&nbsp;&nbsp; * the host's instruction cache at least.<BR>&nbsp;&nbsp;&nbsp;&nbsp; */<BR>&nbsp;&nbsp;&nbsp; if (tcg_enabled()) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return;<BR>&nbsp;&nbsp;&nbsp; }</P>
<P>&nbsp;&nbsp;&nbsp; cpu_physical_memory_write_rom_internal(&amp;address_space_memory,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; start, NULL, len, FLUSH_CACHE);<BR>}</P>
<P>typedef struct {<BR>&nbsp;&nbsp;&nbsp; MemoryRegion *mr;<BR>&nbsp;&nbsp;&nbsp; void *buffer;<BR>&nbsp;&nbsp;&nbsp; hwaddr addr;<BR>&nbsp;&nbsp;&nbsp; hwaddr len;<BR>&nbsp;&nbsp;&nbsp; bool in_use;<BR>} BounceBuffer;</P>
<P>static BounceBuffer bounce;</P>
<P>typedef struct MapClient {<BR>&nbsp;&nbsp;&nbsp; QEMUBH *bh;<BR>&nbsp;&nbsp;&nbsp; QLIST_ENTRY(MapClient) link;<BR>} MapClient;</P>
<P>QemuMutex map_client_list_lock;<BR>static QLIST_HEAD(map_client_list, MapClient) map_client_list<BR>&nbsp;&nbsp;&nbsp; = QLIST_HEAD_INITIALIZER(map_client_list);</P>
<P>static void cpu_unregister_map_client_do(MapClient *client)<BR>{<BR>&nbsp;&nbsp;&nbsp; QLIST_REMOVE(client, link);<BR>&nbsp;&nbsp;&nbsp; g_free(client);<BR>}</P>
<P>static void cpu_notify_map_clients_locked(void)<BR>{<BR>&nbsp;&nbsp;&nbsp; MapClient *client;</P>
<P>&nbsp;&nbsp;&nbsp; while (!QLIST_EMPTY(&amp;map_client_list)) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; client = QLIST_FIRST(&amp;map_client_list);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; qemu_bh_schedule(client-&gt;bh);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; cpu_unregister_map_client_do(client);<BR>&nbsp;&nbsp;&nbsp; }<BR>}</P>
<P>void cpu_register_map_client(QEMUBH *bh)<BR>{<BR>&nbsp;&nbsp;&nbsp; MapClient *client = g_malloc(sizeof(*client));</P>
<P>&nbsp;&nbsp;&nbsp; qemu_mutex_lock(&amp;map_client_list_lock);<BR>&nbsp;&nbsp;&nbsp; client-&gt;bh = bh;<BR>&nbsp;&nbsp;&nbsp; QLIST_INSERT_HEAD(&amp;map_client_list, client, link);<BR>&nbsp;&nbsp;&nbsp; if (!atomic_read(&amp;bounce.in_use)) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; cpu_notify_map_clients_locked();<BR>&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; qemu_mutex_unlock(&amp;map_client_list_lock);<BR>}</P>
<P>void cpu_exec_init_all(void)<BR>{<BR>&nbsp;&nbsp;&nbsp; qemu_mutex_init(&amp;ram_list.mutex);<BR>&nbsp;&nbsp;&nbsp; /* The data structures we set up here depend on knowing the page size,<BR>&nbsp;&nbsp;&nbsp;&nbsp; * so no more changes can be made after this point.<BR>&nbsp;&nbsp;&nbsp;&nbsp; * In an ideal world, nothing we did before we had finished the<BR>&nbsp;&nbsp;&nbsp;&nbsp; * machine setup would care about the target page size, and we could<BR>&nbsp;&nbsp;&nbsp;&nbsp; * do this much later, rather than requiring board models to state<BR>&nbsp;&nbsp;&nbsp;&nbsp; * up front what their requirements are.<BR>&nbsp;&nbsp;&nbsp;&nbsp; */<BR>&nbsp;&nbsp;&nbsp; finalize_target_page_bits();<BR>&nbsp;&nbsp;&nbsp; io_mem_init();<BR>&nbsp;&nbsp;&nbsp; memory_map_init();<BR>&nbsp;&nbsp;&nbsp; qemu_mutex_init(&amp;map_client_list_lock);<BR>}</P>
<P>void cpu_unregister_map_client(QEMUBH *bh)<BR>{<BR>&nbsp;&nbsp;&nbsp; MapClient *client;</P>
<P>&nbsp;&nbsp;&nbsp; qemu_mutex_lock(&amp;map_client_list_lock);<BR>&nbsp;&nbsp;&nbsp; QLIST_FOREACH(client, &amp;map_client_list, link) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (client-&gt;bh == bh) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; cpu_unregister_map_client_do(client);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; break;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; qemu_mutex_unlock(&amp;map_client_list_lock);<BR>}</P>
<P>static void cpu_notify_map_clients(void)<BR>{<BR>&nbsp;&nbsp;&nbsp; qemu_mutex_lock(&amp;map_client_list_lock);<BR>&nbsp;&nbsp;&nbsp; cpu_notify_map_clients_locked();<BR>&nbsp;&nbsp;&nbsp; qemu_mutex_unlock(&amp;map_client_list_lock);<BR>}</P>
<P>bool address_space_access_valid(AddressSpace *as, hwaddr addr, int len, bool is_write)<BR>{<BR>&nbsp;&nbsp;&nbsp; MemoryRegion *mr;<BR>&nbsp;&nbsp;&nbsp; hwaddr l, xlat;</P>
<P>&nbsp;&nbsp;&nbsp; rcu_read_lock();<BR>&nbsp;&nbsp;&nbsp; while (len &gt; 0) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; l = len;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; mr = address_space_translate(as, addr, &amp;xlat, &amp;l, is_write);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (!memory_access_is_direct(mr, is_write)) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; l = memory_access_size(mr, l, addr);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (!memory_region_access_valid(mr, xlat, l, is_write)) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return false;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; len -= l;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; addr += l;<BR>&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; rcu_read_unlock();<BR>&nbsp;&nbsp;&nbsp; return true;<BR>}</P>
<P>static hwaddr<BR>address_space_extend_translation(AddressSpace *as, hwaddr addr, hwaddr target_len,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; MemoryRegion *mr, hwaddr base, hwaddr len,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; bool is_write)<BR>{<BR>&nbsp;&nbsp;&nbsp; hwaddr done = 0;<BR>&nbsp;&nbsp;&nbsp; hwaddr xlat;<BR>&nbsp;&nbsp;&nbsp; MemoryRegion *this_mr;</P>
<P>&nbsp;&nbsp;&nbsp; for (;;) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; target_len -= len;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; addr += len;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; done += len;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (target_len == 0) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return done;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; len = target_len;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; this_mr = address_space_translate(as, addr, &amp;xlat, &amp;len, is_write);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (this_mr != mr || xlat != base + done) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return done;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; }<BR>}</P>
<P>/* Map a physical memory region into a host virtual address.<BR>&nbsp;* May map a subset of the requested range, given by and returned in *plen.<BR>&nbsp;* May return NULL if resources needed to perform the mapping are exhausted.<BR>&nbsp;* Use only for reads OR writes - not for read-modify-write operations.<BR>&nbsp;* Use cpu_register_map_client() to know when retrying the map operation is<BR>&nbsp;* likely to succeed.<BR>&nbsp;*/<BR>void *address_space_map(AddressSpace *as,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; hwaddr addr,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; hwaddr *plen,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; bool is_write)<BR>{<BR>&nbsp;&nbsp;&nbsp; hwaddr len = *plen;<BR>&nbsp;&nbsp;&nbsp; hwaddr l, xlat;<BR>&nbsp;&nbsp;&nbsp; MemoryRegion *mr;<BR>&nbsp;&nbsp;&nbsp; void *ptr;</P>
<P>&nbsp;&nbsp;&nbsp; if (len == 0) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return NULL;<BR>&nbsp;&nbsp;&nbsp; }</P>
<P>&nbsp;&nbsp;&nbsp; l = len;<BR>&nbsp;&nbsp;&nbsp; rcu_read_lock();<BR>&nbsp;&nbsp;&nbsp; mr = address_space_translate(as, addr, &amp;xlat, &amp;l, is_write);</P>
<P>&nbsp;&nbsp;&nbsp; if (!memory_access_is_direct(mr, is_write)) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (atomic_xchg(&amp;bounce.in_use, true)) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; rcu_read_unlock();<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return NULL;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; /* Avoid unbounded allocations */<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; l = MIN(l, TARGET_PAGE_SIZE);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; bounce.buffer = qemu_memalign(TARGET_PAGE_SIZE, l);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; bounce.addr = addr;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; bounce.len = l;</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; memory_region_ref(mr);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; bounce.mr = mr;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (!is_write) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; address_space_read(as, addr, MEMTXATTRS_UNSPECIFIED,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; bounce.buffer, l);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; rcu_read_unlock();<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; *plen = l;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return bounce.buffer;<BR>&nbsp;&nbsp;&nbsp; }</P>
<P><BR>&nbsp;&nbsp;&nbsp; memory_region_ref(mr);<BR>&nbsp;&nbsp;&nbsp; *plen = address_space_extend_translation(as, addr, len, mr, xlat, l, is_write);<BR>&nbsp;&nbsp;&nbsp; ptr = qemu_ram_ptr_length(mr-&gt;ram_block, xlat, plen);<BR>&nbsp;&nbsp;&nbsp; rcu_read_unlock();</P>
<P>&nbsp;&nbsp;&nbsp; return ptr;<BR>}</P>
<P>/* Unmaps a memory region previously mapped by address_space_map().<BR>&nbsp;* Will also mark the memory as dirty if is_write == 1.&nbsp; access_len gives<BR>&nbsp;* the amount of memory that was actually read or written by the caller.<BR>&nbsp;*/<BR>void address_space_unmap(AddressSpace *as, void *buffer, hwaddr len,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; int is_write, hwaddr access_len)<BR>{<BR>&nbsp;&nbsp;&nbsp; if (buffer != bounce.buffer) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; MemoryRegion *mr;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ram_addr_t addr1;</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; mr = memory_region_from_host(buffer, &amp;addr1);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; assert(mr != NULL);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (is_write) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; invalidate_and_set_dirty(mr, addr1, access_len);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (xen_enabled()) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; xen_invalidate_map_cache_entry(buffer);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; memory_region_unref(mr);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return;<BR>&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; if (is_write) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; address_space_write(as, bounce.addr, MEMTXATTRS_UNSPECIFIED,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; bounce.buffer, access_len);<BR>&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; qemu_vfree(bounce.buffer);<BR>&nbsp;&nbsp;&nbsp; bounce.buffer = NULL;<BR>&nbsp;&nbsp;&nbsp; memory_region_unref(bounce.mr);<BR>&nbsp;&nbsp;&nbsp; atomic_mb_set(&amp;bounce.in_use, false);<BR>&nbsp;&nbsp;&nbsp; cpu_notify_map_clients();<BR>}</P>
<P>void *cpu_physical_memory_map(hwaddr addr,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; hwaddr *plen,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; int is_write)<BR>{<BR>&nbsp;&nbsp;&nbsp; return address_space_map(&amp;address_space_memory, addr, plen, is_write);<BR>}</P>
<P>void cpu_physical_memory_unmap(void *buffer, hwaddr len,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; int is_write, hwaddr access_len)<BR>{<BR>&nbsp;&nbsp;&nbsp; return address_space_unmap(&amp;address_space_memory, buffer, len, is_write, access_len);<BR>}</P>
<P><FONT class=extract>#define ARG1_DECL&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; AddressSpace *as<BR>#define ARG1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; as<BR>#define SUFFIX<BR>#define TRANSLATE(...)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; address_space_translate(as, __VA_ARGS__)<BR>#define IS_DIRECT(mr, is_write)&nbsp; memory_access_is_direct(mr, is_write)<BR>#define MAP_RAM(mr, ofs)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; qemu_map_ram_ptr((mr)-&gt;ram_block, ofs)<BR>#define INVALIDATE(mr, ofs, len) invalidate_and_set_dirty(mr, ofs, len)<BR>#define RCU_READ_LOCK(...)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; rcu_read_lock()<BR>#define RCU_READ_UNLOCK(...)&nbsp;&nbsp;&nbsp;&nbsp; rcu_read_unlock()<BR>#include "memory_ldst.inc.c"</FONT></P>
<P>int64_t address_space_cache_init(MemoryRegionCache *cache,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; AddressSpace *as,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; hwaddr addr,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; hwaddr len,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; bool is_write)<BR>{<BR>&nbsp;&nbsp;&nbsp; hwaddr l, xlat;<BR>&nbsp;&nbsp;&nbsp; MemoryRegion *mr;<BR>&nbsp;&nbsp;&nbsp; void *ptr;</P>
<P>&nbsp;&nbsp;&nbsp; assert(len &gt; 0);</P>
<P>&nbsp;&nbsp;&nbsp; l = len;<BR>&nbsp;&nbsp;&nbsp; mr = address_space_translate(as, addr, &amp;xlat, &amp;l, is_write);<BR>&nbsp;&nbsp;&nbsp; if (!memory_access_is_direct(mr, is_write)) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return -EINVAL;<BR>&nbsp;&nbsp;&nbsp; }</P>
<P>&nbsp;&nbsp;&nbsp; l = address_space_extend_translation(as, addr, len, mr, xlat, l, is_write);<BR>&nbsp;&nbsp;&nbsp; ptr = qemu_ram_ptr_length(mr-&gt;ram_block, xlat, &amp;l);</P>
<P>&nbsp;&nbsp;&nbsp; cache-&gt;xlat = xlat;<BR>&nbsp;&nbsp;&nbsp; cache-&gt;is_write = is_write;<BR>&nbsp;&nbsp;&nbsp; cache-&gt;mr = mr;<BR>&nbsp;&nbsp;&nbsp; cache-&gt;ptr = ptr;<BR>&nbsp;&nbsp;&nbsp; cache-&gt;len = l;<BR>&nbsp;&nbsp;&nbsp; memory_region_ref(cache-&gt;mr);</P>
<P>&nbsp;&nbsp;&nbsp; return l;<BR>}</P>
<P>void address_space_cache_invalidate(MemoryRegionCache *cache,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; hwaddr addr,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; hwaddr access_len)<BR>{<BR>&nbsp;&nbsp;&nbsp; assert(cache-&gt;is_write);<BR>&nbsp;&nbsp;&nbsp; invalidate_and_set_dirty(cache-&gt;mr, addr + cache-&gt;xlat, access_len);<BR>}</P>
<P>void address_space_cache_destroy(MemoryRegionCache *cache)<BR>{<BR>&nbsp;&nbsp;&nbsp; if (!cache-&gt;mr) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return;<BR>&nbsp;&nbsp;&nbsp; }</P>
<P>&nbsp;&nbsp;&nbsp; if (xen_enabled()) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; xen_invalidate_map_cache_entry(cache-&gt;ptr);<BR>&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; memory_region_unref(cache-&gt;mr);<BR>}</P>
<P>/* Called from RCU critical section.&nbsp; This function has the same<BR>&nbsp;* semantics as address_space_translate, but it only works on a<BR>&nbsp;* predefined range of a MemoryRegion that was mapped with<BR>&nbsp;* address_space_cache_init.<BR>&nbsp;*/<BR>static inline MemoryRegion *address_space_translate_cached(<BR>&nbsp;&nbsp;&nbsp; MemoryRegionCache *cache, hwaddr addr, hwaddr *xlat,<BR>&nbsp;&nbsp;&nbsp; hwaddr *plen, bool is_write)<BR>{<BR>&nbsp;&nbsp;&nbsp; assert(addr &lt; cache-&gt;len &amp;&amp; *plen &lt;= cache-&gt;len - addr);<BR>&nbsp;&nbsp;&nbsp; *xlat = addr + cache-&gt;xlat;<BR>&nbsp;&nbsp;&nbsp; return cache-&gt;mr;<BR>}</P>
<P><FONT class=extract>#define ARG1_DECL&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; MemoryRegionCache *cache<BR>#define ARG1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; cache<BR>#define SUFFIX&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; _cached<BR>#define TRANSLATE(...)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; address_space_translate_cached(cache, __VA_ARGS__)<BR>#define IS_DIRECT(mr, is_write)&nbsp; true<BR>#define MAP_RAM(mr, ofs)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (cache-&gt;ptr + (ofs - cache-&gt;xlat))<BR>#define INVALIDATE(mr, ofs, len) ((void)0)<BR>#define RCU_READ_LOCK()&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ((void)0)<BR>#define RCU_READ_UNLOCK()&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ((void)0)<BR>#include "memory_ldst.inc.c"</FONT></P>
<P>/* virtual memory access for debug (includes writing to ROM) */<BR>int cpu_memory_rw_debug(CPUState *cpu, target_ulong addr,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; uint8_t *buf, int len, int is_write)<BR>{<BR>&nbsp;&nbsp;&nbsp; int l;<BR>&nbsp;&nbsp;&nbsp; hwaddr phys_addr;<BR>&nbsp;&nbsp;&nbsp; target_ulong page;</P>
<P>&nbsp;&nbsp;&nbsp; while (len &gt; 0) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; int asidx;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; MemTxAttrs attrs;</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; page = addr &amp; TARGET_PAGE_MASK;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; phys_addr = cpu_get_phys_page_attrs_debug(cpu, page, &amp;attrs);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; asidx = cpu_asidx_from_attrs(cpu, attrs);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; /* if no physical page mapped, return an error */<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (phys_addr == -1)<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return -1;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; l = (page + TARGET_PAGE_SIZE) - addr;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (l &gt; len)<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; l = len;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; phys_addr += (addr &amp; ~TARGET_PAGE_MASK);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (is_write) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; cpu_physical_memory_write_rom(cpu-&gt;cpu_ases[asidx].as,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; phys_addr, buf, l);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; } else {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; address_space_rw(cpu-&gt;cpu_ases[asidx].as, phys_addr,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; MEMTXATTRS_UNSPECIFIED,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; buf, l, 0);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; len -= l;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; buf += l;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; addr += l;<BR>&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; return 0;<BR>}</P>
<P>/*<BR>&nbsp;* Allows code that needs to deal with migration bitmaps etc to still be built<BR>&nbsp;* target independent.<BR>&nbsp;*/<BR>size_t qemu_target_page_bits(void)<BR>{<BR>&nbsp;&nbsp;&nbsp; return TARGET_PAGE_BITS;<BR>}</P>
<P>#endif</P>
<P>/*<BR>&nbsp;* A helper function for the _utterly broken_ virtio device model to find out if<BR>&nbsp;* it's running on a big endian machine. Don't do this at home kids!<BR>&nbsp;*/<BR>bool target_words_bigendian(void);<BR>bool target_words_bigendian(void)<BR>{<BR>#if defined(TARGET_WORDS_BIGENDIAN)<BR>&nbsp;&nbsp;&nbsp; return true;<BR>#else<BR>&nbsp;&nbsp;&nbsp; return false;<BR>#endif<BR>}</P>
<P>#ifndef CONFIG_USER_ONLY<BR>bool cpu_physical_memory_is_io(hwaddr phys_addr)<BR>{<BR>&nbsp;&nbsp;&nbsp; MemoryRegion*mr;<BR>&nbsp;&nbsp;&nbsp; hwaddr l = 1;<BR>&nbsp;&nbsp;&nbsp; bool res;</P>
<P>&nbsp;&nbsp;&nbsp; rcu_read_lock();<BR>&nbsp;&nbsp;&nbsp; mr = address_space_translate(&amp;address_space_memory,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; phys_addr, &amp;phys_addr, &amp;l, false);</P>
<P>&nbsp;&nbsp;&nbsp; res = !(memory_region_is_ram(mr) || memory_region_is_romd(mr));<BR>&nbsp;&nbsp;&nbsp; rcu_read_unlock();<BR>&nbsp;&nbsp;&nbsp; return res;<BR>}</P>
<P>int qemu_ram_foreach_block(RAMBlockIterFunc func, void *opaque)<BR>{<BR>&nbsp;&nbsp;&nbsp; RAMBlock *block;<BR>&nbsp;&nbsp;&nbsp; int ret = 0;</P>
<P>&nbsp;&nbsp;&nbsp; rcu_read_lock();<BR>&nbsp;&nbsp;&nbsp; QLIST_FOREACH_RCU(block, &amp;ram_list.blocks, next) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ret = func(block-&gt;idstr, block-&gt;host, block-&gt;offset,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; block-&gt;used_length, opaque);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (ret) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; break;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; rcu_read_unlock();<BR>&nbsp;&nbsp;&nbsp; return ret;<BR>}<BR>#endif