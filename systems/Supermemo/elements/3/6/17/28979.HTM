/*<BR>&nbsp;*&nbsp; linux/kernel/timer.c<BR>&nbsp;*<BR>&nbsp;*&nbsp; Kernel internal timers<BR>&nbsp;*<BR>&nbsp;*&nbsp; Copyright (C) 1991, 1992&nbsp; Linus Torvalds<BR>&nbsp;*<BR>&nbsp;*&nbsp; 1997-01-28&nbsp; Modified by Finn Arne Gangstad to make timers scale better.<BR>&nbsp;*<BR>&nbsp;*&nbsp; 1997-09-10&nbsp; Updated NTP code according to technical memorandum Jan '96<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "A Kernel Model for Precision Timekeeping" by Dave Mills<BR>&nbsp;*&nbsp; 1998-12-24&nbsp; Fixed a xtime SMP race (we need the xtime_lock rw spinlock to<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; serialize accesses to xtime/lost_ticks).<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Copyright (C) 1998&nbsp; Andrea Arcangeli<BR>&nbsp;*&nbsp; 1999-03-10&nbsp; Improved NTP compatibility by Ulrich Windl<BR>&nbsp;*&nbsp; 2002-05-31&nbsp;Move sys_sysinfo here and make its locking sane, Robert Love<BR>&nbsp;*&nbsp; 2000-10-05&nbsp; Implemented scalable SMP per-CPU timer handling.<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Copyright (C) 2000, 2001, 2002&nbsp; Ingo Molnar<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Designed by David S. Miller, Alexey Kuznetsov and Ingo Molnar<BR>&nbsp;*/ 
<P></P>
<P>#include &lt;linux/kernel_stat.h&gt;<BR>#include &lt;linux/export.h&gt;<BR>#include &lt;linux/interrupt.h&gt;<BR>#include &lt;linux/percpu.h&gt;<BR>#include &lt;linux/init.h&gt;<BR>#include &lt;linux/mm.h&gt;<BR>#include &lt;linux/swap.h&gt;<BR>#include &lt;linux/pid_namespace.h&gt;<BR>#include &lt;linux/notifier.h&gt;<BR>#include &lt;linux/thread_info.h&gt;<BR>#include &lt;linux/time.h&gt;<BR>#include &lt;linux/jiffies.h&gt;<BR>#include &lt;linux/posix-timers.h&gt;<BR>#include &lt;linux/cpu.h&gt;<BR>#include &lt;linux/syscalls.h&gt;<BR>#include &lt;linux/delay.h&gt;<BR>#include &lt;linux/tick.h&gt;<BR>#include &lt;linux/kallsyms.h&gt;<BR>#include &lt;linux/irq_work.h&gt;<BR>#include &lt;linux/sched.h&gt;<BR>#include &lt;linux/sched/sysctl.h&gt;<BR>#include &lt;linux/slab.h&gt;<BR>#include &lt;linux/compat.h&gt;</P>
<P>#include &lt;asm/uaccess.h&gt;<BR>#include &lt;asm/unistd.h&gt;<BR>#include &lt;asm/div64.h&gt;<BR>#include &lt;asm/timex.h&gt;<BR>#include &lt;asm/io.h&gt;</P>
<P>#include "tick-internal.h"</P>
<P>#define CREATE_TRACE_POINTS<BR>#include &lt;trace/events/timer.h&gt;</P>
<P>__visible u64 jiffies_64 __cacheline_aligned_in_smp = INITIAL_JIFFIES;</P>
<P>EXPORT_SYMBOL(jiffies_64);</P>
<P>/*<BR>&nbsp;* per-CPU timer vector definitions:<BR>&nbsp;*/<BR>#define TVN_BITS (CONFIG_BASE_SMALL ? 4 : 6)<BR>#define TVR_BITS (CONFIG_BASE_SMALL ? 6 : 8)<BR>#define TVN_SIZE (1 &lt;&lt; TVN_BITS)<BR>#define TVR_SIZE (1 &lt;&lt; TVR_BITS)<BR>#define TVN_MASK (TVN_SIZE - 1)<BR>#define TVR_MASK (TVR_SIZE - 1)<BR>#define MAX_TVAL ((unsigned long)((1ULL &lt;&lt; (TVR_BITS + 4*TVN_BITS)) - 1))</P>
<P>struct tvec {<BR>&nbsp;struct hlist_head vec[TVN_SIZE];<BR>};</P>
<P>struct tvec_root {<BR>&nbsp;struct hlist_head vec[TVR_SIZE];<BR>};</P>
<P>struct tvec_base {<BR>&nbsp;spinlock_t lock;<BR>&nbsp;struct timer_list *running_timer;<BR>&nbsp;unsigned long timer_jiffies;<BR>&nbsp;unsigned long next_timer;<BR>&nbsp;unsigned long active_timers;<BR>&nbsp;unsigned long all_timers;<BR>&nbsp;int cpu;<BR>&nbsp;bool migration_enabled;<BR>&nbsp;bool nohz_active;<BR>&nbsp;struct tvec_root tv1;<BR>&nbsp;struct tvec tv2;<BR>&nbsp;struct tvec tv3;<BR>&nbsp;struct tvec tv4;<BR>&nbsp;struct tvec tv5;<BR>} ____cacheline_aligned;</P>
<P><BR>static DEFINE_PER_CPU(struct tvec_base, tvec_bases);</P>
<P>#if defined(CONFIG_SMP) &amp;&amp; defined(CONFIG_NO_HZ_COMMON)<BR>unsigned int sysctl_timer_migration = 1;</P>
<P>void timers_update_migration(bool update_nohz)<BR>{<BR>&nbsp;bool on = sysctl_timer_migration &amp;&amp; tick_nohz_active;<BR>&nbsp;unsigned int cpu;</P>
<P>&nbsp;/* Avoid the loop, if nothing to update */<BR>&nbsp;if (this_cpu_read(tvec_bases.migration_enabled) == on)<BR>&nbsp;&nbsp;return;</P>
<P>&nbsp;for_each_possible_cpu(cpu) {<BR>&nbsp;&nbsp;per_cpu(tvec_bases.migration_enabled, cpu) = on;<BR>&nbsp;&nbsp;per_cpu(hrtimer_bases.migration_enabled, cpu) = on;<BR>&nbsp;&nbsp;if (!update_nohz)<BR>&nbsp;&nbsp;&nbsp;continue;<BR>&nbsp;&nbsp;per_cpu(tvec_bases.nohz_active, cpu) = true;<BR>&nbsp;&nbsp;per_cpu(hrtimer_bases.nohz_active, cpu) = true;<BR>&nbsp;}<BR>}</P>
<P>int timer_migration_handler(struct ctl_table *table, int write,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; void __user *buffer, size_t *lenp,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; loff_t *ppos)<BR>{<BR>&nbsp;static DEFINE_MUTEX(mutex);<BR>&nbsp;int ret;</P>
<P>&nbsp;mutex_lock(&amp;mutex);<BR>&nbsp;ret = proc_dointvec(table, write, buffer, lenp, ppos);<BR>&nbsp;if (!ret &amp;&amp; write)<BR>&nbsp;&nbsp;timers_update_migration(false);<BR>&nbsp;mutex_unlock(&amp;mutex);<BR>&nbsp;return ret;<BR>}</P>
<P>static inline struct tvec_base *get_target_base(struct tvec_base *base,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;int pinned)<BR>{<BR>&nbsp;if (pinned || !base-&gt;migration_enabled)<BR>&nbsp;&nbsp;return this_cpu_ptr(&amp;tvec_bases);<BR>&nbsp;return per_cpu_ptr(&amp;tvec_bases, get_nohz_timer_target());<BR>}<BR>#else<BR>static inline struct tvec_base *get_target_base(struct tvec_base *base,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;int pinned)<BR>{<BR>&nbsp;return this_cpu_ptr(&amp;tvec_bases);<BR>}<BR>#endif</P>
<P>static unsigned long round_jiffies_common(unsigned long j, int cpu,<BR>&nbsp;&nbsp;bool force_up)<BR>{<BR>&nbsp;int rem;<BR>&nbsp;unsigned long original = j;</P>
<P>&nbsp;/*<BR>&nbsp; * We don't want all cpus firing their timers at once hitting the<BR>&nbsp; * same lock or cachelines, so we skew each extra cpu with an extra<BR>&nbsp; * 3 jiffies. This 3 jiffies came originally from the mm/ code which<BR>&nbsp; * already did this.<BR>&nbsp; * The skew is done by adding 3*cpunr, then round, then subtract this<BR>&nbsp; * extra offset again.<BR>&nbsp; */<BR>&nbsp;j += cpu * 3;</P>
<P>&nbsp;rem = j % HZ;</P>
<P>&nbsp;/*<BR>&nbsp; * If the target jiffie is just after a whole second (which can happen<BR>&nbsp; * due to delays of the timer irq, long irq off times etc etc) then<BR>&nbsp; * we should round down to the whole second, not up. Use 1/4th second<BR>&nbsp; * as cutoff for this rounding as an extreme upper bound for this.<BR>&nbsp; * But never round down if @force_up is set.<BR>&nbsp; */<BR>&nbsp;if (rem &lt; HZ/4 &amp;&amp; !force_up) /* round down */<BR>&nbsp;&nbsp;j = j - rem;<BR>&nbsp;else /* round up */<BR>&nbsp;&nbsp;j = j - rem + HZ;</P>
<P>&nbsp;/* now that we have rounded, subtract the extra skew again */<BR>&nbsp;j -= cpu * 3;</P>
<P>&nbsp;/*<BR>&nbsp; * Make sure j is still in the future. Otherwise return the<BR>&nbsp; * unmodified value.<BR>&nbsp; */<BR>&nbsp;return time_is_after_jiffies(j) ? j : original;<BR>}</P>
<P>/**<BR>&nbsp;* __round_jiffies - function to round jiffies to a full second<BR>&nbsp;* @j: the time in (absolute) jiffies that should be rounded<BR>&nbsp;* @cpu: the processor number on which the timeout will happen<BR>&nbsp;*<BR>&nbsp;* __round_jiffies() rounds an absolute time in the future (in jiffies)<BR>&nbsp;* up or down to (approximately) full seconds. This is useful for timers<BR>&nbsp;* for which the exact time they fire does not matter too much, as long as<BR>&nbsp;* they fire approximately every X seconds.<BR>&nbsp;*<BR>&nbsp;* By rounding these timers to whole seconds, all such timers will fire<BR>&nbsp;* at the same time, rather than at various times spread out. The goal<BR>&nbsp;* of this is to have the CPU wake up less, which saves power.<BR>&nbsp;*<BR>&nbsp;* The exact rounding is skewed for each processor to avoid all<BR>&nbsp;* processors firing at the exact same time, which could lead<BR>&nbsp;* to lock contention or spurious cache line bouncing.<BR>&nbsp;*<BR>&nbsp;* The return value is the rounded version of the @j parameter.<BR>&nbsp;*/<BR>unsigned long __round_jiffies(unsigned long j, int cpu)<BR>{<BR>&nbsp;return round_jiffies_common(j, cpu, false);<BR>}<BR>EXPORT_SYMBOL_GPL(__round_jiffies);</P>
<P>/**<BR>&nbsp;* __round_jiffies_relative - function to round jiffies to a full second<BR>&nbsp;* @j: the time in (relative) jiffies that should be rounded<BR>&nbsp;* @cpu: the processor number on which the timeout will happen<BR>&nbsp;*<BR>&nbsp;* __round_jiffies_relative() rounds a time delta&nbsp; in the future (in jiffies)<BR>&nbsp;* up or down to (approximately) full seconds. This is useful for timers<BR>&nbsp;* for which the exact time they fire does not matter too much, as long as<BR>&nbsp;* they fire approximately every X seconds.<BR>&nbsp;*<BR>&nbsp;* By rounding these timers to whole seconds, all such timers will fire<BR>&nbsp;* at the same time, rather than at various times spread out. The goal<BR>&nbsp;* of this is to have the CPU wake up less, which saves power.<BR>&nbsp;*<BR>&nbsp;* The exact rounding is skewed for each processor to avoid all<BR>&nbsp;* processors firing at the exact same time, which could lead<BR>&nbsp;* to lock contention or spurious cache line bouncing.<BR>&nbsp;*<BR>&nbsp;* The return value is the rounded version of the @j parameter.<BR>&nbsp;*/<BR>unsigned long __round_jiffies_relative(unsigned long j, int cpu)<BR>{<BR>&nbsp;unsigned long j0 = jiffies;</P>
<P>&nbsp;/* Use j0 because jiffies might change while we run */<BR>&nbsp;return round_jiffies_common(j + j0, cpu, false) - j0;<BR>}<BR>EXPORT_SYMBOL_GPL(__round_jiffies_relative);</P>
<P>/**<BR>&nbsp;* round_jiffies - function to round jiffies to a full second<BR>&nbsp;* @j: the time in (absolute) jiffies that should be rounded<BR>&nbsp;*<BR>&nbsp;* round_jiffies() rounds an absolute time in the future (in jiffies)<BR>&nbsp;* up or down to (approximately) full seconds. This is useful for timers<BR>&nbsp;* for which the exact time they fire does not matter too much, as long as<BR>&nbsp;* they fire approximately every X seconds.<BR>&nbsp;*<BR>&nbsp;* By rounding these timers to whole seconds, all such timers will fire<BR>&nbsp;* at the same time, rather than at various times spread out. The goal<BR>&nbsp;* of this is to have the CPU wake up less, which saves power.<BR>&nbsp;*<BR>&nbsp;* The return value is the rounded version of the @j parameter.<BR>&nbsp;*/<BR>unsigned long round_jiffies(unsigned long j)<BR>{<BR>&nbsp;return round_jiffies_common(j, raw_smp_processor_id(), false);<BR>}<BR>EXPORT_SYMBOL_GPL(round_jiffies);</P>
<P>/**<BR>&nbsp;* round_jiffies_relative - function to round jiffies to a full second<BR>&nbsp;* @j: the time in (relative) jiffies that should be rounded<BR>&nbsp;*<BR>&nbsp;* round_jiffies_relative() rounds a time delta&nbsp; in the future (in jiffies)<BR>&nbsp;* up or down to (approximately) full seconds. This is useful for timers<BR>&nbsp;* for which the exact time they fire does not matter too much, as long as<BR>&nbsp;* they fire approximately every X seconds.<BR>&nbsp;*<BR>&nbsp;* By rounding these timers to whole seconds, all such timers will fire<BR>&nbsp;* at the same time, rather than at various times spread out. The goal<BR>&nbsp;* of this is to have the CPU wake up less, which saves power.<BR>&nbsp;*<BR>&nbsp;* The return value is the rounded version of the @j parameter.<BR>&nbsp;*/<BR>unsigned long round_jiffies_relative(unsigned long j)<BR>{<BR>&nbsp;return __round_jiffies_relative(j, raw_smp_processor_id());<BR>}<BR>EXPORT_SYMBOL_GPL(round_jiffies_relative);</P>
<P>/**<BR>&nbsp;* __round_jiffies_up - function to round jiffies up to a full second<BR>&nbsp;* @j: the time in (absolute) jiffies that should be rounded<BR>&nbsp;* @cpu: the processor number on which the timeout will happen<BR>&nbsp;*<BR>&nbsp;* This is the same as __round_jiffies() except that it will never<BR>&nbsp;* round down.&nbsp; This is useful for timeouts for which the exact time<BR>&nbsp;* of firing does not matter too much, as long as they don't fire too<BR>&nbsp;* early.<BR>&nbsp;*/<BR>unsigned long __round_jiffies_up(unsigned long j, int cpu)<BR>{<BR>&nbsp;return round_jiffies_common(j, cpu, true);<BR>}<BR>EXPORT_SYMBOL_GPL(__round_jiffies_up);</P>
<P>/**<BR>&nbsp;* __round_jiffies_up_relative - function to round jiffies up to a full second<BR>&nbsp;* @j: the time in (relative) jiffies that should be rounded<BR>&nbsp;* @cpu: the processor number on which the timeout will happen<BR>&nbsp;*<BR>&nbsp;* This is the same as __round_jiffies_relative() except that it will never<BR>&nbsp;* round down.&nbsp; This is useful for timeouts for which the exact time<BR>&nbsp;* of firing does not matter too much, as long as they don't fire too<BR>&nbsp;* early.<BR>&nbsp;*/<BR>unsigned long __round_jiffies_up_relative(unsigned long j, int cpu)<BR>{<BR>&nbsp;unsigned long j0 = jiffies;</P>
<P>&nbsp;/* Use j0 because jiffies might change while we run */<BR>&nbsp;return round_jiffies_common(j + j0, cpu, true) - j0;<BR>}<BR>EXPORT_SYMBOL_GPL(__round_jiffies_up_relative);</P>
<P>/**<BR>&nbsp;* round_jiffies_up - function to round jiffies up to a full second<BR>&nbsp;* @j: the time in (absolute) jiffies that should be rounded<BR>&nbsp;*<BR>&nbsp;* This is the same as round_jiffies() except that it will never<BR>&nbsp;* round down.&nbsp; This is useful for timeouts for which the exact time<BR>&nbsp;* of firing does not matter too much, as long as they don't fire too<BR>&nbsp;* early.<BR>&nbsp;*/<BR>unsigned long round_jiffies_up(unsigned long j)<BR>{<BR>&nbsp;return round_jiffies_common(j, raw_smp_processor_id(), true);<BR>}<BR>EXPORT_SYMBOL_GPL(round_jiffies_up);</P>
<P>/**<BR>&nbsp;* round_jiffies_up_relative - function to round jiffies up to a full second<BR>&nbsp;* @j: the time in (relative) jiffies that should be rounded<BR>&nbsp;*<BR>&nbsp;* This is the same as round_jiffies_relative() except that it will never<BR>&nbsp;* round down.&nbsp; This is useful for timeouts for which the exact time<BR>&nbsp;* of firing does not matter too much, as long as they don't fire too<BR>&nbsp;* early.<BR>&nbsp;*/<BR>unsigned long round_jiffies_up_relative(unsigned long j)<BR>{<BR>&nbsp;return __round_jiffies_up_relative(j, raw_smp_processor_id());<BR>}<BR>EXPORT_SYMBOL_GPL(round_jiffies_up_relative);</P>
<P>/**<BR>&nbsp;* set_timer_slack - set the allowed slack for a timer<BR>&nbsp;* @timer: the timer to be modified<BR>&nbsp;* @slack_hz: the amount of time (in jiffies) allowed for rounding<BR>&nbsp;*<BR>&nbsp;* Set the amount of time, in jiffies, that a certain timer has<BR>&nbsp;* in terms of slack. By setting this value, the timer subsystem<BR>&nbsp;* will schedule the actual timer somewhere between<BR>&nbsp;* the time mod_timer() asks for, and that time plus the slack.<BR>&nbsp;*<BR>&nbsp;* By setting the slack to -1, a percentage of the delay is used<BR>&nbsp;* instead.<BR>&nbsp;*/<BR>void set_timer_slack(struct timer_list *timer, int slack_hz)<BR>{<BR>&nbsp;timer-&gt;slack = slack_hz;<BR>}<BR>EXPORT_SYMBOL_GPL(set_timer_slack);</P>
<P>static void<BR>__internal_add_timer(struct tvec_base *base, struct timer_list *timer)<BR>{<BR>&nbsp;unsigned long expires = timer-&gt;expires;<BR>&nbsp;unsigned long idx = expires - base-&gt;timer_jiffies;<BR>&nbsp;struct hlist_head *vec;</P>
<P>&nbsp;if (idx &lt; TVR_SIZE) {<BR>&nbsp;&nbsp;int i = expires &amp; TVR_MASK;<BR>&nbsp;&nbsp;vec = base-&gt;tv1.vec + i;<BR>&nbsp;} else if (idx &lt; 1 &lt;&lt; (TVR_BITS + TVN_BITS)) {<BR>&nbsp;&nbsp;int i = (expires &gt;&gt; TVR_BITS) &amp; TVN_MASK;<BR>&nbsp;&nbsp;vec = base-&gt;tv2.vec + i;<BR>&nbsp;} else if (idx &lt; 1 &lt;&lt; (TVR_BITS + 2 * TVN_BITS)) {<BR>&nbsp;&nbsp;int i = (expires &gt;&gt; (TVR_BITS + TVN_BITS)) &amp; TVN_MASK;<BR>&nbsp;&nbsp;vec = base-&gt;tv3.vec + i;<BR>&nbsp;} else if (idx &lt; 1 &lt;&lt; (TVR_BITS + 3 * TVN_BITS)) {<BR>&nbsp;&nbsp;int i = (expires &gt;&gt; (TVR_BITS + 2 * TVN_BITS)) &amp; TVN_MASK;<BR>&nbsp;&nbsp;vec = base-&gt;tv4.vec + i;<BR>&nbsp;} else if ((signed long) idx &lt; 0) {<BR>&nbsp;&nbsp;/*<BR>&nbsp;&nbsp; * Can happen if you add a timer with expires == jiffies,<BR>&nbsp;&nbsp; * or you set a timer to go off in the past<BR>&nbsp;&nbsp; */<BR>&nbsp;&nbsp;vec = base-&gt;tv1.vec + (base-&gt;timer_jiffies &amp; TVR_MASK);<BR>&nbsp;} else {<BR>&nbsp;&nbsp;int i;<BR>&nbsp;&nbsp;/* If the timeout is larger than MAX_TVAL (on 64-bit<BR>&nbsp;&nbsp; * architectures or with CONFIG_BASE_SMALL=1) then we<BR>&nbsp;&nbsp; * use the maximum timeout.<BR>&nbsp;&nbsp; */<BR>&nbsp;&nbsp;if (idx &gt; MAX_TVAL) {<BR>&nbsp;&nbsp;&nbsp;idx = MAX_TVAL;<BR>&nbsp;&nbsp;&nbsp;expires = idx + base-&gt;timer_jiffies;<BR>&nbsp;&nbsp;}<BR>&nbsp;&nbsp;i = (expires &gt;&gt; (TVR_BITS + 3 * TVN_BITS)) &amp; TVN_MASK;<BR>&nbsp;&nbsp;vec = base-&gt;tv5.vec + i;<BR>&nbsp;}</P>
<P>&nbsp;hlist_add_head(&amp;timer-&gt;entry, vec);<BR>}</P>
<P>static void internal_add_timer(struct tvec_base *base, struct timer_list *timer)<BR>{<BR>&nbsp;/* Advance base-&gt;jiffies, if the base is empty */<BR>&nbsp;if (!base-&gt;all_timers++)<BR>&nbsp;&nbsp;base-&gt;timer_jiffies = jiffies;</P>
<P>&nbsp;__internal_add_timer(base, timer);<BR>&nbsp;/*<BR>&nbsp; * Update base-&gt;active_timers and base-&gt;next_timer<BR>&nbsp; */<BR>&nbsp;if (!(timer-&gt;flags &amp; TIMER_DEFERRABLE)) {<BR>&nbsp;&nbsp;if (!base-&gt;active_timers++ ||<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; time_before(timer-&gt;expires, base-&gt;next_timer))<BR>&nbsp;&nbsp;&nbsp;base-&gt;next_timer = timer-&gt;expires;<BR>&nbsp;}</P>
<P>&nbsp;/*<BR>&nbsp; * Check whether the other CPU is in dynticks mode and needs<BR>&nbsp; * to be triggered to reevaluate the timer wheel.<BR>&nbsp; * We are protected against the other CPU fiddling<BR>&nbsp; * with the timer by holding the timer base lock. This also<BR>&nbsp; * makes sure that a CPU on the way to stop its tick can not<BR>&nbsp; * evaluate the timer wheel.<BR>&nbsp; *<BR>&nbsp; * Spare the IPI for deferrable timers on idle targets though.<BR>&nbsp; * The next busy ticks will take care of it. Except full dynticks<BR>&nbsp; * require special care against races with idle_cpu(), lets deal<BR>&nbsp; * with that later.<BR>&nbsp; */<BR>&nbsp;if (base-&gt;nohz_active) {<BR>&nbsp;&nbsp;if (!(timer-&gt;flags &amp; TIMER_DEFERRABLE) ||<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; tick_nohz_full_cpu(base-&gt;cpu))<BR>&nbsp;&nbsp;&nbsp;wake_up_nohz_cpu(base-&gt;cpu);<BR>&nbsp;}<BR>}</P>
<P>#ifdef CONFIG_TIMER_STATS<BR>void __timer_stats_timer_set_start_info(struct timer_list *timer, void *addr)<BR>{<BR>&nbsp;if (timer-&gt;start_site)<BR>&nbsp;&nbsp;return;</P>
<P>&nbsp;timer-&gt;start_site = addr;<BR>&nbsp;memcpy(timer-&gt;start_comm, current-&gt;comm, TASK_COMM_LEN);<BR>&nbsp;timer-&gt;start_pid = current-&gt;pid;<BR>}</P>
<P>static void timer_stats_account_timer(struct timer_list *timer)<BR>{<BR>&nbsp;void *site;</P>
<P>&nbsp;/*<BR>&nbsp; * start_site can be concurrently reset by<BR>&nbsp; * timer_stats_timer_clear_start_info()<BR>&nbsp; */<BR>&nbsp;site = READ_ONCE(timer-&gt;start_site);<BR>&nbsp;if (likely(!site))<BR>&nbsp;&nbsp;return;</P>
<P>&nbsp;timer_stats_update_stats(timer, timer-&gt;start_pid, site,<BR>&nbsp;&nbsp;&nbsp;&nbsp; timer-&gt;function, timer-&gt;start_comm,<BR>&nbsp;&nbsp;&nbsp;&nbsp; timer-&gt;flags);<BR>}</P>
<P>#else<BR>static void timer_stats_account_timer(struct timer_list *timer) {}<BR>#endif</P>
<P>#ifdef CONFIG_DEBUG_OBJECTS_TIMERS</P>
<P>static struct debug_obj_descr timer_debug_descr;</P>
<P>static void *timer_debug_hint(void *addr)<BR>{<BR>&nbsp;return ((struct timer_list *) addr)-&gt;function;<BR>}</P>
<P>/*<BR>&nbsp;* fixup_init is called when:<BR>&nbsp;* - an active object is initialized<BR>&nbsp;*/<BR>static int timer_fixup_init(void *addr, enum debug_obj_state state)<BR>{<BR>&nbsp;struct timer_list *timer = addr;</P>
<P>&nbsp;switch (state) {<BR>&nbsp;case ODEBUG_STATE_ACTIVE:<BR>&nbsp;&nbsp;del_timer_sync(timer);<BR>&nbsp;&nbsp;debug_object_init(timer, &amp;timer_debug_descr);<BR>&nbsp;&nbsp;return 1;<BR>&nbsp;default:<BR>&nbsp;&nbsp;return 0;<BR>&nbsp;}<BR>}</P>
<P>/* Stub timer callback for improperly used timers. */<BR>static void stub_timer(unsigned long data)<BR>{<BR>&nbsp;WARN_ON(1);<BR>}</P>
<P>/*<BR>&nbsp;* fixup_activate is called when:<BR>&nbsp;* - an active object is activated<BR>&nbsp;* - an unknown object is activated (might be a statically initialized object)<BR>&nbsp;*/<BR>static int timer_fixup_activate(void *addr, enum debug_obj_state state)<BR>{<BR>&nbsp;struct timer_list *timer = addr;</P>
<P>&nbsp;switch (state) {</P>
<P>&nbsp;case ODEBUG_STATE_NOTAVAILABLE:<BR>&nbsp;&nbsp;/*<BR>&nbsp;&nbsp; * This is not really a fixup. The timer was<BR>&nbsp;&nbsp; * statically initialized. We just make sure that it<BR>&nbsp;&nbsp; * is tracked in the object tracker.<BR>&nbsp;&nbsp; */<BR>&nbsp;&nbsp;if (timer-&gt;entry.pprev == NULL &amp;&amp;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; timer-&gt;entry.next == TIMER_ENTRY_STATIC) {<BR>&nbsp;&nbsp;&nbsp;debug_object_init(timer, &amp;timer_debug_descr);<BR>&nbsp;&nbsp;&nbsp;debug_object_activate(timer, &amp;timer_debug_descr);<BR>&nbsp;&nbsp;&nbsp;return 0;<BR>&nbsp;&nbsp;} else {<BR>&nbsp;&nbsp;&nbsp;setup_timer(timer, stub_timer, 0);<BR>&nbsp;&nbsp;&nbsp;return 1;<BR>&nbsp;&nbsp;}<BR>&nbsp;&nbsp;return 0;</P>
<P>&nbsp;case ODEBUG_STATE_ACTIVE:<BR>&nbsp;&nbsp;WARN_ON(1);</P>
<P>&nbsp;default:<BR>&nbsp;&nbsp;return 0;<BR>&nbsp;}<BR>}</P>
<P>/*<BR>&nbsp;* fixup_free is called when:<BR>&nbsp;* - an active object is freed<BR>&nbsp;*/<BR>static int timer_fixup_free(void *addr, enum debug_obj_state state)<BR>{<BR>&nbsp;struct timer_list *timer = addr;</P>
<P>&nbsp;switch (state) {<BR>&nbsp;case ODEBUG_STATE_ACTIVE:<BR>&nbsp;&nbsp;del_timer_sync(timer);<BR>&nbsp;&nbsp;debug_object_free(timer, &amp;timer_debug_descr);<BR>&nbsp;&nbsp;return 1;<BR>&nbsp;default:<BR>&nbsp;&nbsp;return 0;<BR>&nbsp;}<BR>}</P>
<P>/*<BR>&nbsp;* fixup_assert_init is called when:<BR>&nbsp;* - an untracked/uninit-ed object is found<BR>&nbsp;*/<BR>static int timer_fixup_assert_init(void *addr, enum debug_obj_state state)<BR>{<BR>&nbsp;struct timer_list *timer = addr;</P>
<P>&nbsp;switch (state) {<BR>&nbsp;case ODEBUG_STATE_NOTAVAILABLE:<BR>&nbsp;&nbsp;if (timer-&gt;entry.next == TIMER_ENTRY_STATIC) {<BR>&nbsp;&nbsp;&nbsp;/*<BR>&nbsp;&nbsp;&nbsp; * This is not really a fixup. The timer was<BR>&nbsp;&nbsp;&nbsp; * statically initialized. We just make sure that it<BR>&nbsp;&nbsp;&nbsp; * is tracked in the object tracker.<BR>&nbsp;&nbsp;&nbsp; */<BR>&nbsp;&nbsp;&nbsp;debug_object_init(timer, &amp;timer_debug_descr);<BR>&nbsp;&nbsp;&nbsp;return 0;<BR>&nbsp;&nbsp;} else {<BR>&nbsp;&nbsp;&nbsp;setup_timer(timer, stub_timer, 0);<BR>&nbsp;&nbsp;&nbsp;return 1;<BR>&nbsp;&nbsp;}<BR>&nbsp;default:<BR>&nbsp;&nbsp;return 0;<BR>&nbsp;}<BR>}</P>
<P>static struct debug_obj_descr timer_debug_descr = {<BR>&nbsp;.name&nbsp;&nbsp;&nbsp;= "timer_list",<BR>&nbsp;.debug_hint&nbsp;&nbsp;= timer_debug_hint,<BR>&nbsp;.fixup_init&nbsp;&nbsp;= timer_fixup_init,<BR>&nbsp;.fixup_activate&nbsp;&nbsp;= timer_fixup_activate,<BR>&nbsp;.fixup_free&nbsp;&nbsp;= timer_fixup_free,<BR>&nbsp;.fixup_assert_init&nbsp;= timer_fixup_assert_init,<BR>};</P>
<P>static inline void debug_timer_init(struct timer_list *timer)<BR>{<BR>&nbsp;debug_object_init(timer, &amp;timer_debug_descr);<BR>}</P>
<P>static inline void debug_timer_activate(struct timer_list *timer)<BR>{<BR>&nbsp;debug_object_activate(timer, &amp;timer_debug_descr);<BR>}</P>
<P>static inline void debug_timer_deactivate(struct timer_list *timer)<BR>{<BR>&nbsp;debug_object_deactivate(timer, &amp;timer_debug_descr);<BR>}</P>
<P>static inline void debug_timer_free(struct timer_list *timer)<BR>{<BR>&nbsp;debug_object_free(timer, &amp;timer_debug_descr);<BR>}</P>
<P>static inline void debug_timer_assert_init(struct timer_list *timer)<BR>{<BR>&nbsp;debug_object_assert_init(timer, &amp;timer_debug_descr);<BR>}</P>
<P>static void do_init_timer(struct timer_list *timer, unsigned int flags,<BR>&nbsp;&nbsp;&nbsp;&nbsp; const char *name, struct lock_class_key *key);</P>
<P>void init_timer_on_stack_key(struct timer_list *timer, unsigned int flags,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; const char *name, struct lock_class_key *key)<BR>{<BR>&nbsp;debug_object_init_on_stack(timer, &amp;timer_debug_descr);<BR>&nbsp;do_init_timer(timer, flags, name, key);<BR>}<BR>EXPORT_SYMBOL_GPL(init_timer_on_stack_key);</P>
<P>void destroy_timer_on_stack(struct timer_list *timer)<BR>{<BR>&nbsp;debug_object_free(timer, &amp;timer_debug_descr);<BR>}<BR>EXPORT_SYMBOL_GPL(destroy_timer_on_stack);</P>
<P>#else<BR>static inline void debug_timer_init(struct timer_list *timer) { }<BR>static inline void debug_timer_activate(struct timer_list *timer) { }<BR>static inline void debug_timer_deactivate(struct timer_list *timer) { }<BR>static inline void debug_timer_assert_init(struct timer_list *timer) { }<BR>#endif</P>
<P>static inline void debug_init(struct timer_list *timer)<BR>{<BR>&nbsp;debug_timer_init(timer);<BR>&nbsp;trace_timer_init(timer);<BR>}</P>
<P>static inline void<BR>debug_activate(struct timer_list *timer, unsigned long expires)<BR>{<BR>&nbsp;debug_timer_activate(timer);<BR>&nbsp;trace_timer_start(timer, expires, timer-&gt;flags);<BR>}</P>
<P>static inline void debug_deactivate(struct timer_list *timer)<BR>{<BR>&nbsp;debug_timer_deactivate(timer);<BR>&nbsp;trace_timer_cancel(timer);<BR>}</P>
<P>static inline void debug_assert_init(struct timer_list *timer)<BR>{<BR>&nbsp;debug_timer_assert_init(timer);<BR>}</P>
<P>static void do_init_timer(struct timer_list *timer, unsigned int flags,<BR>&nbsp;&nbsp;&nbsp;&nbsp; const char *name, struct lock_class_key *key)<BR>{<BR>&nbsp;timer-&gt;entry.pprev = NULL;<BR>&nbsp;timer-&gt;flags = flags | raw_smp_processor_id();<BR>&nbsp;timer-&gt;slack = -1;<BR>#ifdef CONFIG_TIMER_STATS<BR>&nbsp;timer-&gt;start_site = NULL;<BR>&nbsp;timer-&gt;start_pid = -1;<BR>&nbsp;memset(timer-&gt;start_comm, 0, TASK_COMM_LEN);<BR>#endif<BR>&nbsp;lockdep_init_map(&amp;timer-&gt;lockdep_map, name, key, 0);<BR>}</P>
<P>/**<BR>&nbsp;* init_timer_key - initialize a timer<BR>&nbsp;* @timer: the timer to be initialized<BR>&nbsp;* @flags: timer flags<BR>&nbsp;* @name: name of the timer<BR>&nbsp;* @key: lockdep class key of the fake lock used for tracking timer<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; sync lock dependencies<BR>&nbsp;*<BR>&nbsp;* init_timer_key() must be done to a timer prior calling *any* of the<BR>&nbsp;* other timer functions.<BR>&nbsp;*/<BR>void init_timer_key(struct timer_list *timer, unsigned int flags,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; const char *name, struct lock_class_key *key)<BR>{<BR>&nbsp;debug_init(timer);<BR>&nbsp;do_init_timer(timer, flags, name, key);<BR>}<BR>EXPORT_SYMBOL(init_timer_key);</P>
<P>static inline void detach_timer(struct timer_list *timer, bool clear_pending)<BR>{<BR>&nbsp;struct hlist_node *entry = &amp;timer-&gt;entry;</P>
<P>&nbsp;debug_deactivate(timer);</P>
<P>&nbsp;__hlist_del(entry);<BR>&nbsp;if (clear_pending)<BR>&nbsp;&nbsp;entry-&gt;pprev = NULL;<BR>&nbsp;entry-&gt;next = LIST_POISON2;<BR>}</P>
<P>static inline void<BR>detach_expired_timer(struct timer_list *timer, struct tvec_base *base)<BR>{<BR>&nbsp;detach_timer(timer, true);<BR>&nbsp;if (!(timer-&gt;flags &amp; TIMER_DEFERRABLE))<BR>&nbsp;&nbsp;base-&gt;active_timers--;<BR>&nbsp;base-&gt;all_timers--;<BR>}</P>
<P>static int detach_if_pending(struct timer_list *timer, struct tvec_base *base,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; bool clear_pending)<BR>{<BR>&nbsp;if (!timer_pending(timer))<BR>&nbsp;&nbsp;return 0;</P>
<P>&nbsp;detach_timer(timer, clear_pending);<BR>&nbsp;if (!(timer-&gt;flags &amp; TIMER_DEFERRABLE)) {<BR>&nbsp;&nbsp;base-&gt;active_timers--;<BR>&nbsp;&nbsp;if (timer-&gt;expires == base-&gt;next_timer)<BR>&nbsp;&nbsp;&nbsp;base-&gt;next_timer = base-&gt;timer_jiffies;<BR>&nbsp;}<BR>&nbsp;/* If this was the last timer, advance base-&gt;jiffies */<BR>&nbsp;if (!--base-&gt;all_timers)<BR>&nbsp;&nbsp;base-&gt;timer_jiffies = jiffies;<BR>&nbsp;return 1;<BR>}</P>
<P>/*<BR>&nbsp;* We are using hashed locking: holding per_cpu(tvec_bases).lock<BR>&nbsp;* means that all timers which are tied to this base via timer-&gt;base are<BR>&nbsp;* locked, and the base itself is locked too.<BR>&nbsp;*<BR>&nbsp;* So __run_timers/migrate_timers can safely modify all timers which could<BR>&nbsp;* be found on -&gt;tvX lists.<BR>&nbsp;*<BR>&nbsp;* When the timer's base is locked and removed from the list, the<BR>&nbsp;* TIMER_MIGRATING flag is set, FIXME<BR>&nbsp;*/<BR>static struct tvec_base *lock_timer_base(struct timer_list *timer,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;unsigned long *flags)<BR>&nbsp;__acquires(timer-&gt;base-&gt;lock)<BR>{<BR>&nbsp;for (;;) {<BR>&nbsp;&nbsp;u32 tf = timer-&gt;flags;<BR>&nbsp;&nbsp;struct tvec_base *base;</P>
<P>&nbsp;&nbsp;if (!(tf &amp; TIMER_MIGRATING)) {<BR>&nbsp;&nbsp;&nbsp;base = per_cpu_ptr(&amp;tvec_bases, tf &amp; TIMER_CPUMASK);<BR>&nbsp;&nbsp;&nbsp;spin_lock_irqsave(&amp;base-&gt;lock, *flags);<BR>&nbsp;&nbsp;&nbsp;if (timer-&gt;flags == tf)<BR>&nbsp;&nbsp;&nbsp;&nbsp;return base;<BR>&nbsp;&nbsp;&nbsp;spin_unlock_irqrestore(&amp;base-&gt;lock, *flags);<BR>&nbsp;&nbsp;}<BR>&nbsp;&nbsp;cpu_relax();<BR>&nbsp;}<BR>}</P>
<P>static inline int<BR>__mod_timer(struct timer_list *timer, unsigned long expires,<BR>&nbsp;&nbsp;&nbsp;&nbsp; bool pending_only, int pinned)<BR>{<BR>&nbsp;struct tvec_base *base, *new_base;<BR>&nbsp;unsigned long flags;<BR>&nbsp;int ret = 0;</P>
<P>&nbsp;timer_stats_timer_set_start_info(timer);<BR>&nbsp;BUG_ON(!timer-&gt;function);</P>
<P>&nbsp;base = lock_timer_base(timer, &amp;flags);</P>
<P>&nbsp;ret = detach_if_pending(timer, base, false);<BR>&nbsp;if (!ret &amp;&amp; pending_only)<BR>&nbsp;&nbsp;goto out_unlock;</P>
<P>&nbsp;debug_activate(timer, expires);</P>
<P>&nbsp;new_base = get_target_base(base, pinned);</P>
<P>&nbsp;if (base != new_base) {<BR>&nbsp;&nbsp;/*<BR>&nbsp;&nbsp; * We are trying to schedule the timer on the local CPU.<BR>&nbsp;&nbsp; * However we can't change timer's base while it is running,<BR>&nbsp;&nbsp; * otherwise del_timer_sync() can't detect that the timer's<BR>&nbsp;&nbsp; * handler yet has not finished. This also guarantees that<BR>&nbsp;&nbsp; * the timer is serialized wrt itself.<BR>&nbsp;&nbsp; */<BR>&nbsp;&nbsp;if (likely(base-&gt;running_timer != timer)) {<BR>&nbsp;&nbsp;&nbsp;/* See the comment in lock_timer_base() */<BR>&nbsp;&nbsp;&nbsp;timer-&gt;flags |= TIMER_MIGRATING;</P>
<P>&nbsp;&nbsp;&nbsp;spin_unlock(&amp;base-&gt;lock);<BR>&nbsp;&nbsp;&nbsp;base = new_base;<BR>&nbsp;&nbsp;&nbsp;spin_lock(&amp;base-&gt;lock);<BR>&nbsp;&nbsp;&nbsp;WRITE_ONCE(timer-&gt;flags,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (timer-&gt;flags &amp; ~TIMER_BASEMASK) | base-&gt;cpu);<BR>&nbsp;&nbsp;}<BR>&nbsp;}</P>
<P>&nbsp;timer-&gt;expires = expires;<BR>&nbsp;internal_add_timer(base, timer);</P>
<P>out_unlock:<BR>&nbsp;spin_unlock_irqrestore(&amp;base-&gt;lock, flags);</P>
<P>&nbsp;return ret;<BR>}</P>
<P>/**<BR>&nbsp;* mod_timer_pending - modify a pending timer's timeout<BR>&nbsp;* @timer: the pending timer to be modified<BR>&nbsp;* @expires: new timeout in jiffies<BR>&nbsp;*<BR>&nbsp;* mod_timer_pending() is the same for pending timers as mod_timer(),<BR>&nbsp;* but will not re-activate and modify already deleted timers.<BR>&nbsp;*<BR>&nbsp;* It is useful for unserialized use of timers.<BR>&nbsp;*/<BR>int mod_timer_pending(struct timer_list *timer, unsigned long expires)<BR>{<BR>&nbsp;return __mod_timer(timer, expires, true, TIMER_NOT_PINNED);<BR>}<BR>EXPORT_SYMBOL(mod_timer_pending);</P>
<P>/*<BR>&nbsp;* Decide where to put the timer while taking the slack into account<BR>&nbsp;*<BR>&nbsp;* Algorithm:<BR>&nbsp;*&nbsp;&nbsp; 1) calculate the maximum (absolute) time<BR>&nbsp;*&nbsp;&nbsp; 2) calculate the highest bit where the expires and new max are different<BR>&nbsp;*&nbsp;&nbsp; 3) use this bit to make a mask<BR>&nbsp;*&nbsp;&nbsp; 4) use the bitmask to round down the maximum time, so that all last<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; bits are zeros<BR>&nbsp;*/<BR>static inline<BR>unsigned long apply_slack(struct timer_list *timer, unsigned long expires)<BR>{<BR>&nbsp;unsigned long expires_limit, mask;<BR>&nbsp;int bit;</P>
<P>&nbsp;if (timer-&gt;slack &gt;= 0) {<BR>&nbsp;&nbsp;expires_limit = expires + timer-&gt;slack;<BR>&nbsp;} else {<BR>&nbsp;&nbsp;long delta = expires - jiffies;</P>
<P>&nbsp;&nbsp;if (delta &lt; 256)<BR>&nbsp;&nbsp;&nbsp;return expires;</P>
<P>&nbsp;&nbsp;expires_limit = expires + delta / 256;<BR>&nbsp;}<BR>&nbsp;mask = expires ^ expires_limit;<BR>&nbsp;if (mask == 0)<BR>&nbsp;&nbsp;return expires;</P>
<P>&nbsp;bit = __fls(mask);</P>
<P>&nbsp;mask = (1UL &lt;&lt; bit) - 1;</P>
<P>&nbsp;expires_limit = expires_limit &amp; ~(mask);</P>
<P>&nbsp;return expires_limit;<BR>}</P>
<P><FONT class=extract>/**<BR>&nbsp;* mod_timer - modify a timer's timeout<BR>&nbsp;* @timer: the timer to be modified<BR>&nbsp;* @expires: new timeout in jiffies<BR>&nbsp;*<BR>&nbsp;* mod_timer() is a more efficient way to update the expire field of an<BR>&nbsp;* active timer (if the timer is inactive it will be activated)<BR>&nbsp;*<BR>&nbsp;* mod_timer(timer, expires) is equivalent to:<BR>&nbsp;*<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp; del_timer(timer); timer-&gt;expires = expires; add_timer(timer);<BR>&nbsp;*<BR>&nbsp;* Note that if there are multiple unserialized concurrent users of the<BR>&nbsp;* same timer, then mod_timer() is the only safe way to modify the timeout,<BR>&nbsp;* since add_timer() cannot modify an already running timer.<BR>&nbsp;*<BR>&nbsp;* The function returns whether it has modified a pending timer or not.<BR>&nbsp;* (ie. mod_timer() of an inactive timer returns 0, mod_timer() of an<BR>&nbsp;* active timer returns 1.)<BR>&nbsp;*/<BR>int mod_timer(struct timer_list *timer, unsigned long expires)<BR>{<BR>&nbsp;expires = apply_slack(timer, expires);</FONT></P>
<P><FONT class=extract>&nbsp;/*<BR>&nbsp; * This is a common optimization triggered by the<BR>&nbsp; * networking code - if the timer is re-modified<BR>&nbsp; * to be the same thing then just return:<BR>&nbsp; */<BR>&nbsp;if (timer_pending(timer) &amp;&amp; timer-&gt;expires == expires)<BR>&nbsp;&nbsp;return 1;</FONT></P>
<P><FONT class=extract>&nbsp;return __mod_timer(timer, expires, false, TIMER_NOT_PINNED);<BR>}<BR>EXPORT_SYMBOL(mod_timer);</FONT></P>
<P><FONT class=extract>/**<BR>&nbsp;* mod_timer_pinned - modify a timer's timeout<BR>&nbsp;* @timer: the timer to be modified<BR>&nbsp;* @expires: new timeout in jiffies<BR>&nbsp;*<BR>&nbsp;* mod_timer_pinned() is a way to update the expire field of an<BR>&nbsp;* active timer (if the timer is inactive it will be activated)<BR>&nbsp;* and to ensure that the timer is scheduled on the current CPU.<BR>&nbsp;*<BR>&nbsp;* Note that this does not prevent the timer from being migrated<BR>&nbsp;* when the current CPU goes offline.&nbsp; If this is a problem for<BR>&nbsp;* you, use CPU-hotplug notifiers to handle it correctly, for<BR>&nbsp;* example, cancelling the timer when the corresponding CPU goes<BR>&nbsp;* offline.<BR>&nbsp;*<BR>&nbsp;* mod_timer_pinned(timer, expires) is equivalent to:<BR>&nbsp;*<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp; del_timer(timer); timer-&gt;expires = expires; add_timer(timer);<BR>&nbsp;*/<BR>int mod_timer_pinned(struct timer_list *timer, unsigned long expires)<BR>{<BR>&nbsp;if (timer-&gt;expires == expires &amp;&amp; timer_pending(timer))<BR>&nbsp;&nbsp;return 1;</FONT></P>
<P><FONT class=extract>&nbsp;return __mod_timer(timer, expires, false, TIMER_PINNED);<BR>}<BR>EXPORT_SYMBOL(mod_timer_pinned);</FONT></P>
<P><FONT class=extract>/**<BR>&nbsp;* add_timer - start a timer<BR>&nbsp;* @timer: the timer to be added<BR>&nbsp;*<BR>&nbsp;* The kernel will do a -&gt;function(-&gt;data) callback from the<BR>&nbsp;* timer interrupt at the -&gt;expires point in the future. The<BR>&nbsp;* current time is 'jiffies'.<BR>&nbsp;*<BR>&nbsp;* The timer's -&gt;expires, -&gt;function (and if the handler uses it, -&gt;data)<BR>&nbsp;* fields must be set prior calling this function.<BR>&nbsp;*<BR>&nbsp;* Timers with an -&gt;expires field in the past will be executed in the next<BR>&nbsp;* timer tick.<BR>&nbsp;*/<BR>void add_timer(struct timer_list *timer)<BR>{<BR>&nbsp;BUG_ON(timer_pending(timer));<BR>&nbsp;mod_timer(timer, timer-&gt;expires);<BR>}<BR>EXPORT_SYMBOL(add_timer);</FONT></P>
<P>/**<BR>&nbsp;* add_timer_on - start a timer on a particular CPU<BR>&nbsp;* @timer: the timer to be added<BR>&nbsp;* @cpu: the CPU to start it on<BR>&nbsp;*<BR>&nbsp;* This is not very scalable on SMP. Double adds are not possible.<BR>&nbsp;*/<BR>void add_timer_on(struct timer_list *timer, int cpu)<BR>{<BR>&nbsp;struct tvec_base *new_base = per_cpu_ptr(&amp;tvec_bases, cpu);<BR>&nbsp;struct tvec_base *base;<BR>&nbsp;unsigned long flags;</P>
<P>&nbsp;timer_stats_timer_set_start_info(timer);<BR>&nbsp;BUG_ON(timer_pending(timer) || !timer-&gt;function);</P>
<P>&nbsp;/*<BR>&nbsp; * If @timer was on a different CPU, it should be migrated with the<BR>&nbsp; * old base locked to prevent other operations proceeding with the<BR>&nbsp; * wrong base locked.&nbsp; See lock_timer_base().<BR>&nbsp; */<BR>&nbsp;base = lock_timer_base(timer, &amp;flags);<BR>&nbsp;if (base != new_base) {<BR>&nbsp;&nbsp;timer-&gt;flags |= TIMER_MIGRATING;</P>
<P>&nbsp;&nbsp;spin_unlock(&amp;base-&gt;lock);<BR>&nbsp;&nbsp;base = new_base;<BR>&nbsp;&nbsp;spin_lock(&amp;base-&gt;lock);<BR>&nbsp;&nbsp;WRITE_ONCE(timer-&gt;flags,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (timer-&gt;flags &amp; ~TIMER_BASEMASK) | cpu);<BR>&nbsp;}</P>
<P>&nbsp;debug_activate(timer, timer-&gt;expires);<BR>&nbsp;internal_add_timer(base, timer);<BR>&nbsp;spin_unlock_irqrestore(&amp;base-&gt;lock, flags);<BR>}<BR>EXPORT_SYMBOL_GPL(add_timer_on);</P>
<P><FONT class=extract>/**<BR>&nbsp;* del_timer - deactive a timer.<BR>&nbsp;* @timer: the timer to be deactivated<BR>&nbsp;*<BR>&nbsp;* del_timer() deactivates a timer - this works on both active and inactive<BR>&nbsp;* timers.<BR>&nbsp;*<BR>&nbsp;* The function returns whether it has deactivated a pending timer or not.<BR>&nbsp;* (ie. del_timer() of an inactive timer returns 0, del_timer() of an<BR>&nbsp;* active timer returns 1.)<BR>&nbsp;*/<BR>int del_timer(struct timer_list *timer)<BR>{<BR>&nbsp;struct tvec_base *base;<BR>&nbsp;unsigned long flags;<BR>&nbsp;int ret = 0;</FONT></P>
<P><FONT class=extract>&nbsp;debug_assert_init(timer);</FONT></P>
<P><FONT class=extract>&nbsp;timer_stats_timer_clear_start_info(timer);<BR>&nbsp;if (timer_pending(timer)) {<BR>&nbsp;&nbsp;base = lock_timer_base(timer, &amp;flags);<BR>&nbsp;&nbsp;ret = detach_if_pending(timer, base, true);<BR>&nbsp;&nbsp;spin_unlock_irqrestore(&amp;base-&gt;lock, flags);<BR>&nbsp;}</FONT></P>
<P><FONT class=extract>&nbsp;return ret;<BR>}<BR>EXPORT_SYMBOL(del_timer);</FONT></P>
<P>/**<BR>&nbsp;* try_to_del_timer_sync - Try to deactivate a timer<BR>&nbsp;* @timer: timer do del<BR>&nbsp;*<BR>&nbsp;* This function tries to deactivate a timer. Upon successful (ret &gt;= 0)<BR>&nbsp;* exit the timer is not queued and the handler is not running on any CPU.<BR>&nbsp;*/<BR>int try_to_del_timer_sync(struct timer_list *timer)<BR>{<BR>&nbsp;struct tvec_base *base;<BR>&nbsp;unsigned long flags;<BR>&nbsp;int ret = -1;</P>
<P>&nbsp;debug_assert_init(timer);</P>
<P>&nbsp;base = lock_timer_base(timer, &amp;flags);</P>
<P>&nbsp;if (base-&gt;running_timer != timer) {<BR>&nbsp;&nbsp;timer_stats_timer_clear_start_info(timer);<BR>&nbsp;&nbsp;ret = detach_if_pending(timer, base, true);<BR>&nbsp;}<BR>&nbsp;spin_unlock_irqrestore(&amp;base-&gt;lock, flags);</P>
<P>&nbsp;return ret;<BR>}<BR>EXPORT_SYMBOL(try_to_del_timer_sync);</P>
<P>#ifdef CONFIG_SMP<BR>/**<BR>&nbsp;* del_timer_sync - deactivate a timer and wait for the handler to finish.<BR>&nbsp;* @timer: the timer to be deactivated<BR>&nbsp;*<BR>&nbsp;* This function only differs from del_timer() on SMP: besides deactivating<BR>&nbsp;* the timer it also makes sure the handler has finished executing on other<BR>&nbsp;* CPUs.<BR>&nbsp;*<BR>&nbsp;* Synchronization rules: Callers must prevent restarting of the timer,<BR>&nbsp;* otherwise this function is meaningless. It must not be called from<BR>&nbsp;* interrupt contexts unless the timer is an irqsafe one. The caller must<BR>&nbsp;* not hold locks which would prevent completion of the timer's<BR>&nbsp;* handler. The timer's handler must not call add_timer_on(). Upon exit the<BR>&nbsp;* timer is not queued and the handler is not running on any CPU.<BR>&nbsp;*<BR>&nbsp;* Note: For !irqsafe timers, you must not hold locks that are held in<BR>&nbsp;*&nbsp;&nbsp; interrupt context while calling this function. Even if the lock has<BR>&nbsp;*&nbsp;&nbsp; nothing to do with the timer in question.&nbsp; Here's why:<BR>&nbsp;*<BR>&nbsp;*&nbsp;&nbsp;&nbsp; CPU0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; CPU1<BR>&nbsp;*&nbsp;&nbsp;&nbsp; ----&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ----<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;SOFTIRQ&gt;<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; call_timer_fn();<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; base-&gt;running_timer = mytimer;<BR>&nbsp;*&nbsp; spin_lock_irq(somelock);<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;IRQ&gt;<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; spin_lock(somelock);<BR>&nbsp;*&nbsp; del_timer_sync(mytimer);<BR>&nbsp;*&nbsp;&nbsp; while (base-&gt;running_timer == mytimer);<BR>&nbsp;*<BR>&nbsp;* Now del_timer_sync() will never return and never release somelock.<BR>&nbsp;* The interrupt on the other CPU is waiting to grab somelock but<BR>&nbsp;* it has interrupted the softirq that CPU0 is waiting to finish.<BR>&nbsp;*<BR>&nbsp;* The function returns whether it has deactivated a pending timer or not.<BR>&nbsp;*/<BR>int del_timer_sync(struct timer_list *timer)<BR>{<BR>#ifdef CONFIG_LOCKDEP<BR>&nbsp;unsigned long flags;</P>
<P>&nbsp;/*<BR>&nbsp; * If lockdep gives a backtrace here, please reference<BR>&nbsp; * the synchronization rules above.<BR>&nbsp; */<BR>&nbsp;local_irq_save(flags);<BR>&nbsp;lock_map_acquire(&amp;timer-&gt;lockdep_map);<BR>&nbsp;lock_map_release(&amp;timer-&gt;lockdep_map);<BR>&nbsp;local_irq_restore(flags);<BR>#endif<BR>&nbsp;/*<BR>&nbsp; * don't use it in hardirq context, because it<BR>&nbsp; * could lead to deadlock.<BR>&nbsp; */<BR>&nbsp;WARN_ON(in_irq() &amp;&amp; !(timer-&gt;flags &amp; TIMER_IRQSAFE));<BR>&nbsp;for (;;) {<BR>&nbsp;&nbsp;int ret = try_to_del_timer_sync(timer);<BR>&nbsp;&nbsp;if (ret &gt;= 0)<BR>&nbsp;&nbsp;&nbsp;return ret;<BR>&nbsp;&nbsp;cpu_relax();<BR>&nbsp;}<BR>}<BR>EXPORT_SYMBOL(del_timer_sync);<BR>#endif</P>
<P>static int cascade(struct tvec_base *base, struct tvec *tv, int index)<BR>{<BR>&nbsp;/* cascade all the timers from tv up one level */<BR>&nbsp;struct timer_list *timer;<BR>&nbsp;struct hlist_node *tmp;<BR>&nbsp;struct hlist_head tv_list;</P>
<P>&nbsp;hlist_move_list(tv-&gt;vec + index, &amp;tv_list);</P>
<P>&nbsp;/*<BR>&nbsp; * We are removing _all_ timers from the list, so we<BR>&nbsp; * don't have to detach them individually.<BR>&nbsp; */<BR>&nbsp;hlist_for_each_entry_safe(timer, tmp, &amp;tv_list, entry) {<BR>&nbsp;&nbsp;/* No accounting, while moving them */<BR>&nbsp;&nbsp;__internal_add_timer(base, timer);<BR>&nbsp;}</P>
<P>&nbsp;return index;<BR>}</P>
<P>static void call_timer_fn(struct timer_list *timer, void (*fn)(unsigned long),<BR>&nbsp;&nbsp;&nbsp;&nbsp; unsigned long data)<BR>{<BR>&nbsp;int count = preempt_count();</P>
<P>#ifdef CONFIG_LOCKDEP<BR>&nbsp;/*<BR>&nbsp; * It is permissible to free the timer from inside the<BR>&nbsp; * function that is called from it, this we need to take into<BR>&nbsp; * account for lockdep too. To avoid bogus "held lock freed"<BR>&nbsp; * warnings as well as problems when looking into<BR>&nbsp; * timer-&gt;lockdep_map, make a copy and use that here.<BR>&nbsp; */<BR>&nbsp;struct lockdep_map lockdep_map;</P>
<P>&nbsp;lockdep_copy_map(&amp;lockdep_map, &amp;timer-&gt;lockdep_map);<BR>#endif<BR>&nbsp;/*<BR>&nbsp; * Couple the lock chain with the lock chain at<BR>&nbsp; * del_timer_sync() by acquiring the lock_map around the fn()<BR>&nbsp; * call here and in del_timer_sync().<BR>&nbsp; */<BR>&nbsp;lock_map_acquire(&amp;lockdep_map);</P>
<P>&nbsp;trace_timer_expire_entry(timer);<BR>&nbsp;fn(data);<BR>&nbsp;trace_timer_expire_exit(timer);</P>
<P>&nbsp;lock_map_release(&amp;lockdep_map);</P>
<P>&nbsp;if (count != preempt_count()) {<BR>&nbsp;&nbsp;WARN_ONCE(1, "timer: %pF preempt leak: %08x -&gt; %08x\n",<BR>&nbsp;&nbsp;&nbsp;&nbsp; fn, count, preempt_count());<BR>&nbsp;&nbsp;/*<BR>&nbsp;&nbsp; * Restore the preempt count. That gives us a decent<BR>&nbsp;&nbsp; * chance to survive and extract information. If the<BR>&nbsp;&nbsp; * callback kept a lock held, bad luck, but not worse<BR>&nbsp;&nbsp; * than the BUG() we had.<BR>&nbsp;&nbsp; */<BR>&nbsp;&nbsp;preempt_count_set(count);<BR>&nbsp;}<BR>}</P>
<P>#define INDEX(N) ((base-&gt;timer_jiffies &gt;&gt; (TVR_BITS + (N) * TVN_BITS)) &amp; TVN_MASK)</P>
<P>/**<BR>&nbsp;* __run_timers - run all expired timers (if any) on this CPU.<BR>&nbsp;* @base: the timer vector to be processed.<BR>&nbsp;*<BR>&nbsp;* This function cascades all vectors and executes all expired timer<BR>&nbsp;* vectors.<BR>&nbsp;*/<BR>static inline void __run_timers(struct tvec_base *base)<BR>{<BR>&nbsp;struct timer_list *timer;</P>
<P>&nbsp;spin_lock_irq(&amp;base-&gt;lock);</P>
<P>&nbsp;while (time_after_eq(jiffies, base-&gt;timer_jiffies)) {<BR>&nbsp;&nbsp;struct hlist_head work_list;<BR>&nbsp;&nbsp;struct hlist_head *head = &amp;work_list;<BR>&nbsp;&nbsp;int index;</P>
<P>&nbsp;&nbsp;if (!base-&gt;all_timers) {<BR>&nbsp;&nbsp;&nbsp;base-&gt;timer_jiffies = jiffies;<BR>&nbsp;&nbsp;&nbsp;break;<BR>&nbsp;&nbsp;}</P>
<P>&nbsp;&nbsp;index = base-&gt;timer_jiffies &amp; TVR_MASK;</P>
<P>&nbsp;&nbsp;/*<BR>&nbsp;&nbsp; * Cascade timers:<BR>&nbsp;&nbsp; */<BR>&nbsp;&nbsp;if (!index &amp;&amp;<BR>&nbsp;&nbsp;&nbsp;(!cascade(base, &amp;base-&gt;tv2, INDEX(0))) &amp;&amp;<BR>&nbsp;&nbsp;&nbsp;&nbsp;(!cascade(base, &amp;base-&gt;tv3, INDEX(1))) &amp;&amp;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;!cascade(base, &amp;base-&gt;tv4, INDEX(2)))<BR>&nbsp;&nbsp;&nbsp;cascade(base, &amp;base-&gt;tv5, INDEX(3));<BR>&nbsp;&nbsp;++base-&gt;timer_jiffies;<BR>&nbsp;&nbsp;hlist_move_list(base-&gt;tv1.vec + index, head);<BR>&nbsp;&nbsp;while (!hlist_empty(head)) {<BR>&nbsp;&nbsp;&nbsp;void (*fn)(unsigned long);<BR>&nbsp;&nbsp;&nbsp;unsigned long data;<BR>&nbsp;&nbsp;&nbsp;bool irqsafe;</P>
<P>&nbsp;&nbsp;&nbsp;timer = hlist_entry(head-&gt;first, struct timer_list, entry);<BR>&nbsp;&nbsp;&nbsp;fn = timer-&gt;function;<BR>&nbsp;&nbsp;&nbsp;data = timer-&gt;data;<BR>&nbsp;&nbsp;&nbsp;irqsafe = timer-&gt;flags &amp; TIMER_IRQSAFE;</P>
<P>&nbsp;&nbsp;&nbsp;timer_stats_account_timer(timer);</P>
<P>&nbsp;&nbsp;&nbsp;base-&gt;running_timer = timer;<BR>&nbsp;&nbsp;&nbsp;detach_expired_timer(timer, base);</P>
<P>&nbsp;&nbsp;&nbsp;if (irqsafe) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;spin_unlock(&amp;base-&gt;lock);<BR>&nbsp;&nbsp;&nbsp;&nbsp;call_timer_fn(timer, fn, data);<BR>&nbsp;&nbsp;&nbsp;&nbsp;spin_lock(&amp;base-&gt;lock);<BR>&nbsp;&nbsp;&nbsp;} else {<BR>&nbsp;&nbsp;&nbsp;&nbsp;spin_unlock_irq(&amp;base-&gt;lock);<BR>&nbsp;&nbsp;&nbsp;&nbsp;call_timer_fn(timer, fn, data);<BR>&nbsp;&nbsp;&nbsp;&nbsp;spin_lock_irq(&amp;base-&gt;lock);<BR>&nbsp;&nbsp;&nbsp;}<BR>&nbsp;&nbsp;}<BR>&nbsp;}<BR>&nbsp;base-&gt;running_timer = NULL;<BR>&nbsp;spin_unlock_irq(&amp;base-&gt;lock);<BR>}</P>
<P>#ifdef CONFIG_NO_HZ_COMMON<BR>/*<BR>&nbsp;* Find out when the next timer event is due to happen. This<BR>&nbsp;* is used on S/390 to stop all activity when a CPU is idle.<BR>&nbsp;* This function needs to be called with interrupts disabled.<BR>&nbsp;*/<BR>static unsigned long __next_timer_interrupt(struct tvec_base *base)<BR>{<BR>&nbsp;unsigned long timer_jiffies = base-&gt;timer_jiffies;<BR>&nbsp;unsigned long expires = timer_jiffies + NEXT_TIMER_MAX_DELTA;<BR>&nbsp;int index, slot, array, found = 0;<BR>&nbsp;struct timer_list *nte;<BR>&nbsp;struct tvec *varray[4];</P>
<P>&nbsp;/* Look for timer events in tv1. */<BR>&nbsp;index = slot = timer_jiffies &amp; TVR_MASK;<BR>&nbsp;do {<BR>&nbsp;&nbsp;hlist_for_each_entry(nte, base-&gt;tv1.vec + slot, entry) {<BR>&nbsp;&nbsp;&nbsp;if (nte-&gt;flags &amp; TIMER_DEFERRABLE)<BR>&nbsp;&nbsp;&nbsp;&nbsp;continue;</P>
<P>&nbsp;&nbsp;&nbsp;found = 1;<BR>&nbsp;&nbsp;&nbsp;expires = nte-&gt;expires;<BR>&nbsp;&nbsp;&nbsp;/* Look at the cascade bucket(s)? */<BR>&nbsp;&nbsp;&nbsp;if (!index || slot &lt; index)<BR>&nbsp;&nbsp;&nbsp;&nbsp;goto cascade;<BR>&nbsp;&nbsp;&nbsp;return expires;<BR>&nbsp;&nbsp;}<BR>&nbsp;&nbsp;slot = (slot + 1) &amp; TVR_MASK;<BR>&nbsp;} while (slot != index);</P>
<P>cascade:<BR>&nbsp;/* Calculate the next cascade event */<BR>&nbsp;if (index)<BR>&nbsp;&nbsp;timer_jiffies += TVR_SIZE - index;<BR>&nbsp;timer_jiffies &gt;&gt;= TVR_BITS;</P>
<P>&nbsp;/* Check tv2-tv5. */<BR>&nbsp;varray[0] = &amp;base-&gt;tv2;<BR>&nbsp;varray[1] = &amp;base-&gt;tv3;<BR>&nbsp;varray[2] = &amp;base-&gt;tv4;<BR>&nbsp;varray[3] = &amp;base-&gt;tv5;</P>
<P>&nbsp;for (array = 0; array &lt; 4; array++) {<BR>&nbsp;&nbsp;struct tvec *varp = varray[array];</P>
<P>&nbsp;&nbsp;index = slot = timer_jiffies &amp; TVN_MASK;<BR>&nbsp;&nbsp;do {<BR>&nbsp;&nbsp;&nbsp;hlist_for_each_entry(nte, varp-&gt;vec + slot, entry) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;if (nte-&gt;flags &amp; TIMER_DEFERRABLE)<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;continue;</P>
<P>&nbsp;&nbsp;&nbsp;&nbsp;found = 1;<BR>&nbsp;&nbsp;&nbsp;&nbsp;if (time_before(nte-&gt;expires, expires))<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;expires = nte-&gt;expires;<BR>&nbsp;&nbsp;&nbsp;}<BR>&nbsp;&nbsp;&nbsp;/*<BR>&nbsp;&nbsp;&nbsp; * Do we still search for the first timer or are<BR>&nbsp;&nbsp;&nbsp; * we looking up the cascade buckets ?<BR>&nbsp;&nbsp;&nbsp; */<BR>&nbsp;&nbsp;&nbsp;if (found) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;/* Look at the cascade bucket(s)? */<BR>&nbsp;&nbsp;&nbsp;&nbsp;if (!index || slot &lt; index)<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;break;<BR>&nbsp;&nbsp;&nbsp;&nbsp;return expires;<BR>&nbsp;&nbsp;&nbsp;}<BR>&nbsp;&nbsp;&nbsp;slot = (slot + 1) &amp; TVN_MASK;<BR>&nbsp;&nbsp;} while (slot != index);</P>
<P>&nbsp;&nbsp;if (index)<BR>&nbsp;&nbsp;&nbsp;timer_jiffies += TVN_SIZE - index;<BR>&nbsp;&nbsp;timer_jiffies &gt;&gt;= TVN_BITS;<BR>&nbsp;}<BR>&nbsp;return expires;<BR>}</P>
<P>/*<BR>&nbsp;* Check, if the next hrtimer event is before the next timer wheel<BR>&nbsp;* event:<BR>&nbsp;*/<BR>static u64 cmp_next_hrtimer_event(u64 basem, u64 expires)<BR>{<BR>&nbsp;u64 nextevt = hrtimer_get_next_event();</P>
<P>&nbsp;/*<BR>&nbsp; * If high resolution timers are enabled<BR>&nbsp; * hrtimer_get_next_event() returns KTIME_MAX.<BR>&nbsp; */<BR>&nbsp;if (expires &lt;= nextevt)<BR>&nbsp;&nbsp;return expires;</P>
<P>&nbsp;/*<BR>&nbsp; * If the next timer is already expired, return the tick base<BR>&nbsp; * time so the tick is fired immediately.<BR>&nbsp; */<BR>&nbsp;if (nextevt &lt;= basem)<BR>&nbsp;&nbsp;return basem;</P>
<P>&nbsp;/*<BR>&nbsp; * Round up to the next jiffie. High resolution timers are<BR>&nbsp; * off, so the hrtimers are expired in the tick and we need to<BR>&nbsp; * make sure that this tick really expires the timer to avoid<BR>&nbsp; * a ping pong of the nohz stop code.<BR>&nbsp; *<BR>&nbsp; * Use DIV_ROUND_UP_ULL to prevent gcc calling __divdi3<BR>&nbsp; */<BR>&nbsp;return DIV_ROUND_UP_ULL(nextevt, TICK_NSEC) * TICK_NSEC;<BR>}</P>
<P>/**<BR>&nbsp;* get_next_timer_interrupt - return the time (clock mono) of the next timer<BR>&nbsp;* @basej:&nbsp;base time jiffies<BR>&nbsp;* @basem:&nbsp;base time clock monotonic<BR>&nbsp;*<BR>&nbsp;* Returns the tick aligned clock monotonic time of the next pending<BR>&nbsp;* timer or KTIME_MAX if no timer is pending.<BR>&nbsp;*/<BR>u64 get_next_timer_interrupt(unsigned long basej, u64 basem)<BR>{<BR>&nbsp;struct tvec_base *base = this_cpu_ptr(&amp;tvec_bases);<BR>&nbsp;u64 expires = KTIME_MAX;<BR>&nbsp;unsigned long nextevt;</P>
<P>&nbsp;/*<BR>&nbsp; * Pretend that there is no timer pending if the cpu is offline.<BR>&nbsp; * Possible pending timers will be migrated later to an active cpu.<BR>&nbsp; */<BR>&nbsp;if (cpu_is_offline(smp_processor_id()))<BR>&nbsp;&nbsp;return expires;</P>
<P>&nbsp;spin_lock(&amp;base-&gt;lock);<BR>&nbsp;if (base-&gt;active_timers) {<BR>&nbsp;&nbsp;if (time_before_eq(base-&gt;next_timer, base-&gt;timer_jiffies))<BR>&nbsp;&nbsp;&nbsp;base-&gt;next_timer = __next_timer_interrupt(base);<BR>&nbsp;&nbsp;nextevt = base-&gt;next_timer;<BR>&nbsp;&nbsp;if (time_before_eq(nextevt, basej))<BR>&nbsp;&nbsp;&nbsp;expires = basem;<BR>&nbsp;&nbsp;else<BR>&nbsp;&nbsp;&nbsp;expires = basem + (nextevt - basej) * TICK_NSEC;<BR>&nbsp;}<BR>&nbsp;spin_unlock(&amp;base-&gt;lock);</P>
<P>&nbsp;return cmp_next_hrtimer_event(basem, expires);<BR>}<BR>#endif</P>
<P>/*<BR>&nbsp;* Called from the timer interrupt handler to charge one tick to the current<BR>&nbsp;* process.&nbsp; user_tick is 1 if the tick is user time, 0 for system.<BR>&nbsp;*/<BR>void update_process_times(int user_tick)<BR>{<BR>&nbsp;struct task_struct *p = current;</P>
<P>&nbsp;/* Note: this timer irq context must be accounted for as well. */<BR>&nbsp;account_process_tick(p, user_tick);<BR>&nbsp;run_local_timers();<BR>&nbsp;rcu_check_callbacks(user_tick);<BR>#ifdef CONFIG_IRQ_WORK<BR>&nbsp;if (in_irq())<BR>&nbsp;&nbsp;irq_work_tick();<BR>#endif<BR>&nbsp;scheduler_tick();<BR>&nbsp;run_posix_cpu_timers(p);<BR>}</P>
<P>/*<BR>&nbsp;* This function runs timers and the timer-tq in bottom half context.<BR>&nbsp;*/<BR>static void run_timer_softirq(struct softirq_action *h)<BR>{<BR>&nbsp;struct tvec_base *base = this_cpu_ptr(&amp;tvec_bases);</P>
<P>&nbsp;if (time_after_eq(jiffies, base-&gt;timer_jiffies))<BR>&nbsp;&nbsp;__run_timers(base);<BR>}</P>
<P>/*<BR>&nbsp;* Called by the local, per-CPU timer interrupt on SMP.<BR>&nbsp;*/<BR>void run_local_timers(void)<BR>{<BR>&nbsp;hrtimer_run_queues();<BR>&nbsp;raise_softirq(TIMER_SOFTIRQ);<BR>}</P>
<P>#ifdef __ARCH_WANT_SYS_ALARM</P>
<P>/*<BR>&nbsp;* For backwards compatibility?&nbsp; This can be done in libc so Alpha<BR>&nbsp;* and all newer ports shouldn't need it.<BR>&nbsp;*/<BR>SYSCALL_DEFINE1(alarm, unsigned int, seconds)<BR>{<BR>&nbsp;return alarm_setitimer(seconds);<BR>}</P>
<P>#endif</P>
<P>static void process_timeout(unsigned long __data)<BR>{<BR>&nbsp;wake_up_process((struct task_struct *)__data);<BR>}</P>
<P><FONT class=extract>/**<BR>&nbsp;* schedule_timeout - sleep until timeout<BR>&nbsp;* @timeout: timeout value in jiffies<BR>&nbsp;*<BR>&nbsp;* Make the current task sleep until @timeout jiffies have<BR>&nbsp;* elapsed. The routine will return immediately unless<BR>&nbsp;* the current task state has been set (see set_current_state()).<BR>&nbsp;*<BR>&nbsp;* You can set the task state as follows -<BR>&nbsp;*<BR>&nbsp;* %TASK_UNINTERRUPTIBLE - at least @timeout jiffies are guaranteed to<BR>&nbsp;* pass before the routine returns. The routine will return 0<BR>&nbsp;*<BR>&nbsp;* %TASK_INTERRUPTIBLE - the routine may return early if a signal is<BR>&nbsp;* delivered to the current task. In this case the remaining time<BR>&nbsp;* in jiffies will be returned, or 0 if the timer expired in time<BR>&nbsp;*<BR>&nbsp;* The current task state is guaranteed to be TASK_RUNNING when this<BR>&nbsp;* routine returns.<BR>&nbsp;*<BR>&nbsp;* Specifying a @timeout value of %MAX_SCHEDULE_TIMEOUT will schedule<BR>&nbsp;* the CPU away without a bound on the timeout. In this case the return<BR>&nbsp;* value will be %MAX_SCHEDULE_TIMEOUT.<BR>&nbsp;*<BR>&nbsp;* In all cases the return value is guaranteed to be non-negative.<BR>&nbsp;*/<BR>signed long __sched schedule_timeout(signed long timeout)<BR>{<BR>&nbsp;struct timer_list timer;<BR>&nbsp;unsigned long expire;</FONT></P>
<P><FONT class=extract>&nbsp;switch (timeout)<BR>&nbsp;{<BR>&nbsp;case MAX_SCHEDULE_TIMEOUT:<BR>&nbsp;&nbsp;/*<BR>&nbsp;&nbsp; * These two special cases are useful to be comfortable<BR>&nbsp;&nbsp; * in the caller. Nothing more. We could take<BR>&nbsp;&nbsp; * MAX_SCHEDULE_TIMEOUT from one of the negative value<BR>&nbsp;&nbsp; * but I' d like to return a valid offset (&gt;=0) to allow<BR>&nbsp;&nbsp; * the caller to do everything it want with the retval.<BR>&nbsp;&nbsp; */<BR>&nbsp;&nbsp;schedule();<BR>&nbsp;&nbsp;goto out;<BR>&nbsp;default:<BR>&nbsp;&nbsp;/*<BR>&nbsp;&nbsp; * Another bit of PARANOID. Note that the retval will be<BR>&nbsp;&nbsp; * 0 since no piece of kernel is supposed to do a check<BR>&nbsp;&nbsp; * for a negative retval of schedule_timeout() (since it<BR>&nbsp;&nbsp; * should never happens anyway). You just have the printk()<BR>&nbsp;&nbsp; * that will tell you if something is gone wrong and where.<BR>&nbsp;&nbsp; */<BR>&nbsp;&nbsp;if (timeout &lt; 0) {<BR>&nbsp;&nbsp;&nbsp;printk(KERN_ERR "schedule_timeout: wrong timeout "<BR>&nbsp;&nbsp;&nbsp;&nbsp;"value %lx\n", timeout);<BR>&nbsp;&nbsp;&nbsp;dump_stack();<BR>&nbsp;&nbsp;&nbsp;current-&gt;state = TASK_RUNNING;<BR>&nbsp;&nbsp;&nbsp;goto out;<BR>&nbsp;&nbsp;}<BR>&nbsp;}</FONT></P>
<P><FONT class=extract>&nbsp;expire = timeout + jiffies;</FONT></P>
<P><FONT class=extract>&nbsp;setup_timer_on_stack(&amp;timer, process_timeout, (unsigned long)current);<BR>&nbsp;__mod_timer(&amp;timer, expire, false, TIMER_NOT_PINNED);<BR>&nbsp;schedule();<BR>&nbsp;del_singleshot_timer_sync(&amp;timer);</FONT></P>
<P><FONT class=extract>&nbsp;/* Remove the timer from the object tracker */<BR>&nbsp;destroy_timer_on_stack(&amp;timer);</FONT></P>
<P><FONT class=extract>&nbsp;timeout = expire - jiffies;</FONT></P>
<P><FONT class=extract>&nbsp;out:<BR>&nbsp;return timeout &lt; 0 ? 0 : timeout;<BR>}<BR>EXPORT_SYMBOL(schedule_timeout);</FONT></P>
<P>/*<BR>&nbsp;* We can use __set_current_state() here because schedule_timeout() calls<BR>&nbsp;* schedule() unconditionally.<BR>&nbsp;*/<BR>signed long __sched schedule_timeout_interruptible(signed long timeout)<BR>{<BR>&nbsp;__set_current_state(TASK_INTERRUPTIBLE);<BR>&nbsp;return schedule_timeout(timeout);<BR>}<BR>EXPORT_SYMBOL(schedule_timeout_interruptible);</P>
<P>signed long __sched schedule_timeout_killable(signed long timeout)<BR>{<BR>&nbsp;__set_current_state(TASK_KILLABLE);<BR>&nbsp;return schedule_timeout(timeout);<BR>}<BR>EXPORT_SYMBOL(schedule_timeout_killable);</P>
<P>signed long __sched schedule_timeout_uninterruptible(signed long timeout)<BR>{<BR>&nbsp;__set_current_state(TASK_UNINTERRUPTIBLE);<BR>&nbsp;return schedule_timeout(timeout);<BR>}<BR>EXPORT_SYMBOL(schedule_timeout_uninterruptible);</P>
<P>/*<BR>&nbsp;* Like schedule_timeout_uninterruptible(), except this task will not contribute<BR>&nbsp;* to load average.<BR>&nbsp;*/<BR>signed long __sched schedule_timeout_idle(signed long timeout)<BR>{<BR>&nbsp;__set_current_state(TASK_IDLE);<BR>&nbsp;return schedule_timeout(timeout);<BR>}<BR>EXPORT_SYMBOL(schedule_timeout_idle);</P>
<P>#ifdef CONFIG_HOTPLUG_CPU<BR>static void migrate_timer_list(struct tvec_base *new_base, struct hlist_head *head)<BR>{<BR>&nbsp;struct timer_list *timer;<BR>&nbsp;int cpu = new_base-&gt;cpu;</P>
<P>&nbsp;while (!hlist_empty(head)) {<BR>&nbsp;&nbsp;timer = hlist_entry(head-&gt;first, struct timer_list, entry);<BR>&nbsp;&nbsp;/* We ignore the accounting on the dying cpu */<BR>&nbsp;&nbsp;detach_timer(timer, false);<BR>&nbsp;&nbsp;timer-&gt;flags = (timer-&gt;flags &amp; ~TIMER_BASEMASK) | cpu;<BR>&nbsp;&nbsp;internal_add_timer(new_base, timer);<BR>&nbsp;}<BR>}</P>
<P>static void migrate_timers(int cpu)<BR>{<BR>&nbsp;struct tvec_base *old_base;<BR>&nbsp;struct tvec_base *new_base;<BR>&nbsp;int i;</P>
<P>&nbsp;BUG_ON(cpu_online(cpu));<BR>&nbsp;old_base = per_cpu_ptr(&amp;tvec_bases, cpu);<BR>&nbsp;new_base = get_cpu_ptr(&amp;tvec_bases);<BR>&nbsp;/*<BR>&nbsp; * The caller is globally serialized and nobody else<BR>&nbsp; * takes two locks at once, deadlock is not possible.<BR>&nbsp; */<BR>&nbsp;spin_lock_irq(&amp;new_base-&gt;lock);<BR>&nbsp;spin_lock_nested(&amp;old_base-&gt;lock, SINGLE_DEPTH_NESTING);</P>
<P>&nbsp;BUG_ON(old_base-&gt;running_timer);</P>
<P>&nbsp;for (i = 0; i &lt; TVR_SIZE; i++)<BR>&nbsp;&nbsp;migrate_timer_list(new_base, old_base-&gt;tv1.vec + i);<BR>&nbsp;for (i = 0; i &lt; TVN_SIZE; i++) {<BR>&nbsp;&nbsp;migrate_timer_list(new_base, old_base-&gt;tv2.vec + i);<BR>&nbsp;&nbsp;migrate_timer_list(new_base, old_base-&gt;tv3.vec + i);<BR>&nbsp;&nbsp;migrate_timer_list(new_base, old_base-&gt;tv4.vec + i);<BR>&nbsp;&nbsp;migrate_timer_list(new_base, old_base-&gt;tv5.vec + i);<BR>&nbsp;}</P>
<P>&nbsp;old_base-&gt;active_timers = 0;<BR>&nbsp;old_base-&gt;all_timers = 0;</P>
<P>&nbsp;spin_unlock(&amp;old_base-&gt;lock);<BR>&nbsp;spin_unlock_irq(&amp;new_base-&gt;lock);<BR>&nbsp;put_cpu_ptr(&amp;tvec_bases);<BR>}</P>
<P>static int timer_cpu_notify(struct notifier_block *self,<BR>&nbsp;&nbsp;&nbsp;&nbsp;unsigned long action, void *hcpu)<BR>{<BR>&nbsp;switch (action) {<BR>&nbsp;case CPU_DEAD:<BR>&nbsp;case CPU_DEAD_FROZEN:<BR>&nbsp;&nbsp;migrate_timers((long)hcpu);<BR>&nbsp;&nbsp;break;<BR>&nbsp;default:<BR>&nbsp;&nbsp;break;<BR>&nbsp;}</P>
<P>&nbsp;return NOTIFY_OK;<BR>}</P>
<P>static inline void timer_register_cpu_notifier(void)<BR>{<BR>&nbsp;cpu_notifier(timer_cpu_notify, 0);<BR>}<BR>#else<BR>static inline void timer_register_cpu_notifier(void) { }<BR>#endif /* CONFIG_HOTPLUG_CPU */</P>
<P>static void __init init_timer_cpu(int cpu)<BR>{<BR>&nbsp;struct tvec_base *base = per_cpu_ptr(&amp;tvec_bases, cpu);</P>
<P>&nbsp;base-&gt;cpu = cpu;<BR>&nbsp;spin_lock_init(&amp;base-&gt;lock);</P>
<P>&nbsp;base-&gt;timer_jiffies = jiffies;<BR>&nbsp;base-&gt;next_timer = base-&gt;timer_jiffies;<BR>}</P>
<P>static void __init init_timer_cpus(void)<BR>{<BR>&nbsp;int cpu;</P>
<P>&nbsp;for_each_possible_cpu(cpu)<BR>&nbsp;&nbsp;init_timer_cpu(cpu);<BR>}</P>
<P>void __init init_timers(void)<BR>{<BR>&nbsp;init_timer_cpus();<BR>&nbsp;init_timer_stats();<BR>&nbsp;timer_register_cpu_notifier();<BR>&nbsp;open_softirq(TIMER_SOFTIRQ, run_timer_softirq);<BR>}</P>
<P>/**<BR>&nbsp;* msleep - sleep safely even with waitqueue interruptions<BR>&nbsp;* @msecs: Time in milliseconds to sleep for<BR>&nbsp;*/<BR>void msleep(unsigned int msecs)<BR>{<BR>&nbsp;unsigned long timeout = msecs_to_jiffies(msecs) + 1;</P>
<P>&nbsp;while (timeout)<BR>&nbsp;&nbsp;timeout = schedule_timeout_uninterruptible(timeout);<BR>}</P>
<P>EXPORT_SYMBOL(msleep);</P>
<P>/**<BR>&nbsp;* msleep_interruptible - sleep waiting for signals<BR>&nbsp;* @msecs: Time in milliseconds to sleep for<BR>&nbsp;*/<BR>unsigned long msleep_interruptible(unsigned int msecs)<BR>{<BR>&nbsp;unsigned long timeout = msecs_to_jiffies(msecs) + 1;</P>
<P>&nbsp;while (timeout &amp;&amp; !signal_pending(current))<BR>&nbsp;&nbsp;timeout = schedule_timeout_interruptible(timeout);<BR>&nbsp;return jiffies_to_msecs(timeout);<BR>}</P>
<P>EXPORT_SYMBOL(msleep_interruptible);</P>
<P>static void __sched do_usleep_range(unsigned long min, unsigned long max)<BR>{<BR>&nbsp;ktime_t kmin;<BR>&nbsp;u64 delta;</P>
<P>&nbsp;kmin = ktime_set(0, min * NSEC_PER_USEC);<BR>&nbsp;delta = (u64)(max - min) * NSEC_PER_USEC;<BR>&nbsp;schedule_hrtimeout_range(&amp;kmin, delta, HRTIMER_MODE_REL);<BR>}</P>
<P>/**<BR>&nbsp;* usleep_range - Drop in replacement for udelay where wakeup is flexible<BR>&nbsp;* @min: Minimum time in usecs to sleep<BR>&nbsp;* @max: Maximum time in usecs to sleep<BR>&nbsp;*/<BR>void __sched usleep_range(unsigned long min, unsigned long max)<BR>{<BR>&nbsp;__set_current_state(TASK_UNINTERRUPTIBLE);<BR>&nbsp;do_usleep_range(min, max);<BR>}<BR>EXPORT_SYMBOL(usleep_range);