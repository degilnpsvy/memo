arch/x86/kernel/trap.c 
<P></P>
<P>/*<BR>&nbsp;*&nbsp; Copyright (C) 1991, 1992&nbsp; Linus Torvalds<BR>&nbsp;*&nbsp; Copyright (C) 2000, 2001, 2002 Andi Kleen, SuSE Labs<BR>&nbsp;*<BR>&nbsp;*&nbsp; Pentium III FXSR, SSE support<BR>&nbsp;*&nbsp;Gareth Hughes &lt;<A href="mailto:gareth@valinux.com">gareth@valinux.com</A>&gt;, May 2000<BR>&nbsp;*/</P>
<P></P>
<P>/*<BR>&nbsp;* Handle hardware traps and faults.<BR>&nbsp;*/</P>
<P>#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt</P>
<P>#include &lt;linux/context_tracking.h&gt;<BR>#include &lt;linux/interrupt.h&gt;<BR>#include &lt;linux/kallsyms.h&gt;<BR>#include &lt;linux/spinlock.h&gt;<BR>#include &lt;linux/kprobes.h&gt;<BR>#include &lt;linux/uaccess.h&gt;<BR>#include &lt;linux/kdebug.h&gt;<BR>#include &lt;linux/kgdb.h&gt;<BR>#include &lt;linux/kernel.h&gt;<BR>#include &lt;linux/module.h&gt;<BR>#include &lt;linux/ptrace.h&gt;<BR>#include &lt;linux/uprobes.h&gt;<BR>#include &lt;linux/string.h&gt;<BR>#include &lt;linux/delay.h&gt;<BR>#include &lt;linux/errno.h&gt;<BR>#include &lt;linux/kexec.h&gt;<BR>#include &lt;linux/sched.h&gt;<BR>#include &lt;linux/timer.h&gt;<BR>#include &lt;linux/init.h&gt;<BR>#include &lt;linux/bug.h&gt;<BR>#include &lt;linux/nmi.h&gt;<BR>#include &lt;linux/mm.h&gt;<BR>#include &lt;linux/smp.h&gt;<BR>#include &lt;linux/io.h&gt;</P>
<P>#ifdef CONFIG_EISA<BR>#include &lt;linux/ioport.h&gt;<BR>#include &lt;linux/eisa.h&gt;<BR>#endif</P>
<P>#if defined(CONFIG_EDAC)<BR>#include &lt;linux/edac.h&gt;<BR>#endif</P>
<P>#include &lt;asm/kmemcheck.h&gt;<BR>#include &lt;asm/stacktrace.h&gt;<BR>#include &lt;asm/processor.h&gt;<BR>#include &lt;asm/debugreg.h&gt;<BR>#include &lt;linux/atomic.h&gt;<BR>#include &lt;asm/text-patching.h&gt;<BR>#include &lt;asm/ftrace.h&gt;<BR>#include &lt;asm/traps.h&gt;<BR>#include &lt;asm/desc.h&gt;<BR>#include &lt;asm/fpu/internal.h&gt;<BR>#include &lt;asm/mce.h&gt;<BR>#include &lt;asm/fixmap.h&gt;<BR>#include &lt;asm/mach_traps.h&gt;<BR>#include &lt;asm/alternative.h&gt;<BR>#include &lt;asm/fpu/xstate.h&gt;<BR>#include &lt;asm/trace/mpx.h&gt;<BR>#include &lt;asm/mpx.h&gt;<BR>#include &lt;asm/vm86.h&gt;</P>
<P>#ifdef CONFIG_X86_64<BR>#include &lt;asm/x86_init.h&gt;<BR>#include &lt;asm/pgalloc.h&gt;<BR>#include &lt;asm/proto.h&gt;</P>
<P>/* No need to be aligned, but done to keep all IDTs defined the same way. */<BR>gate_desc debug_idt_table[NR_VECTORS] __page_aligned_bss;<BR>#else<BR>#include &lt;asm/processor-flags.h&gt;<BR>#include &lt;asm/setup.h&gt;<BR>#include &lt;asm/proto.h&gt;<BR>#endif</P>
<P>/* Must be page-aligned because the real IDT is used in a fixmap. */<BR>gate_desc idt_table[NR_VECTORS] __page_aligned_bss;</P>
<P>DECLARE_BITMAP(used_vectors, NR_VECTORS);<BR>EXPORT_SYMBOL_GPL(used_vectors);</P>
<P>static inline void cond_local_irq_enable(struct pt_regs *regs)<BR>{<BR>&nbsp;if (regs-&gt;flags &amp; X86_EFLAGS_IF)<BR>&nbsp;&nbsp;local_irq_enable();<BR>}</P>
<P>static inline void cond_local_irq_disable(struct pt_regs *regs)<BR>{<BR>&nbsp;if (regs-&gt;flags &amp; X86_EFLAGS_IF)<BR>&nbsp;&nbsp;local_irq_disable();<BR>}</P>
<P>void ist_enter(struct pt_regs *regs)<BR>{<BR>&nbsp;if (user_mode(regs)) {<BR>&nbsp;&nbsp;RCU_LOCKDEP_WARN(!rcu_is_watching(), "entry code didn't wake RCU");<BR>&nbsp;} else {<BR>&nbsp;&nbsp;/*<BR>&nbsp;&nbsp; * We might have interrupted pretty much anything.&nbsp; In<BR>&nbsp;&nbsp; * fact, if we're a machine check, we can even interrupt<BR>&nbsp;&nbsp; * NMI processing.&nbsp; We don't want in_nmi() to return true,<BR>&nbsp;&nbsp; * but we need to notify RCU.<BR>&nbsp;&nbsp; */<BR>&nbsp;&nbsp;rcu_nmi_enter();<BR>&nbsp;}</P>
<P>&nbsp;/*<BR>&nbsp; * We are atomic because we're on the IST stack; or we're on<BR>&nbsp; * x86_32, in which case we still shouldn't schedule; or we're<BR>&nbsp; * on x86_64 and entered from user mode, in which case we're<BR>&nbsp; * still atomic unless ist_begin_non_atomic is called.<BR>&nbsp; */<BR>&nbsp;preempt_count_add(HARDIRQ_OFFSET);</P>
<P>&nbsp;/* This code is a bit fragile.&nbsp; Test it. */<BR>&nbsp;RCU_LOCKDEP_WARN(!rcu_is_watching(), "ist_enter didn't work");<BR>}</P>
<P>void ist_exit(struct pt_regs *regs)<BR>{<BR>&nbsp;preempt_count_sub(HARDIRQ_OFFSET);</P>
<P>&nbsp;if (!user_mode(regs))<BR>&nbsp;&nbsp;rcu_nmi_exit();<BR>}</P>
<P>/**<BR>&nbsp;* ist_begin_non_atomic() - begin a non-atomic section in an IST exception<BR>&nbsp;* @regs:&nbsp;regs passed to the IST exception handler<BR>&nbsp;*<BR>&nbsp;* IST exception handlers normally cannot schedule.&nbsp; As a special<BR>&nbsp;* exception, if the exception interrupted userspace code (i.e.<BR>&nbsp;* user_mode(regs) would return true) and the exception was not<BR>&nbsp;* a double fault, it can be safe to schedule.&nbsp; ist_begin_non_atomic()<BR>&nbsp;* begins a non-atomic section within an ist_enter()/ist_exit() region.<BR>&nbsp;* Callers are responsible for enabling interrupts themselves inside<BR>&nbsp;* the non-atomic section, and callers must call ist_end_non_atomic()<BR>&nbsp;* before ist_exit().<BR>&nbsp;*/<BR>void ist_begin_non_atomic(struct pt_regs *regs)<BR>{<BR>&nbsp;BUG_ON(!user_mode(regs));</P>
<P>&nbsp;/*<BR>&nbsp; * Sanity check: we need to be on the normal thread stack.&nbsp; This<BR>&nbsp; * will catch asm bugs and any attempt to use ist_preempt_enable<BR>&nbsp; * from double_fault.<BR>&nbsp; */<BR>&nbsp;BUG_ON((unsigned long)(current_top_of_stack() -<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; current_stack_pointer()) &gt;= THREAD_SIZE);</P>
<P>&nbsp;preempt_count_sub(HARDIRQ_OFFSET);<BR>}</P>
<P>/**<BR>&nbsp;* ist_end_non_atomic() - begin a non-atomic section in an IST exception<BR>&nbsp;*<BR>&nbsp;* Ends a non-atomic section started with ist_begin_non_atomic().<BR>&nbsp;*/<BR>void ist_end_non_atomic(void)<BR>{<BR>&nbsp;preempt_count_add(HARDIRQ_OFFSET);<BR>}</P>
<P>static nokprobe_inline int<BR>do_trap_no_signal(struct task_struct *tsk, int trapnr, char *str,<BR>&nbsp;&nbsp;&nbsp; struct pt_regs *regs,&nbsp;long error_code)<BR>{<BR>&nbsp;if (v8086_mode(regs)) {<BR>&nbsp;&nbsp;/*<BR>&nbsp;&nbsp; * Traps 0, 1, 3, 4, and 5 should be forwarded to vm86.<BR>&nbsp;&nbsp; * On nmi (interrupt 2), do_trap should not be called.<BR>&nbsp;&nbsp; */<BR>&nbsp;&nbsp;if (trapnr &lt; X86_TRAP_UD) {<BR>&nbsp;&nbsp;&nbsp;if (!handle_vm86_trap((struct kernel_vm86_regs *) regs,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;error_code, trapnr))<BR>&nbsp;&nbsp;&nbsp;&nbsp;return 0;<BR>&nbsp;&nbsp;}<BR>&nbsp;&nbsp;return -1;<BR>&nbsp;}</P>
<P>&nbsp;if (!user_mode(regs)) {<BR>&nbsp;&nbsp;if (!fixup_exception(regs, trapnr)) {<BR>&nbsp;&nbsp;&nbsp;tsk-&gt;thread.error_code = error_code;<BR>&nbsp;&nbsp;&nbsp;tsk-&gt;thread.trap_nr = trapnr;<BR>&nbsp;&nbsp;&nbsp;die(str, regs, error_code);<BR>&nbsp;&nbsp;}<BR>&nbsp;&nbsp;return 0;<BR>&nbsp;}</P>
<P>&nbsp;return -1;<BR>}</P>
<P>static siginfo_t *fill_trap_info(struct pt_regs *regs, int signr, int trapnr,<BR>&nbsp;&nbsp;&nbsp;&nbsp;siginfo_t *info)<BR>{<BR>&nbsp;unsigned long siaddr;<BR>&nbsp;int sicode;</P>
<P>&nbsp;switch (trapnr) {<BR>&nbsp;default:<BR>&nbsp;&nbsp;return SEND_SIG_PRIV;</P>
<P>&nbsp;case X86_TRAP_DE:<BR>&nbsp;&nbsp;sicode = FPE_INTDIV;<BR>&nbsp;&nbsp;siaddr = uprobe_get_trap_addr(regs);<BR>&nbsp;&nbsp;break;<BR>&nbsp;case X86_TRAP_UD:<BR>&nbsp;&nbsp;sicode = ILL_ILLOPN;<BR>&nbsp;&nbsp;siaddr = uprobe_get_trap_addr(regs);<BR>&nbsp;&nbsp;break;<BR>&nbsp;case X86_TRAP_AC:<BR>&nbsp;&nbsp;sicode = BUS_ADRALN;<BR>&nbsp;&nbsp;siaddr = 0;<BR>&nbsp;&nbsp;break;<BR>&nbsp;}</P>
<P>&nbsp;info-&gt;si_signo = signr;<BR>&nbsp;info-&gt;si_errno = 0;<BR>&nbsp;info-&gt;si_code = sicode;<BR>&nbsp;info-&gt;si_addr = (void __user *)siaddr;<BR>&nbsp;return info;<BR>}</P>
<P>static void<BR>do_trap(int trapnr, int signr, char *str, struct pt_regs *regs,<BR>&nbsp;long error_code, siginfo_t *info)<BR>{<BR>&nbsp;struct task_struct *tsk = current;</P>
<P><BR>&nbsp;if (!do_trap_no_signal(tsk, trapnr, str, regs, error_code))<BR>&nbsp;&nbsp;return;<BR>&nbsp;/*<BR>&nbsp; * We want error_code and trap_nr set for userspace faults and<BR>&nbsp; * kernelspace faults which result in die(), but not<BR>&nbsp; * kernelspace faults which are fixed up.&nbsp; die() gives the<BR>&nbsp; * process no chance to handle the signal and notice the<BR>&nbsp; * kernel fault information, so that won't result in polluting<BR>&nbsp; * the information about previously queued, but not yet<BR>&nbsp; * delivered, faults.&nbsp; See also do_general_protection below.<BR>&nbsp; */<BR>&nbsp;tsk-&gt;thread.error_code = error_code;<BR>&nbsp;tsk-&gt;thread.trap_nr = trapnr;</P>
<P>&nbsp;if (show_unhandled_signals &amp;&amp; unhandled_signal(tsk, signr) &amp;&amp;<BR>&nbsp;&nbsp;&nbsp;&nbsp; printk_ratelimit()) {<BR>&nbsp;&nbsp;pr_info("%s[%d] trap %s ip:%lx sp:%lx error:%lx",<BR>&nbsp;&nbsp;&nbsp;tsk-&gt;comm, tsk-&gt;pid, str,<BR>&nbsp;&nbsp;&nbsp;regs-&gt;ip, regs-&gt;sp, error_code);<BR>&nbsp;&nbsp;print_vma_addr(" in ", regs-&gt;ip);<BR>&nbsp;&nbsp;pr_cont("\n");<BR>&nbsp;}</P>
<P>&nbsp;force_sig_info(signr, info ?: SEND_SIG_PRIV, tsk);<BR>}<BR>NOKPROBE_SYMBOL(do_trap);</P>
<P>static void do_error_trap(struct pt_regs *regs, long error_code, char *str,<BR>&nbsp;&nbsp;&nbsp;&nbsp; unsigned long trapnr, int signr)<BR>{<BR>&nbsp;siginfo_t info;</P>
<P>&nbsp;RCU_LOCKDEP_WARN(!rcu_is_watching(), "entry code didn't wake RCU");</P>
<P>&nbsp;if (notify_die(DIE_TRAP, str, regs, error_code, trapnr, signr) !=<BR>&nbsp;&nbsp;&nbsp;NOTIFY_STOP) {<BR>&nbsp;&nbsp;cond_local_irq_enable(regs);<BR>&nbsp;&nbsp;do_trap(trapnr, signr, str, regs, error_code,<BR>&nbsp;&nbsp;&nbsp;fill_trap_info(regs, signr, trapnr, &amp;info));<BR>&nbsp;}<BR>}</P>
<P>#define DO_ERROR(trapnr, signr, str, name)&nbsp;&nbsp;&nbsp;&nbsp;\<BR>dotraplinkage void do_##name(struct pt_regs *regs, long error_code)&nbsp;\<BR>{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\<BR>&nbsp;do_error_trap(regs, error_code, str, trapnr, signr);&nbsp;&nbsp;\<BR>}</P>
<P>DO_ERROR(X86_TRAP_DE,&nbsp;&nbsp;&nbsp;&nbsp; SIGFPE,&nbsp; "divide error",&nbsp;&nbsp;divide_error)<BR>DO_ERROR(X86_TRAP_OF,&nbsp;&nbsp;&nbsp;&nbsp; SIGSEGV, "overflow",&nbsp;&nbsp;&nbsp;overflow)<BR>DO_ERROR(X86_TRAP_UD,&nbsp;&nbsp;&nbsp;&nbsp; SIGILL,&nbsp; "invalid opcode",&nbsp;&nbsp;invalid_op)<BR>DO_ERROR(X86_TRAP_OLD_MF, SIGFPE,&nbsp; "coprocessor segment overrun",coprocessor_segment_overrun)<BR>DO_ERROR(X86_TRAP_TS,&nbsp;&nbsp;&nbsp;&nbsp; SIGSEGV, "invalid TSS",&nbsp;&nbsp;invalid_TSS)<BR>DO_ERROR(X86_TRAP_NP,&nbsp;&nbsp;&nbsp;&nbsp; SIGBUS,&nbsp; "segment not present",&nbsp;segment_not_present)<BR>DO_ERROR(X86_TRAP_SS,&nbsp;&nbsp;&nbsp;&nbsp; SIGBUS,&nbsp; "stack segment",&nbsp;&nbsp;stack_segment)<BR>DO_ERROR(X86_TRAP_AC,&nbsp;&nbsp;&nbsp;&nbsp; SIGBUS,&nbsp; "alignment check",&nbsp;&nbsp;alignment_check)</P>
<P>#ifdef CONFIG_X86_64<BR>/* Runs on IST stack */<BR>dotraplinkage void do_double_fault(struct pt_regs *regs, long error_code)<BR>{<BR>&nbsp;static const char str[] = "double fault";<BR>&nbsp;struct task_struct *tsk = current;</P>
<P>#ifdef CONFIG_X86_ESPFIX64<BR>&nbsp;extern unsigned char native_irq_return_iret[];</P>
<P>&nbsp;/*<BR>&nbsp; * If IRET takes a non-IST fault on the espfix64 stack, then we<BR>&nbsp; * end up promoting it to a doublefault.&nbsp; In that case, modify<BR>&nbsp; * the stack to make it look like we just entered the #GP<BR>&nbsp; * handler from user space, similar to bad_iret.<BR>&nbsp; *<BR>&nbsp; * No need for ist_enter here because we don't use RCU.<BR>&nbsp; */<BR>&nbsp;if (((long)regs-&gt;sp &gt;&gt; PGDIR_SHIFT) == ESPFIX_PGD_ENTRY &amp;&amp;<BR>&nbsp;&nbsp;regs-&gt;cs == __KERNEL_CS &amp;&amp;<BR>&nbsp;&nbsp;regs-&gt;ip == (unsigned long)native_irq_return_iret)<BR>&nbsp;{<BR>&nbsp;&nbsp;struct pt_regs *normal_regs = task_pt_regs(current);</P>
<P>&nbsp;&nbsp;/* Fake a #GP(0) from userspace. */<BR>&nbsp;&nbsp;memmove(&amp;normal_regs-&gt;ip, (void *)regs-&gt;sp, 5*8);<BR>&nbsp;&nbsp;normal_regs-&gt;orig_ax = 0;&nbsp; /* Missing (lost) #GP error code */<BR>&nbsp;&nbsp;regs-&gt;ip = (unsigned long)general_protection;<BR>&nbsp;&nbsp;regs-&gt;sp = (unsigned long)&amp;normal_regs-&gt;orig_ax;</P>
<P>&nbsp;&nbsp;return;<BR>&nbsp;}<BR>#endif</P>
<P>&nbsp;ist_enter(regs);<BR>&nbsp;notify_die(DIE_TRAP, str, regs, error_code, X86_TRAP_DF, SIGSEGV);</P>
<P>&nbsp;tsk-&gt;thread.error_code = error_code;<BR>&nbsp;tsk-&gt;thread.trap_nr = X86_TRAP_DF;</P>
<P>#ifdef CONFIG_DOUBLEFAULT<BR>&nbsp;df_debug(regs, error_code);<BR>#endif<BR>&nbsp;/*<BR>&nbsp; * This is always a kernel trap and never fixable (and thus must<BR>&nbsp; * never return).<BR>&nbsp; */<BR>&nbsp;for (;;)<BR>&nbsp;&nbsp;die(str, regs, error_code);<BR>}<BR>#endif</P>
<P>dotraplinkage void do_bounds(struct pt_regs *regs, long error_code)<BR>{<BR>&nbsp;const struct mpx_bndcsr *bndcsr;<BR>&nbsp;siginfo_t *info;</P>
<P>&nbsp;RCU_LOCKDEP_WARN(!rcu_is_watching(), "entry code didn't wake RCU");<BR>&nbsp;if (notify_die(DIE_TRAP, "bounds", regs, error_code,<BR>&nbsp;&nbsp;&nbsp;X86_TRAP_BR, SIGSEGV) == NOTIFY_STOP)<BR>&nbsp;&nbsp;return;<BR>&nbsp;cond_local_irq_enable(regs);</P>
<P>&nbsp;if (!user_mode(regs))<BR>&nbsp;&nbsp;die("bounds", regs, error_code);</P>
<P>&nbsp;if (!cpu_feature_enabled(X86_FEATURE_MPX)) {<BR>&nbsp;&nbsp;/* The exception is not from Intel MPX */<BR>&nbsp;&nbsp;goto exit_trap;<BR>&nbsp;}</P>
<P>&nbsp;/*<BR>&nbsp; * We need to look at BNDSTATUS to resolve this exception.<BR>&nbsp; * A NULL here might mean that it is in its 'init state',<BR>&nbsp; * which is all zeros which indicates MPX was not<BR>&nbsp; * responsible for the exception.<BR>&nbsp; */<BR>&nbsp;bndcsr = get_xsave_field_ptr(XFEATURE_MASK_BNDCSR);<BR>&nbsp;if (!bndcsr)<BR>&nbsp;&nbsp;goto exit_trap;</P>
<P>&nbsp;trace_bounds_exception_mpx(bndcsr);<BR>&nbsp;/*<BR>&nbsp; * The error code field of the BNDSTATUS register communicates status<BR>&nbsp; * information of a bound range exception #BR or operation involving<BR>&nbsp; * bound directory.<BR>&nbsp; */<BR>&nbsp;switch (bndcsr-&gt;bndstatus &amp; MPX_BNDSTA_ERROR_CODE) {<BR>&nbsp;case 2:&nbsp;/* Bound directory has invalid entry. */<BR>&nbsp;&nbsp;if (mpx_handle_bd_fault())<BR>&nbsp;&nbsp;&nbsp;goto exit_trap;<BR>&nbsp;&nbsp;break; /* Success, it was handled */<BR>&nbsp;case 1: /* Bound violation. */<BR>&nbsp;&nbsp;info = mpx_generate_siginfo(regs);<BR>&nbsp;&nbsp;if (IS_ERR(info)) {<BR>&nbsp;&nbsp;&nbsp;/*<BR>&nbsp;&nbsp;&nbsp; * We failed to decode the MPX instruction.&nbsp; Act as if<BR>&nbsp;&nbsp;&nbsp; * the exception was not caused by MPX.<BR>&nbsp;&nbsp;&nbsp; */<BR>&nbsp;&nbsp;&nbsp;goto exit_trap;<BR>&nbsp;&nbsp;}<BR>&nbsp;&nbsp;/*<BR>&nbsp;&nbsp; * Success, we decoded the instruction and retrieved<BR>&nbsp;&nbsp; * an 'info' containing the address being accessed<BR>&nbsp;&nbsp; * which caused the exception.&nbsp; This information<BR>&nbsp;&nbsp; * allows and application to possibly handle the<BR>&nbsp;&nbsp; * #BR exception itself.<BR>&nbsp;&nbsp; */<BR>&nbsp;&nbsp;do_trap(X86_TRAP_BR, SIGSEGV, "bounds", regs, error_code, info);<BR>&nbsp;&nbsp;kfree(info);<BR>&nbsp;&nbsp;break;<BR>&nbsp;case 0: /* No exception caused by Intel MPX operations. */<BR>&nbsp;&nbsp;goto exit_trap;<BR>&nbsp;default:<BR>&nbsp;&nbsp;die("bounds", regs, error_code);<BR>&nbsp;}</P>
<P>&nbsp;return;</P>
<P>exit_trap:<BR>&nbsp;/*<BR>&nbsp; * This path out is for all the cases where we could not<BR>&nbsp; * handle the exception in some way (like allocating a<BR>&nbsp; * table or telling userspace about it.&nbsp; We will also end<BR>&nbsp; * up here if the kernel has MPX turned off at compile<BR>&nbsp; * time..<BR>&nbsp; */<BR>&nbsp;do_trap(X86_TRAP_BR, SIGSEGV, "bounds", regs, error_code, NULL);<BR>}</P>
<P>dotraplinkage void<BR>do_general_protection(struct pt_regs *regs, long error_code)<BR>{<BR>&nbsp;struct task_struct *tsk;</P>
<P>&nbsp;RCU_LOCKDEP_WARN(!rcu_is_watching(), "entry code didn't wake RCU");<BR>&nbsp;cond_local_irq_enable(regs);</P>
<P>&nbsp;if (v8086_mode(regs)) {<BR>&nbsp;&nbsp;local_irq_enable();<BR>&nbsp;&nbsp;handle_vm86_fault((struct kernel_vm86_regs *) regs, error_code);<BR>&nbsp;&nbsp;return;<BR>&nbsp;}</P>
<P>&nbsp;tsk = current;<BR>&nbsp;if (!user_mode(regs)) {<BR>&nbsp;&nbsp;if (fixup_exception(regs, X86_TRAP_GP))<BR>&nbsp;&nbsp;&nbsp;return;</P>
<P>&nbsp;&nbsp;tsk-&gt;thread.error_code = error_code;<BR>&nbsp;&nbsp;tsk-&gt;thread.trap_nr = X86_TRAP_GP;<BR>&nbsp;&nbsp;if (notify_die(DIE_GPF, "general protection fault", regs, error_code,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; X86_TRAP_GP, SIGSEGV) != NOTIFY_STOP)<BR>&nbsp;&nbsp;&nbsp;die("general protection fault", regs, error_code);<BR>&nbsp;&nbsp;return;<BR>&nbsp;}</P>
<P>&nbsp;tsk-&gt;thread.error_code = error_code;<BR>&nbsp;tsk-&gt;thread.trap_nr = X86_TRAP_GP;</P>
<P>&nbsp;if (show_unhandled_signals &amp;&amp; unhandled_signal(tsk, SIGSEGV) &amp;&amp;<BR>&nbsp;&nbsp;&nbsp;printk_ratelimit()) {<BR>&nbsp;&nbsp;pr_info("%s[%d] general protection ip:%lx sp:%lx error:%lx",<BR>&nbsp;&nbsp;&nbsp;tsk-&gt;comm, task_pid_nr(tsk),<BR>&nbsp;&nbsp;&nbsp;regs-&gt;ip, regs-&gt;sp, error_code);<BR>&nbsp;&nbsp;print_vma_addr(" in ", regs-&gt;ip);<BR>&nbsp;&nbsp;pr_cont("\n");<BR>&nbsp;}</P>
<P>&nbsp;force_sig_info(SIGSEGV, SEND_SIG_PRIV, tsk);<BR>}<BR>NOKPROBE_SYMBOL(do_general_protection);</P>
<P>/* May run on IST stack. */<BR>dotraplinkage void notrace do_int3(struct pt_regs *regs, long error_code)<BR>{<BR>#ifdef CONFIG_DYNAMIC_FTRACE<BR>&nbsp;/*<BR>&nbsp; * ftrace must be first, everything else may cause a recursive crash.<BR>&nbsp; * See note by declaration of modifying_ftrace_code in ftrace.c<BR>&nbsp; */<BR>&nbsp;if (unlikely(atomic_read(&amp;modifying_ftrace_code)) &amp;&amp;<BR>&nbsp;&nbsp;&nbsp;&nbsp; ftrace_int3_handler(regs))<BR>&nbsp;&nbsp;return;<BR>#endif<BR>&nbsp;if (poke_int3_handler(regs))<BR>&nbsp;&nbsp;return;</P>
<P>&nbsp;ist_enter(regs);<BR>&nbsp;RCU_LOCKDEP_WARN(!rcu_is_watching(), "entry code didn't wake RCU");<BR>#ifdef CONFIG_KGDB_LOW_LEVEL_TRAP<BR>&nbsp;if (kgdb_ll_trap(DIE_INT3, "int3", regs, error_code, X86_TRAP_BP,<BR>&nbsp;&nbsp;&nbsp;&nbsp;SIGTRAP) == NOTIFY_STOP)<BR>&nbsp;&nbsp;goto exit;<BR>#endif /* CONFIG_KGDB_LOW_LEVEL_TRAP */</P>
<P>#ifdef CONFIG_KPROBES<BR>&nbsp;if (kprobe_int3_handler(regs))<BR>&nbsp;&nbsp;goto exit;<BR>#endif</P>
<P>&nbsp;if (notify_die(DIE_INT3, "int3", regs, error_code, X86_TRAP_BP,<BR>&nbsp;&nbsp;&nbsp;SIGTRAP) == NOTIFY_STOP)<BR>&nbsp;&nbsp;goto exit;</P>
<P>&nbsp;/*<BR>&nbsp; * Let others (NMI) know that the debug stack is in use<BR>&nbsp; * as we may switch to the interrupt stack.<BR>&nbsp; */<BR>&nbsp;debug_stack_usage_inc();<BR>&nbsp;preempt_disable();<BR>&nbsp;cond_local_irq_enable(regs);<BR>&nbsp;do_trap(X86_TRAP_BP, SIGTRAP, "int3", regs, error_code, NULL);<BR>&nbsp;cond_local_irq_disable(regs);<BR>&nbsp;preempt_enable_no_resched();<BR>&nbsp;debug_stack_usage_dec();<BR>exit:<BR>&nbsp;ist_exit(regs);<BR>}<BR>NOKPROBE_SYMBOL(do_int3);</P>
<P>#ifdef CONFIG_X86_64<BR>/*<BR>&nbsp;* Help handler running on IST stack to switch off the IST stack if the<BR>&nbsp;* interrupted code was in user mode. The actual stack switch is done in<BR>&nbsp;* entry_64.S<BR>&nbsp;*/<BR>asmlinkage __visible notrace struct pt_regs *sync_regs(struct pt_regs *eregs)<BR>{<BR>&nbsp;struct pt_regs *regs = task_pt_regs(current);<BR>&nbsp;*regs = *eregs;<BR>&nbsp;return regs;<BR>}<BR>NOKPROBE_SYMBOL(sync_regs);</P>
<P>struct bad_iret_stack {<BR>&nbsp;void *error_entry_ret;<BR>&nbsp;struct pt_regs regs;<BR>};</P>
<P>asmlinkage __visible notrace<BR>struct bad_iret_stack *fixup_bad_iret(struct bad_iret_stack *s)<BR>{<BR>&nbsp;/*<BR>&nbsp; * This is called from entry_64.S early in handling a fault<BR>&nbsp; * caused by a bad iret to user mode.&nbsp; To handle the fault<BR>&nbsp; * correctly, we want move our stack frame to task_pt_regs<BR>&nbsp; * and we want to pretend that the exception came from the<BR>&nbsp; * iret target.<BR>&nbsp; */<BR>&nbsp;struct bad_iret_stack *new_stack =<BR>&nbsp;&nbsp;container_of(task_pt_regs(current),<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; struct bad_iret_stack, regs);</P>
<P>&nbsp;/* Copy the IRET target to the new stack. */<BR>&nbsp;memmove(&amp;new_stack-&gt;regs.ip, (void *)s-&gt;regs.sp, 5*8);</P>
<P>&nbsp;/* Copy the remainder of the stack from the current stack. */<BR>&nbsp;memmove(new_stack, s, offsetof(struct bad_iret_stack, regs.ip));</P>
<P>&nbsp;BUG_ON(!user_mode(&amp;new_stack-&gt;regs));<BR>&nbsp;return new_stack;<BR>}<BR>NOKPROBE_SYMBOL(fixup_bad_iret);<BR>#endif</P>
<P>static bool is_sysenter_singlestep(struct pt_regs *regs)<BR>{<BR>&nbsp;/*<BR>&nbsp; * We don't try for precision here.&nbsp; If we're anywhere in the region of<BR>&nbsp; * code that can be single-stepped in the SYSENTER entry path, then<BR>&nbsp; * assume that this is a useless single-step trap due to SYSENTER<BR>&nbsp; * being invoked with TF set.&nbsp; (We don't know in advance exactly<BR>&nbsp; * which instructions will be hit because BTF could plausibly<BR>&nbsp; * be set.)<BR>&nbsp; */<BR>#ifdef CONFIG_X86_32<BR>&nbsp;return (regs-&gt;ip - (unsigned long)__begin_SYSENTER_singlestep_region) &lt;<BR>&nbsp;&nbsp;(unsigned long)__end_SYSENTER_singlestep_region -<BR>&nbsp;&nbsp;(unsigned long)__begin_SYSENTER_singlestep_region;<BR>#elif defined(CONFIG_IA32_EMULATION)<BR>&nbsp;return (regs-&gt;ip - (unsigned long)entry_SYSENTER_compat) &lt;<BR>&nbsp;&nbsp;(unsigned long)__end_entry_SYSENTER_compat -<BR>&nbsp;&nbsp;(unsigned long)entry_SYSENTER_compat;<BR>#else<BR>&nbsp;return false;<BR>#endif<BR>}</P>
<P>/*<BR>&nbsp;* Our handling of the processor debug registers is non-trivial.<BR>&nbsp;* We do not clear them on entry and exit from the kernel. Therefore<BR>&nbsp;* it is possible to get a watchpoint trap here from inside the kernel.<BR>&nbsp;* However, the code in ./ptrace.c has ensured that the user can<BR>&nbsp;* only set watchpoints on userspace addresses. Therefore the in-kernel<BR>&nbsp;* watchpoint trap can only occur in code which is reading/writing<BR>&nbsp;* from user space. Such code must not hold kernel locks (since it<BR>&nbsp;* can equally take a page fault), therefore it is safe to call<BR>&nbsp;* force_sig_info even though that claims and releases locks.<BR>&nbsp;*<BR>&nbsp;* Code in ./signal.c ensures that the debug control register<BR>&nbsp;* is restored before we deliver any signal, and therefore that<BR>&nbsp;* user code runs with the correct debug control register even though<BR>&nbsp;* we clear it here.<BR>&nbsp;*<BR>&nbsp;* Being careful here means that we don't have to be as careful in a<BR>&nbsp;* lot of more complicated places (task switching can be a bit lazy<BR>&nbsp;* about restoring all the debug state, and ptrace doesn't have to<BR>&nbsp;* find every occurrence of the TF bit that could be saved away even<BR>&nbsp;* by user code)<BR>&nbsp;*<BR>&nbsp;* May run on IST stack.<BR>&nbsp;*/<BR>dotraplinkage void do_debug(struct pt_regs *regs, long error_code)<BR>{<BR>&nbsp;struct task_struct *tsk = current;<BR>&nbsp;int user_icebp = 0;<BR>&nbsp;unsigned long dr6;<BR>&nbsp;int si_code;</P>
<P>&nbsp;ist_enter(regs);</P>
<P>&nbsp;get_debugreg(dr6, 6);<BR>&nbsp;/*<BR>&nbsp; * The Intel SDM says:<BR>&nbsp; *<BR>&nbsp; *&nbsp;&nbsp; Certain debug exceptions may clear bits 0-3. The remaining<BR>&nbsp; *&nbsp;&nbsp; contents of the DR6 register are never cleared by the<BR>&nbsp; *&nbsp;&nbsp; processor. To avoid confusion in identifying debug<BR>&nbsp; *&nbsp;&nbsp; exceptions, debug handlers should clear the register before<BR>&nbsp; *&nbsp;&nbsp; returning to the interrupted task.<BR>&nbsp; *<BR>&nbsp; * Keep it simple: clear DR6 immediately.<BR>&nbsp; */<BR>&nbsp;set_debugreg(0, 6);</P>
<P>&nbsp;/* Filter out all the reserved bits which are preset to 1 */<BR>&nbsp;dr6 &amp;= ~DR6_RESERVED;</P>
<P>&nbsp;/*<BR>&nbsp; * The SDM says "The processor clears the BTF flag when it<BR>&nbsp; * generates a debug exception."&nbsp; Clear TIF_BLOCKSTEP to keep<BR>&nbsp; * TIF_BLOCKSTEP in sync with the hardware BTF flag.<BR>&nbsp; */<BR>&nbsp;clear_tsk_thread_flag(tsk, TIF_BLOCKSTEP);</P>
<P>&nbsp;if (unlikely(!user_mode(regs) &amp;&amp; (dr6 &amp; DR_STEP) &amp;&amp;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; is_sysenter_singlestep(regs))) {<BR>&nbsp;&nbsp;dr6 &amp;= ~DR_STEP;<BR>&nbsp;&nbsp;if (!dr6)<BR>&nbsp;&nbsp;&nbsp;goto exit;<BR>&nbsp;&nbsp;/*<BR>&nbsp;&nbsp; * else we might have gotten a single-step trap and hit a<BR>&nbsp;&nbsp; * watchpoint at the same time, in which case we should fall<BR>&nbsp;&nbsp; * through and handle the watchpoint.<BR>&nbsp;&nbsp; */<BR>&nbsp;}</P>
<P>&nbsp;/*<BR>&nbsp; * If dr6 has no reason to give us about the origin of this trap,<BR>&nbsp; * then it's very likely the result of an icebp/int01 trap.<BR>&nbsp; * User wants a sigtrap for that.<BR>&nbsp; */<BR>&nbsp;if (!dr6 &amp;&amp; user_mode(regs))<BR>&nbsp;&nbsp;user_icebp = 1;</P>
<P>&nbsp;/* Catch kmemcheck conditions! */<BR>&nbsp;if ((dr6 &amp; DR_STEP) &amp;&amp; kmemcheck_trap(regs))<BR>&nbsp;&nbsp;goto exit;</P>
<P>&nbsp;/* Store the virtualized DR6 value */<BR>&nbsp;tsk-&gt;thread.debugreg6 = dr6;</P>
<P>#ifdef CONFIG_KPROBES<BR>&nbsp;if (kprobe_debug_handler(regs))<BR>&nbsp;&nbsp;goto exit;<BR>#endif</P>
<P>&nbsp;if (notify_die(DIE_DEBUG, "debug", regs, (long)&amp;dr6, error_code,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;SIGTRAP) == NOTIFY_STOP)<BR>&nbsp;&nbsp;goto exit;</P>
<P>&nbsp;/*<BR>&nbsp; * Let others (NMI) know that the debug stack is in use<BR>&nbsp; * as we may switch to the interrupt stack.<BR>&nbsp; */<BR>&nbsp;debug_stack_usage_inc();</P>
<P>&nbsp;/* It's safe to allow irq's after DR6 has been saved */<BR>&nbsp;preempt_disable();<BR>&nbsp;cond_local_irq_enable(regs);</P>
<P>&nbsp;if (v8086_mode(regs)) {<BR>&nbsp;&nbsp;handle_vm86_trap((struct kernel_vm86_regs *) regs, error_code,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;X86_TRAP_DB);<BR>&nbsp;&nbsp;cond_local_irq_disable(regs);<BR>&nbsp;&nbsp;preempt_enable_no_resched();<BR>&nbsp;&nbsp;debug_stack_usage_dec();<BR>&nbsp;&nbsp;goto exit;<BR>&nbsp;}</P>
<P>&nbsp;if (WARN_ON_ONCE((dr6 &amp; DR_STEP) &amp;&amp; !user_mode(regs))) {<BR>&nbsp;&nbsp;/*<BR>&nbsp;&nbsp; * Historical junk that used to handle SYSENTER single-stepping.<BR>&nbsp;&nbsp; * This should be unreachable now.&nbsp; If we survive for a while<BR>&nbsp;&nbsp; * without anyone hitting this warning, we'll turn this into<BR>&nbsp;&nbsp; * an oops.<BR>&nbsp;&nbsp; */<BR>&nbsp;&nbsp;tsk-&gt;thread.debugreg6 &amp;= ~DR_STEP;<BR>&nbsp;&nbsp;set_tsk_thread_flag(tsk, TIF_SINGLESTEP);<BR>&nbsp;&nbsp;regs-&gt;flags &amp;= ~X86_EFLAGS_TF;<BR>&nbsp;}<BR>&nbsp;si_code = get_si_code(tsk-&gt;thread.debugreg6);<BR>&nbsp;if (tsk-&gt;thread.debugreg6 &amp; (DR_STEP | DR_TRAP_BITS) || user_icebp)<BR>&nbsp;&nbsp;send_sigtrap(tsk, regs, error_code, si_code);<BR>&nbsp;cond_local_irq_disable(regs);<BR>&nbsp;preempt_enable_no_resched();<BR>&nbsp;debug_stack_usage_dec();</P>
<P>exit:<BR>#if defined(CONFIG_X86_32)<BR>&nbsp;/*<BR>&nbsp; * This is the most likely code path that involves non-trivial use<BR>&nbsp; * of the SYSENTER stack.&nbsp; Check that we haven't overrun it.<BR>&nbsp; */<BR>&nbsp;WARN(this_cpu_read(cpu_tss.SYSENTER_stack_canary) != STACK_END_MAGIC,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "Overran or corrupted SYSENTER stack\n");<BR>#endif<BR>&nbsp;ist_exit(regs);<BR>}<BR>NOKPROBE_SYMBOL(do_debug);</P>
<P>/*<BR>&nbsp;* Note that we play around with the 'TS' bit in an attempt to get<BR>&nbsp;* the correct behaviour even in the presence of the asynchronous<BR>&nbsp;* IRQ13 behaviour<BR>&nbsp;*/<BR>static void math_error(struct pt_regs *regs, int error_code, int trapnr)<BR>{<BR>&nbsp;struct task_struct *task = current;<BR>&nbsp;struct fpu *fpu = &amp;task-&gt;thread.fpu;<BR>&nbsp;siginfo_t info;<BR>&nbsp;char *str = (trapnr == X86_TRAP_MF) ? "fpu exception" :<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"simd exception";</P>
<P>&nbsp;if (notify_die(DIE_TRAP, str, regs, error_code, trapnr, SIGFPE) == NOTIFY_STOP)<BR>&nbsp;&nbsp;return;<BR>&nbsp;cond_local_irq_enable(regs);</P>
<P>&nbsp;if (!user_mode(regs)) {<BR>&nbsp;&nbsp;if (!fixup_exception(regs, trapnr)) {<BR>&nbsp;&nbsp;&nbsp;task-&gt;thread.error_code = error_code;<BR>&nbsp;&nbsp;&nbsp;task-&gt;thread.trap_nr = trapnr;<BR>&nbsp;&nbsp;&nbsp;die(str, regs, error_code);<BR>&nbsp;&nbsp;}<BR>&nbsp;&nbsp;return;<BR>&nbsp;}</P>
<P>&nbsp;/*<BR>&nbsp; * Save the info for the exception handler and clear the error.<BR>&nbsp; */<BR>&nbsp;fpu__save(fpu);</P>
<P>&nbsp;task-&gt;thread.trap_nr&nbsp;= trapnr;<BR>&nbsp;task-&gt;thread.error_code = error_code;<BR>&nbsp;info.si_signo&nbsp;&nbsp;= SIGFPE;<BR>&nbsp;info.si_errno&nbsp;&nbsp;= 0;<BR>&nbsp;info.si_addr&nbsp;&nbsp;= (void __user *)uprobe_get_trap_addr(regs);</P>
<P>&nbsp;info.si_code = fpu__exception_code(fpu, trapnr);</P>
<P>&nbsp;/* Retry when we get spurious exceptions: */<BR>&nbsp;if (!info.si_code)<BR>&nbsp;&nbsp;return;</P>
<P>&nbsp;force_sig_info(SIGFPE, &amp;info, task);<BR>}</P>
<P>dotraplinkage void do_coprocessor_error(struct pt_regs *regs, long error_code)<BR>{<BR>&nbsp;RCU_LOCKDEP_WARN(!rcu_is_watching(), "entry code didn't wake RCU");<BR>&nbsp;math_error(regs, error_code, X86_TRAP_MF);<BR>}</P>
<P>dotraplinkage void<BR>do_simd_coprocessor_error(struct pt_regs *regs, long error_code)<BR>{<BR>&nbsp;RCU_LOCKDEP_WARN(!rcu_is_watching(), "entry code didn't wake RCU");<BR>&nbsp;math_error(regs, error_code, X86_TRAP_XF);<BR>}</P>
<P>dotraplinkage void<BR>do_spurious_interrupt_bug(struct pt_regs *regs, long error_code)<BR>{<BR>&nbsp;cond_local_irq_enable(regs);<BR>}</P>
<P>dotraplinkage void<BR>do_device_not_available(struct pt_regs *regs, long error_code)<BR>{<BR>&nbsp;RCU_LOCKDEP_WARN(!rcu_is_watching(), "entry code didn't wake RCU");</P>
<P>#ifdef CONFIG_MATH_EMULATION<BR>&nbsp;if (!boot_cpu_has(X86_FEATURE_FPU) &amp;&amp; (read_cr0() &amp; X86_CR0_EM)) {<BR>&nbsp;&nbsp;struct math_emu_info info = { };</P>
<P>&nbsp;&nbsp;cond_local_irq_enable(regs);</P>
<P>&nbsp;&nbsp;info.regs = regs;<BR>&nbsp;&nbsp;math_emulate(&amp;info);<BR>&nbsp;&nbsp;return;<BR>&nbsp;}<BR>#endif<BR>&nbsp;fpu__restore(&amp;current-&gt;thread.fpu); /* interrupts still off */<BR>#ifdef CONFIG_X86_32<BR>&nbsp;cond_local_irq_enable(regs);<BR>#endif<BR>}<BR>NOKPROBE_SYMBOL(do_device_not_available);</P>
<P>#ifdef CONFIG_X86_32<BR>dotraplinkage void do_iret_error(struct pt_regs *regs, long error_code)<BR>{<BR>&nbsp;siginfo_t info;</P>
<P>&nbsp;RCU_LOCKDEP_WARN(!rcu_is_watching(), "entry code didn't wake RCU");<BR>&nbsp;local_irq_enable();</P>
<P>&nbsp;info.si_signo = SIGILL;<BR>&nbsp;info.si_errno = 0;<BR>&nbsp;info.si_code = ILL_BADSTK;<BR>&nbsp;info.si_addr = NULL;<BR>&nbsp;if (notify_die(DIE_TRAP, "iret exception", regs, error_code,<BR>&nbsp;&nbsp;&nbsp;X86_TRAP_IRET, SIGILL) != NOTIFY_STOP) {<BR>&nbsp;&nbsp;do_trap(X86_TRAP_IRET, SIGILL, "iret exception", regs, error_code,<BR>&nbsp;&nbsp;&nbsp;&amp;info);<BR>&nbsp;}<BR>}<BR>#endif</P>
<P>/* Set of traps needed for early debugging. */<BR>void __init early_trap_init(void)<BR>{<BR>&nbsp;/*<BR>&nbsp; * Don't use IST to set DEBUG_STACK as it doesn't work until TSS<BR>&nbsp; * is ready in cpu_init() &lt;-- trap_init(). Before trap_init(),<BR>&nbsp; * CPU runs at ring 0 so it is impossible to hit an invalid<BR>&nbsp; * stack.&nbsp; Using the original stack works well enough at this<BR>&nbsp; * early stage. DEBUG_STACK will be equipped after cpu_init() in<BR>&nbsp; * trap_init().<BR>&nbsp; *<BR>&nbsp; * We don't need to set trace_idt_table like set_intr_gate(),<BR>&nbsp; * since we don't have trace_debug and it will be reset to<BR>&nbsp; * 'debug' in trap_init() by set_intr_gate_ist().<BR>&nbsp; */<BR>&nbsp;set_intr_gate_notrace(X86_TRAP_DB, debug);<BR>&nbsp;/* int3 can be called from all */<BR>&nbsp;set_system_intr_gate(X86_TRAP_BP, &amp;int3);<BR>#ifdef CONFIG_X86_32<BR>&nbsp;set_intr_gate(X86_TRAP_PF, page_fault);<BR>#endif<BR>&nbsp;load_idt(&amp;idt_descr);<BR>}</P>
<P>void __init early_trap_pf_init(void)<BR>{<BR>#ifdef CONFIG_X86_64<BR>&nbsp;set_intr_gate(X86_TRAP_PF, page_fault);<BR>#endif<BR>}</P>
<P><FONT class=extract>void __init trap_init(void)<BR>{<BR>&nbsp;int i;</FONT></P>
<P><FONT class=extract>#ifdef CONFIG_EISA<BR>&nbsp;void __iomem *p = early_ioremap(0x0FFFD9, 4);</FONT></P>
<P><FONT class=extract>&nbsp;if (readl(p) == 'E' + ('I'&lt;&lt;8) + ('S'&lt;&lt;16) + ('A'&lt;&lt;24))<BR>&nbsp;&nbsp;EISA_bus = 1;<BR>&nbsp;early_iounmap(p, 4);<BR>#endif</FONT></P>
<P><FONT class=extract>&nbsp;set_intr_gate(X86_TRAP_DE, divide_error);<BR>&nbsp;set_intr_gate_ist(X86_TRAP_NMI, &amp;nmi, NMI_STACK);<BR>&nbsp;/* int4 can be called from all */<BR>&nbsp;set_system_intr_gate(X86_TRAP_OF, &amp;overflow);<BR>&nbsp;set_intr_gate(X86_TRAP_BR, bounds);<BR>&nbsp;set_intr_gate(X86_TRAP_UD, invalid_op);<BR>&nbsp;set_intr_gate(X86_TRAP_NM, device_not_available);<BR>#ifdef CONFIG_X86_32<BR>&nbsp;set_task_gate(X86_TRAP_DF, GDT_ENTRY_DOUBLEFAULT_TSS);<BR>#else<BR>&nbsp;set_intr_gate_ist(X86_TRAP_DF, &amp;double_fault, DOUBLEFAULT_STACK);<BR>#endif<BR>&nbsp;set_intr_gate(X86_TRAP_OLD_MF, coprocessor_segment_overrun);<BR>&nbsp;set_intr_gate(X86_TRAP_TS, invalid_TSS);<BR>&nbsp;set_intr_gate(X86_TRAP_NP, segment_not_present);<BR>&nbsp;set_intr_gate(X86_TRAP_SS, stack_segment);<BR>&nbsp;set_intr_gate(X86_TRAP_GP, general_protection);<BR>&nbsp;set_intr_gate(X86_TRAP_SPURIOUS, spurious_interrupt_bug);<BR>&nbsp;set_intr_gate(X86_TRAP_MF, coprocessor_error);<BR>&nbsp;set_intr_gate(X86_TRAP_AC, alignment_check);<BR>#ifdef CONFIG_X86_MCE<BR>&nbsp;set_intr_gate_ist(X86_TRAP_MC, &amp;machine_check, MCE_STACK);<BR>#endif<BR>&nbsp;set_intr_gate(X86_TRAP_XF, simd_coprocessor_error);</FONT></P>
<P><FONT class=extract>&nbsp;/* Reserve all the builtin and the syscall vector: */<BR>&nbsp;for (i = 0; i &lt; FIRST_EXTERNAL_VECTOR; i++)<BR>&nbsp;&nbsp;set_bit(i, used_vectors);</FONT></P>
<P><FONT class=extract>#ifdef CONFIG_IA32_EMULATION<BR>&nbsp;set_system_intr_gate(IA32_SYSCALL_VECTOR, entry_INT80_compat);<BR>&nbsp;set_bit(IA32_SYSCALL_VECTOR, used_vectors);<BR>#endif</FONT></P>
<P><FONT class=extract>#ifdef CONFIG_X86_32<BR>&nbsp;set_system_intr_gate(IA32_SYSCALL_VECTOR, entry_INT80_32);<BR>&nbsp;set_bit(IA32_SYSCALL_VECTOR, used_vectors);<BR>#endif</FONT></P>
<P><FONT class=extract>&nbsp;/*<BR>&nbsp; * Set the IDT descriptor to a fixed read-only location, so that the<BR>&nbsp; * "sidt" instruction will not leak the location of the kernel, and<BR>&nbsp; * to defend the IDT against arbitrary memory write vulnerabilities.<BR>&nbsp; * It will be reloaded in cpu_init() */<BR>&nbsp;__set_fixmap(FIX_RO_IDT, __pa_symbol(idt_table), PAGE_KERNEL_RO);<BR>&nbsp;idt_descr.address = fix_to_virt(FIX_RO_IDT);</FONT></P>
<P><FONT class=extract>&nbsp;/*<BR>&nbsp; * Should be a barrier for any external CPU state:<BR>&nbsp; */<BR>&nbsp;cpu_init();</FONT></P>
<P><FONT class=extract>&nbsp;/*<BR>&nbsp; * X86_TRAP_DB and X86_TRAP_BP have been set<BR>&nbsp; * in early_trap_init(). However, ITS works only after<BR>&nbsp; * cpu_init() loads TSS. See comments in early_trap_init().<BR>&nbsp; */<BR>&nbsp;set_intr_gate_ist(X86_TRAP_DB, &amp;debug, DEBUG_STACK);<BR>&nbsp;/* int3 can be called from all */<BR>&nbsp;set_system_intr_gate_ist(X86_TRAP_BP, &amp;int3, DEBUG_STACK);</FONT></P>
<P><FONT class=extract>&nbsp;x86_init.irqs.trap_init();</FONT></P>
<P><FONT class=extract>#ifdef CONFIG_X86_64<BR>&nbsp;memcpy(&amp;debug_idt_table, &amp;idt_table, IDT_ENTRIES * 16);<BR>&nbsp;set_nmi_gate(X86_TRAP_DB, &amp;debug);<BR>&nbsp;set_nmi_gate(X86_TRAP_BP, &amp;int3);<BR>#endif<BR>}</FONT>