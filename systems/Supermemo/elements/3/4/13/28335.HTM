# qemu:include/exec/exec-all.h 
<P></P>
<P>/*<BR>&nbsp;* internal execution defines for qemu<BR>&nbsp;*<BR>&nbsp;*&nbsp; Copyright (c) 2003 Fabrice Bellard<BR>&nbsp;*<BR>&nbsp;* This library is free software; you can redistribute it and/or<BR>&nbsp;* modify it under the terms of the GNU Lesser General Public<BR>&nbsp;* License as published by the Free Software Foundation; either<BR>&nbsp;* version 2 of the License, or (at your option) any later version.<BR>&nbsp;*<BR>&nbsp;* This library is distributed in the hope that it will be useful,<BR>&nbsp;* but WITHOUT ANY WARRANTY; without even the implied warranty of<BR>&nbsp;* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.&nbsp; See the GNU<BR>&nbsp;* Lesser General Public License for more details.<BR>&nbsp;*<BR>&nbsp;* You should have received a copy of the GNU Lesser General Public<BR>&nbsp;* License along with this library; if not, see &lt;<A href="http://www.gnu.org/licenses/">http://www.gnu.org/licenses/</A>&gt;.<BR>&nbsp;*/</P>
<P></P>
<P>#ifndef EXEC_ALL_H<BR>#define EXEC_ALL_H</P>
<P>#include "qemu-common.h"<BR>#include "exec/tb-context.h"</P>
<P>/* allow to see translation results - the slowdown should be negligible, so we leave it */<BR>#define DEBUG_DISAS</P>
<P>/* Page tracking code uses ram addresses in system mode, and virtual<BR>&nbsp;&nbsp; addresses in userspace mode.&nbsp; Define tb_page_addr_t to be an appropriate<BR>&nbsp;&nbsp; type.&nbsp; */<BR>#if defined(CONFIG_USER_ONLY)<BR>typedef abi_ulong tb_page_addr_t;<BR>#else<BR>typedef ram_addr_t tb_page_addr_t;<BR>#endif</P>
<P><FONT class=extract>/* is_jmp field values */<BR>#define DISAS_NEXT&nbsp;&nbsp;&nbsp; 0 /* next instruction can be analyzed */<BR>#define DISAS_JUMP&nbsp;&nbsp;&nbsp; 1 /* only pc was modified dynamically */<BR>#define DISAS_UPDATE&nbsp; 2 /* cpu state was modified dynamically */<BR>#define DISAS_TB_JUMP 3 /* only pc was modified statically */</FONT></P>
<P>#include "qemu/log.h"</P>
<P>void gen_intermediate_code(CPUArchState *env, struct TranslationBlock *tb);<BR>void restore_state_to_opc(CPUArchState *env, struct TranslationBlock *tb,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; target_ulong *data);</P>
<P>void cpu_gen_init(void);<BR>bool cpu_restore_state(CPUState *cpu, uintptr_t searched_pc);</P>
<P>void QEMU_NORETURN cpu_loop_exit_noexc(CPUState *cpu);<BR>void QEMU_NORETURN cpu_io_recompile(CPUState *cpu, uintptr_t retaddr);<BR>TranslationBlock *tb_gen_code(CPUState *cpu,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; target_ulong pc, target_ulong cs_base,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; uint32_t flags,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; int cflags);</P>
<P>void QEMU_NORETURN cpu_loop_exit(CPUState *cpu);<BR>void QEMU_NORETURN cpu_loop_exit_restore(CPUState *cpu, uintptr_t pc);<BR>void QEMU_NORETURN cpu_loop_exit_atomic(CPUState *cpu, uintptr_t pc);</P>
<P>#if !defined(CONFIG_USER_ONLY)<BR>void cpu_reloading_memory_map(void);<BR>/**<BR>&nbsp;* cpu_address_space_init:<BR>&nbsp;* @cpu: CPU to add this address space to<BR>&nbsp;* @as: address space to add<BR>&nbsp;* @asidx: integer index of this address space<BR>&nbsp;*<BR>&nbsp;* Add the specified address space to the CPU's cpu_ases list.<BR>&nbsp;* The address space added with @asidx 0 is the one used for the<BR>&nbsp;* convenience pointer cpu-&gt;as.<BR>&nbsp;* The target-specific code which registers ASes is responsible<BR>&nbsp;* for defining what semantics address space 0, 1, 2, etc have.<BR>&nbsp;*<BR>&nbsp;* Before the first call to this function, the caller must set<BR>&nbsp;* cpu-&gt;num_ases to the total number of address spaces it needs<BR>&nbsp;* to support.<BR>&nbsp;*<BR>&nbsp;* Note that with KVM only one address space is supported.<BR>&nbsp;*/<BR>void cpu_address_space_init(CPUState *cpu, AddressSpace *as, int asidx);<BR>/* cputlb.c */<BR>/**<BR>&nbsp;* tlb_flush_page:<BR>&nbsp;* @cpu: CPU whose TLB should be flushed<BR>&nbsp;* @addr: virtual address of page to be flushed<BR>&nbsp;*<BR>&nbsp;* Flush one page from the TLB of the specified CPU, for all<BR>&nbsp;* MMU indexes.<BR>&nbsp;*/<BR>void tlb_flush_page(CPUState *cpu, target_ulong addr);<BR>/**<BR>&nbsp;* tlb_flush:<BR>&nbsp;* @cpu: CPU whose TLB should be flushed<BR>&nbsp;*<BR>&nbsp;* Flush the entire TLB for the specified CPU. Most CPU architectures<BR>&nbsp;* allow the implementation to drop entries from the TLB at any time<BR>&nbsp;* so this is generally safe. If more selective flushing is required<BR>&nbsp;* use one of the other functions for efficiency.<BR>&nbsp;*/<BR>void tlb_flush(CPUState *cpu);<BR>/**<BR>&nbsp;* tlb_flush_page_by_mmuidx:<BR>&nbsp;* @cpu: CPU whose TLB should be flushed<BR>&nbsp;* @addr: virtual address of page to be flushed<BR>&nbsp;* @...: list of MMU indexes to flush, terminated by a negative value<BR>&nbsp;*<BR>&nbsp;* Flush one page from the TLB of the specified CPU, for the specified<BR>&nbsp;* MMU indexes.<BR>&nbsp;*/<BR>void tlb_flush_page_by_mmuidx(CPUState *cpu, target_ulong addr, ...);<BR>/**<BR>&nbsp;* tlb_flush_by_mmuidx:<BR>&nbsp;* @cpu: CPU whose TLB should be flushed<BR>&nbsp;* @...: list of MMU indexes to flush, terminated by a negative value<BR>&nbsp;*<BR>&nbsp;* Flush all entries from the TLB of the specified CPU, for the specified<BR>&nbsp;* MMU indexes.<BR>&nbsp;*/<BR>void tlb_flush_by_mmuidx(CPUState *cpu, ...);<BR>/**<BR>&nbsp;* tlb_set_page_with_attrs:<BR>&nbsp;* @cpu: CPU to add this TLB entry for<BR>&nbsp;* @vaddr: virtual address of page to add entry for<BR>&nbsp;* @paddr: physical address of the page<BR>&nbsp;* @attrs: memory transaction attributes<BR>&nbsp;* @prot: access permissions (PAGE_READ/PAGE_WRITE/PAGE_EXEC bits)<BR>&nbsp;* @mmu_idx: MMU index to insert TLB entry for<BR>&nbsp;* @size: size of the page in bytes<BR>&nbsp;*<BR>&nbsp;* Add an entry to this CPU's TLB (a mapping from virtual address<BR>&nbsp;* @vaddr to physical address @paddr) with the specified memory<BR>&nbsp;* transaction attributes. This is generally called by the target CPU<BR>&nbsp;* specific code after it has been called through the tlb_fill()<BR>&nbsp;* entry point and performed a successful page table walk to find<BR>&nbsp;* the physical address and attributes for the virtual address<BR>&nbsp;* which provoked the TLB miss.<BR>&nbsp;*<BR>&nbsp;* At most one entry for a given virtual address is permitted. Only a<BR>&nbsp;* single TARGET_PAGE_SIZE region is mapped; the supplied @size is only<BR>&nbsp;* used by tlb_flush_page.<BR>&nbsp;*/<BR>void tlb_set_page_with_attrs(CPUState *cpu, target_ulong vaddr,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; hwaddr paddr, MemTxAttrs attrs,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; int prot, int mmu_idx, target_ulong size);<BR>/* tlb_set_page:<BR>&nbsp;*<BR>&nbsp;* This function is equivalent to calling tlb_set_page_with_attrs()<BR>&nbsp;* with an @attrs argument of MEMTXATTRS_UNSPECIFIED. It's provided<BR>&nbsp;* as a convenience for CPUs which don't use memory transaction attributes.<BR>&nbsp;*/<BR>void tlb_set_page(CPUState *cpu, target_ulong vaddr,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; hwaddr paddr, int prot,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; int mmu_idx, target_ulong size);<BR>void tb_invalidate_phys_addr(AddressSpace *as, hwaddr addr);<BR>void probe_write(CPUArchState *env, target_ulong addr, int mmu_idx,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; uintptr_t retaddr);<BR>#else<BR>static inline void tlb_flush_page(CPUState *cpu, target_ulong addr)<BR>{<BR>}</P>
<P>static inline void tlb_flush(CPUState *cpu)<BR>{<BR>}</P>
<P>static inline void tlb_flush_page_by_mmuidx(CPUState *cpu,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; target_ulong addr, ...)<BR>{<BR>}</P>
<P>static inline void tlb_flush_by_mmuidx(CPUState *cpu, ...)<BR>{<BR>}<BR>#endif</P>
<P>#define CODE_GEN_ALIGN&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 16 /* must be &gt;= of the size of a icache line */</P>
<P>/* Estimated block size for TB allocation.&nbsp; */<BR>/* ??? The following is based on a 2015 survey of x86_64 host output.<BR>&nbsp;&nbsp; Better would seem to be some sort of dynamically sized TB array,<BR>&nbsp;&nbsp; adapting to the block sizes actually being produced.&nbsp; */<BR>#if defined(CONFIG_SOFTMMU)<BR>#define CODE_GEN_AVG_BLOCK_SIZE 400<BR>#else<BR>#define CODE_GEN_AVG_BLOCK_SIZE 150<BR>#endif</P>
<P>#if defined(__arm__) || defined(_ARCH_PPC) \<BR>&nbsp;&nbsp;&nbsp; || defined(__x86_64__) || defined(__i386__) \<BR>&nbsp;&nbsp;&nbsp; || defined(__sparc__) || defined(__aarch64__) \<BR>&nbsp;&nbsp;&nbsp; || defined(__s390x__) || defined(__mips__) \<BR>&nbsp;&nbsp;&nbsp; || defined(CONFIG_TCG_INTERPRETER)<BR>/* NOTE: Direct jump patching must be atomic to be thread-safe. */<BR>#define USE_DIRECT_JUMP<BR>#endif</P>
<P><FONT class=extract>struct TranslationBlock {<BR>&nbsp;&nbsp;&nbsp; target_ulong pc;&nbsp;&nbsp; /* simulated PC corresponding to this block (EIP + CS base) */<BR>&nbsp;&nbsp;&nbsp; target_ulong cs_base; /* CS base for this block */<BR>&nbsp;&nbsp;&nbsp; uint32_t flags; /* flags defining in which context the code was generated */<BR>&nbsp;&nbsp;&nbsp; uint16_t size;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; /* size of target code for this block (1 &lt;=<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; size &lt;= TARGET_PAGE_SIZE) */<BR>&nbsp;&nbsp;&nbsp; uint16_t icount;<BR>&nbsp;&nbsp;&nbsp; uint32_t cflags;&nbsp;&nbsp;&nbsp; /* compile flags */<BR>#define CF_COUNT_MASK&nbsp; 0x7fff<BR>#define CF_LAST_IO&nbsp;&nbsp;&nbsp;&nbsp; 0x8000 /* Last insn may be an IO access.&nbsp; */<BR>#define CF_NOCACHE&nbsp;&nbsp;&nbsp;&nbsp; 0x10000 /* To be freed after execution */<BR>#define CF_USE_ICOUNT&nbsp; 0x20000<BR>#define CF_IGNORE_ICOUNT 0x40000 /* Do not generate icount code */</FONT></P>
<P><FONT class=extract>&nbsp;&nbsp;&nbsp; uint16_t invalid;</FONT></P>
<P><FONT class=extract>&nbsp;&nbsp;&nbsp; void *tc_ptr;&nbsp;&nbsp;&nbsp; /* pointer to the translated code */<BR>&nbsp;&nbsp;&nbsp; uint8_t *tc_search;&nbsp; /* pointer to search data */<BR>&nbsp;&nbsp;&nbsp; /* original tb when cflags has CF_NOCACHE */<BR>&nbsp;&nbsp;&nbsp; struct TranslationBlock *orig_tb;<BR>&nbsp;&nbsp;&nbsp; /* first and second physical page containing code. The lower bit<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; of the pointer tells the index in page_next[] */<BR>&nbsp;&nbsp;&nbsp; struct TranslationBlock *page_next[2];<BR>&nbsp;&nbsp;&nbsp; tb_page_addr_t page_addr[2];</FONT></P>
<P><FONT class=extract>&nbsp;&nbsp;&nbsp; /* The following data are used to directly call another TB from<BR>&nbsp;&nbsp;&nbsp;&nbsp; * the code of this one. This can be done either by emitting direct or<BR>&nbsp;&nbsp;&nbsp;&nbsp; * indirect native jump instructions. These jumps are reset so that the TB<BR>&nbsp;&nbsp;&nbsp;&nbsp; * just continue its execution. The TB can be linked to another one by<BR>&nbsp;&nbsp;&nbsp;&nbsp; * setting one of the jump targets (or patching the jump instruction). Only<BR>&nbsp;&nbsp;&nbsp;&nbsp; * two of such jumps are supported.<BR>&nbsp;&nbsp;&nbsp;&nbsp; */<BR>&nbsp;&nbsp;&nbsp; uint16_t jmp_reset_offset[2]; /* offset of original jump target */<BR>#define TB_JMP_RESET_OFFSET_INVALID 0xffff /* indicates no jump generated */<BR>#ifdef USE_DIRECT_JUMP<BR>&nbsp;&nbsp;&nbsp; uint16_t jmp_insn_offset[2]; /* offset of native jump instruction */<BR>#else<BR>&nbsp;&nbsp;&nbsp; uintptr_t jmp_target_addr[2]; /* target address for indirect jump */<BR>#endif<BR>&nbsp;&nbsp;&nbsp; /* Each TB has an assosiated circular list of TBs jumping to this one.<BR>&nbsp;&nbsp;&nbsp;&nbsp; * jmp_list_first points to the first TB jumping to this one.<BR>&nbsp;&nbsp;&nbsp;&nbsp; * jmp_list_next is used to point to the next TB in a list.<BR>&nbsp;&nbsp;&nbsp;&nbsp; * Since each TB can have two jumps, it can participate in two lists.<BR>&nbsp;&nbsp;&nbsp;&nbsp; * jmp_list_first and jmp_list_next are 4-byte aligned pointers to a<BR>&nbsp;&nbsp;&nbsp;&nbsp; * TranslationBlock structure, but the two least significant bits of<BR>&nbsp;&nbsp;&nbsp;&nbsp; * them are used to encode which data field of the pointed TB should<BR>&nbsp;&nbsp;&nbsp;&nbsp; * be used to traverse the list further from that TB:<BR>&nbsp;&nbsp;&nbsp;&nbsp; * 0 =&gt; jmp_list_next[0], 1 =&gt; jmp_list_next[1], 2 =&gt; jmp_list_first.<BR>&nbsp;&nbsp;&nbsp;&nbsp; * In other words, 0/1 tells which jump is used in the pointed TB,<BR>&nbsp;&nbsp;&nbsp;&nbsp; * and 2 means that this is a pointer back to the target TB of this list.<BR>&nbsp;&nbsp;&nbsp;&nbsp; */<BR>&nbsp;&nbsp;&nbsp; uintptr_t jmp_list_next[2];<BR>&nbsp;&nbsp;&nbsp; uintptr_t jmp_list_first;<BR>};</FONT></P>
<P>void tb_free(TranslationBlock *tb);<BR>void tb_flush(CPUState *cpu);<BR>void tb_phys_invalidate(TranslationBlock *tb, tb_page_addr_t page_addr);</P>
<P>#if defined(USE_DIRECT_JUMP)</P>
<P>#if defined(CONFIG_TCG_INTERPRETER)<BR>static inline void tb_set_jmp_target1(uintptr_t jmp_addr, uintptr_t addr)<BR>{<BR>&nbsp;&nbsp;&nbsp; /* patch the branch destination */<BR>&nbsp;&nbsp;&nbsp; atomic_set((int32_t *)jmp_addr, addr - (jmp_addr + 4));<BR>&nbsp;&nbsp;&nbsp; /* no need to flush icache explicitly */<BR>}<BR>#elif defined(_ARCH_PPC)<BR>void ppc_tb_set_jmp_target(uintptr_t jmp_addr, uintptr_t addr);<BR>#define tb_set_jmp_target1 ppc_tb_set_jmp_target<BR><SPAN class=cloze>[...]</SPAN><BR>#elif defined(__s390x__)<BR>static inline void tb_set_jmp_target1(uintptr_t jmp_addr, uintptr_t addr)<BR>{<BR>&nbsp;&nbsp;&nbsp; /* patch the branch destination */<BR>&nbsp;&nbsp;&nbsp; intptr_t disp = addr - (jmp_addr - 2);<BR>&nbsp;&nbsp;&nbsp; atomic_set((int32_t *)jmp_addr, disp / 2);<BR>&nbsp;&nbsp;&nbsp; /* no need to flush icache explicitly */<BR>}<BR>#elif defined(__aarch64__)<BR>void aarch64_tb_set_jmp_target(uintptr_t jmp_addr, uintptr_t addr);<BR>#define tb_set_jmp_target1 aarch64_tb_set_jmp_target<BR>#elif defined(__arm__)<BR>void arm_tb_set_jmp_target(uintptr_t jmp_addr, uintptr_t addr);<BR>#define tb_set_jmp_target1 arm_tb_set_jmp_target<BR>#elif defined(__sparc__) || defined(__mips__)<BR>void tb_set_jmp_target1(uintptr_t jmp_addr, uintptr_t addr);<BR>#else<BR>#error tb_set_jmp_target1 is missing<BR>#endif</P>
<P>static inline void tb_set_jmp_target(TranslationBlock *tb,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; int n, uintptr_t addr)<BR>{<BR>&nbsp;&nbsp;&nbsp; uint16_t offset = tb-&gt;jmp_insn_offset[n];<BR>&nbsp;&nbsp;&nbsp; tb_set_jmp_target1((uintptr_t)(tb-&gt;tc_ptr + offset), addr);<BR>}</P>
<P>#else</P>
<P>/* set the jump target */<BR>static inline void tb_set_jmp_target(TranslationBlock *tb,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; int n, uintptr_t addr)<BR>{<BR>&nbsp;&nbsp;&nbsp; tb-&gt;jmp_target_addr[n] = addr;<BR>}</P>
<P>#endif</P>
<P>/* Called with tb_lock held.&nbsp; */<BR>static inline void tb_add_jump(TranslationBlock *tb, int n,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; TranslationBlock *tb_next)<BR>{<BR>&nbsp;&nbsp;&nbsp; if (tb-&gt;jmp_list_next[n]) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; /* Another thread has already done this while we were<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; * outside of the lock; nothing to do in this case */<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return;<BR>&nbsp;&nbsp;&nbsp; }<BR>&nbsp;&nbsp;&nbsp; qemu_log_mask_and_addr(CPU_LOG_EXEC, tb-&gt;pc,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "Linking TBs %p [" TARGET_FMT_lx<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "] index %d -&gt; %p [" TARGET_FMT_lx "]\n",<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; tb-&gt;tc_ptr, tb-&gt;pc, n,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; tb_next-&gt;tc_ptr, tb_next-&gt;pc);</P>
<P>&nbsp;&nbsp;&nbsp; /* patch the native jump address */<BR>&nbsp;&nbsp;&nbsp; tb_set_jmp_target(tb, n, (uintptr_t)tb_next-&gt;tc_ptr);</P>
<P>&nbsp;&nbsp;&nbsp; /* add in TB jmp circular list */<BR>&nbsp;&nbsp;&nbsp; tb-&gt;jmp_list_next[n] = tb_next-&gt;jmp_list_first;<BR>&nbsp;&nbsp;&nbsp; tb_next-&gt;jmp_list_first = (uintptr_t)tb | n;<BR>}</P>
<P>/* GETPC is the true target of the return instruction that we'll execute.&nbsp; */<BR>#if defined(CONFIG_TCG_INTERPRETER)<BR>extern uintptr_t tci_tb_ptr;<BR># define GETPC() tci_tb_ptr<BR>#else<BR># define GETPC() \<BR>&nbsp;&nbsp;&nbsp; ((uintptr_t)__builtin_extract_return_addr(__builtin_return_address(0)))<BR>#endif</P>
<P>/* The true return address will often point to a host insn that is part of<BR>&nbsp;&nbsp; the next translated guest insn.&nbsp; Adjust the address backward to point to<BR>&nbsp;&nbsp; the middle of the call insn.&nbsp; Subtracting one would do the job except for<BR>&nbsp;&nbsp; several compressed mode architectures (arm, mips) which set the low bit<BR>&nbsp;&nbsp; to indicate the compressed mode; subtracting two works around that.&nbsp; It<BR>&nbsp;&nbsp; is also the case that there are no host isas that contain a call insn<BR>&nbsp;&nbsp; smaller than 4 bytes, so we don't worry about special-casing this.&nbsp; */<BR>#define GETPC_ADJ&nbsp;&nbsp; 2</P>
<P>#if !defined(CONFIG_USER_ONLY)</P>
<P>struct MemoryRegion *iotlb_to_region(CPUState *cpu,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; hwaddr index, MemTxAttrs attrs);</P>
<P>void tlb_fill(CPUState *cpu, target_ulong addr, MMUAccessType access_type,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; int mmu_idx, uintptr_t retaddr);</P>
<P>#endif</P>
<P>#if defined(CONFIG_USER_ONLY)<BR>void mmap_lock(void);<BR>void mmap_unlock(void);<BR>bool have_mmap_lock(void);</P>
<P>static inline tb_page_addr_t get_page_addr_code(CPUArchState *env1, target_ulong addr)<BR>{<BR>&nbsp;&nbsp;&nbsp; return addr;<BR>}<BR>#else<BR>static inline void mmap_lock(void) {}<BR>static inline void mmap_unlock(void) {}</P>
<P>/* cputlb.c */<BR>tb_page_addr_t get_page_addr_code(CPUArchState *env1, target_ulong addr);</P>
<P>void tlb_reset_dirty(CPUState *cpu, ram_addr_t start1, ram_addr_t length);<BR>void tlb_set_dirty(CPUState *cpu, target_ulong vaddr);</P>
<P>/* exec.c */<BR>void tb_flush_jmp_cache(CPUState *cpu, target_ulong addr);</P>
<P>MemoryRegionSection *<BR>address_space_translate_for_iotlb(CPUState *cpu, int asidx, hwaddr addr,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; hwaddr *xlat, hwaddr *plen);<BR>hwaddr memory_region_section_get_iotlb(CPUState *cpu,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; MemoryRegionSection *section,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; target_ulong vaddr,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; hwaddr paddr, hwaddr xlat,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; int prot,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; target_ulong *address);<BR>bool memory_region_is_unassigned(MemoryRegion *mr);</P>
<P>#endif</P>
<P>/* vl.c */<BR>extern int singlestep;</P>
<P>/* cpu-exec.c, accessed with atomic_mb_read/atomic_mb_set */<BR>extern CPUState *tcg_current_cpu;<BR>extern bool exit_request;</P>
<P>#endif