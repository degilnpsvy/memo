/**<BR>&nbsp;* tcg_qemu_tb_exec:<BR>&nbsp;* @env: pointer to CPUArchState for the CPU<BR>&nbsp;* @tb_ptr: address of generated code for the TB to execute<BR>&nbsp;*<BR>&nbsp;* Start executing code from a given translation block.<BR>&nbsp;* Where translation blocks have been linked, execution<BR>&nbsp;* may proceed from the given TB into successive ones.<BR>&nbsp;* Control eventually returns only when some action is needed<BR>&nbsp;* from the top-level loop: either control must pass to a TB<BR>&nbsp;* which has not yet been directly linked, or an asynchronous<BR>&nbsp;* event such as an interrupt needs handling.<BR>&nbsp;*<BR>&nbsp;* Return: The return value is the value passed to the corresponding<BR>&nbsp;* tcg_gen_exit_tb() at translation time of the last TB attempted to execute.<BR>&nbsp;* The value is either zero or a 4-byte aligned pointer to that TB combined<BR>&nbsp;* with additional information in its two least significant bits. The<BR>&nbsp;* additional information is encoded as follows:<BR>&nbsp;*&nbsp; 0, 1: the link between this TB and the next is via the specified<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; TB index (0 or 1). That is, we left the TB via (the equivalent<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; of) "goto_tb &lt;index&gt;". The main loop uses this to determine<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; how to link the TB just executed to the next.<BR>&nbsp;*&nbsp; 2:&nbsp;&nbsp;&nbsp; we are using instruction counting code generation, and we<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; did not start executing this TB because the instruction counter<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; would hit zero midway through it. In this case the pointer<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; returned is the TB we were about to execute, and the caller must<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; arrange to execute the remaining count of instructions.<BR>&nbsp;*&nbsp; 3:&nbsp;&nbsp;&nbsp; we stopped because the CPU's exit_request flag was set<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (usually meaning that there is an interrupt that needs to be<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; handled). The pointer returned is the TB we were about to execute<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; when we noticed the pending exit request.<BR>&nbsp;*<BR>&nbsp;* If the bottom two bits indicate an exit-via-index then the CPU<BR>&nbsp;* state is correctly synchronised and ready for execution of the next<BR>&nbsp;* TB (and in particular the guest PC is the address to execute next).<BR>&nbsp;* Otherwise, we gave up on execution of this TB before it started, and<BR>&nbsp;* the caller must fix up the CPU state by calling the CPU's<BR>&nbsp;* synchronize_from_tb() method with the TB pointer we return (falling<BR>&nbsp;* back to calling the CPU's set_pc method with tb-&gt;pb if no<BR>&nbsp;* synchronize_from_tb() method exists).<BR>&nbsp;*<BR>&nbsp;* Note that TCG targets may use a different definition of tcg_qemu_tb_exec<BR>&nbsp;* to this default (which just calls the prologue.code emitted by<BR>&nbsp;* tcg_target_qemu_prologue()).<BR>&nbsp;*/<BR>#define TB_EXIT_MASK 3<BR>#define TB_EXIT_IDX0 0<BR>#define TB_EXIT_IDX1 1<BR>#define TB_EXIT_ICOUNT_EXPIRED 2<BR>#define TB_EXIT_REQUESTED 3
<P></P>
<P>#ifdef HAVE_TCG_QEMU_TB_EXEC<BR>uintptr_t tcg_qemu_tb_exec(CPUArchState *env, uint8_t *tb_ptr);<BR>#else<BR># define tcg_qemu_tb_exec(env, tb_ptr) \<BR>&nbsp;&nbsp;&nbsp; ((uintptr_t (*)(void *, void *))tcg_ctx.code_gen_prologue)(env, tb_ptr)<BR>#endif