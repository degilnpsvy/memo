# kernel-4.5 linux/kernel/smp.c</P>
<P>/*<BR>&nbsp;* Generic helpers for smp ipi calls<BR>&nbsp;*<BR>&nbsp;* (C) Jens Axboe &lt;<A href="mailto:jens.axboe@oracle.com">jens.axboe@oracle.com</A>&gt; 2008<BR>&nbsp;*/<BR>#include &lt;linux/irq_work.h&gt;<BR>#include &lt;linux/rcupdate.h&gt;<BR>#include &lt;linux/rculist.h&gt;<BR>#include &lt;linux/kernel.h&gt;<BR>#include &lt;linux/export.h&gt;<BR>#include &lt;linux/percpu.h&gt;<BR>#include &lt;linux/init.h&gt;<BR>#include &lt;linux/gfp.h&gt;<BR>#include &lt;linux/smp.h&gt;<BR>#include &lt;linux/cpu.h&gt;<BR>#include &lt;linux/sched.h&gt;</P>
<P></P>
<P>#include "smpboot.h"</P>
<P>enum {<BR>&nbsp;CSD_FLAG_LOCK&nbsp;&nbsp;= 0x01,<BR>&nbsp;CSD_FLAG_SYNCHRONOUS&nbsp;= 0x02,<BR>};</P>
<P>struct call_function_data {<BR>&nbsp;struct call_single_data&nbsp;__percpu *csd;<BR>&nbsp;cpumask_var_t&nbsp;&nbsp;cpumask;<BR>};</P>
<P>static DEFINE_PER_CPU_SHARED_ALIGNED(struct call_function_data, cfd_data);</P>
<P>static DEFINE_PER_CPU_SHARED_ALIGNED(struct llist_head, call_single_queue);</P>
<P>static void flush_smp_call_function_queue(bool warn_cpu_offline);</P>
<P>static int<BR>hotplug_cfd(struct notifier_block *nfb, unsigned long action, void *hcpu)<BR>{<BR>&nbsp;long cpu = (long)hcpu;<BR>&nbsp;struct call_function_data *cfd = &amp;per_cpu(cfd_data, cpu);</P>
<P>&nbsp;switch (action) {<BR>&nbsp;case CPU_UP_PREPARE:<BR>&nbsp;case CPU_UP_PREPARE_FROZEN:<BR>&nbsp;&nbsp;if (!zalloc_cpumask_var_node(&amp;cfd-&gt;cpumask, GFP_KERNEL,<BR>&nbsp;&nbsp;&nbsp;&nbsp;cpu_to_node(cpu)))<BR>&nbsp;&nbsp;&nbsp;return notifier_from_errno(-ENOMEM);<BR>&nbsp;&nbsp;cfd-&gt;csd = alloc_percpu(struct call_single_data);<BR>&nbsp;&nbsp;if (!cfd-&gt;csd) {<BR>&nbsp;&nbsp;&nbsp;free_cpumask_var(cfd-&gt;cpumask);<BR>&nbsp;&nbsp;&nbsp;return notifier_from_errno(-ENOMEM);<BR>&nbsp;&nbsp;}<BR>&nbsp;&nbsp;break;</P>
<P>#ifdef CONFIG_HOTPLUG_CPU<BR>&nbsp;case CPU_UP_CANCELED:<BR>&nbsp;case CPU_UP_CANCELED_FROZEN:<BR>&nbsp;&nbsp;/* Fall-through to the CPU_DEAD[_FROZEN] case. */</P>
<P>&nbsp;case CPU_DEAD:<BR>&nbsp;case CPU_DEAD_FROZEN:<BR>&nbsp;&nbsp;free_cpumask_var(cfd-&gt;cpumask);<BR>&nbsp;&nbsp;free_percpu(cfd-&gt;csd);<BR>&nbsp;&nbsp;break;</P>
<P>&nbsp;case CPU_DYING:<BR>&nbsp;case CPU_DYING_FROZEN:<BR>&nbsp;&nbsp;/*<BR>&nbsp;&nbsp; * The IPIs for the smp-call-function callbacks queued by other<BR>&nbsp;&nbsp; * CPUs might arrive late, either due to hardware latencies or<BR>&nbsp;&nbsp; * because this CPU disabled interrupts (inside stop-machine)<BR>&nbsp;&nbsp; * before the IPIs were sent. So flush out any pending callbacks<BR>&nbsp;&nbsp; * explicitly (without waiting for the IPIs to arrive), to<BR>&nbsp;&nbsp; * ensure that the outgoing CPU doesn't go offline with work<BR>&nbsp;&nbsp; * still pending.<BR>&nbsp;&nbsp; */<BR>&nbsp;&nbsp;flush_smp_call_function_queue(false);<BR>&nbsp;&nbsp;break;<BR>#endif<BR>&nbsp;};</P>
<P>&nbsp;return NOTIFY_OK;<BR>}</P>
<P>static struct notifier_block hotplug_cfd_notifier = {<BR>&nbsp;.notifier_call&nbsp;&nbsp;= hotplug_cfd,<BR>};</P>
<P>void __init call_function_init(void)<BR>{<BR>&nbsp;void *cpu = (void *)(long)smp_processor_id();<BR>&nbsp;int i;</P>
<P>&nbsp;for_each_possible_cpu(i)<BR>&nbsp;&nbsp;init_llist_head(&amp;per_cpu(call_single_queue, i));</P>
<P>&nbsp;hotplug_cfd(&amp;hotplug_cfd_notifier, CPU_UP_PREPARE, cpu);<BR>&nbsp;register_cpu_notifier(&amp;hotplug_cfd_notifier);<BR>}</P>
<P>/*<BR>&nbsp;* csd_lock/csd_unlock used to serialize access to per-cpu csd resources<BR>&nbsp;*<BR>&nbsp;* For non-synchronous ipi calls the csd can still be in use by the<BR>&nbsp;* previous function call. For multi-cpu calls its even more interesting<BR>&nbsp;* as we'll have to ensure no other cpu is observing our csd.<BR>&nbsp;*/<BR>static __always_inline void csd_lock_wait(struct call_single_data *csd)<BR>{<BR>&nbsp;smp_cond_acquire(!(csd-&gt;flags &amp; CSD_FLAG_LOCK));<BR>}</P>
<P>static __always_inline void csd_lock(struct call_single_data *csd)<BR>{<BR>&nbsp;csd_lock_wait(csd);<BR>&nbsp;csd-&gt;flags |= CSD_FLAG_LOCK;</P>
<P>&nbsp;/*<BR>&nbsp; * prevent CPU from reordering the above assignment<BR>&nbsp; * to -&gt;flags with any subsequent assignments to other<BR>&nbsp; * fields of the specified call_single_data structure:<BR>&nbsp; */<BR>&nbsp;smp_wmb();<BR>}</P>
<P>static __always_inline void csd_unlock(struct call_single_data *csd)<BR>{<BR>&nbsp;WARN_ON(!(csd-&gt;flags &amp; CSD_FLAG_LOCK));</P>
<P>&nbsp;/*<BR>&nbsp; * ensure we're all done before releasing data:<BR>&nbsp; */<BR>&nbsp;smp_store_release(&amp;csd-&gt;flags, 0);<BR>}</P>
<P>static DEFINE_PER_CPU_SHARED_ALIGNED(struct call_single_data, csd_data);</P>
<P>/*<BR>&nbsp;* Insert a previously allocated call_single_data element<BR>&nbsp;* for execution on the given CPU. data must already have<BR>&nbsp;* -&gt;func, -&gt;info, and -&gt;flags set.<BR>&nbsp;*/<BR>static int generic_exec_single(int cpu, struct call_single_data *csd,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; smp_call_func_t func, void *info)<BR>{<BR>&nbsp;if (cpu == smp_processor_id()) {<BR>&nbsp;&nbsp;unsigned long flags;</P>
<P>&nbsp;&nbsp;/*<BR>&nbsp;&nbsp; * We can unlock early even for the synchronous on-stack case,<BR>&nbsp;&nbsp; * since we're doing this from the same CPU..<BR>&nbsp;&nbsp; */<BR>&nbsp;&nbsp;csd_unlock(csd);<BR>&nbsp;&nbsp;local_irq_save(flags);<BR>&nbsp;&nbsp;func(info);<BR>&nbsp;&nbsp;local_irq_restore(flags);<BR>&nbsp;&nbsp;return 0;<BR>&nbsp;}</P>
<P><BR>&nbsp;if ((unsigned)cpu &gt;= nr_cpu_ids || !cpu_online(cpu)) {<BR>&nbsp;&nbsp;csd_unlock(csd);<BR>&nbsp;&nbsp;return -ENXIO;<BR>&nbsp;}</P>
<P>&nbsp;csd-&gt;func = func;<BR>&nbsp;csd-&gt;info = info;</P>
<P>&nbsp;/*<BR>&nbsp; * The list addition should be visible before sending the IPI<BR>&nbsp; * handler locks the list to pull the entry off it because of<BR>&nbsp; * normal cache coherency rules implied by spinlocks.<BR>&nbsp; *<BR>&nbsp; * If IPIs can go out of order to the cache coherency protocol<BR>&nbsp; * in an architecture, sufficient synchronisation should be added<BR>&nbsp; * to arch code to make it appear to obey cache coherency WRT<BR>&nbsp; * locking and barrier primitives. Generic code isn't really<BR>&nbsp; * equipped to do the right thing...<BR>&nbsp; */<BR>&nbsp;if (llist_add(&amp;csd-&gt;llist, &amp;per_cpu(call_single_queue, cpu)))<BR>&nbsp;&nbsp;arch_send_call_function_single_ipi(cpu);</P>
<P>&nbsp;return 0;<BR>}</P>
<P>/**<BR>&nbsp;* generic_smp_call_function_single_interrupt - Execute SMP IPI callbacks<BR>&nbsp;*<BR>&nbsp;* Invoked by arch to handle an IPI for call function single.<BR>&nbsp;* Must be called with interrupts disabled.<BR>&nbsp;*/<BR>void generic_smp_call_function_single_interrupt(void)<BR>{<BR>&nbsp;flush_smp_call_function_queue(true);<BR>}</P>
<P>/**<BR>&nbsp;* flush_smp_call_function_queue - Flush pending smp-call-function callbacks<BR>&nbsp;*<BR>&nbsp;* @warn_cpu_offline: If set to 'true', warn if callbacks were queued on an<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; offline CPU. Skip this check if set to 'false'.<BR>&nbsp;*<BR>&nbsp;* Flush any pending smp-call-function callbacks queued on this CPU. This is<BR>&nbsp;* invoked by the generic IPI handler, as well as by a CPU about to go offline,<BR>&nbsp;* to ensure that all pending IPI callbacks are run before it goes completely<BR>&nbsp;* offline.<BR>&nbsp;*<BR>&nbsp;* Loop through the call_single_queue and run all the queued callbacks.<BR>&nbsp;* Must be called with interrupts disabled.<BR>&nbsp;*/<BR>static void flush_smp_call_function_queue(bool warn_cpu_offline)<BR>{<BR>&nbsp;struct llist_head *head;<BR>&nbsp;struct llist_node *entry;<BR>&nbsp;struct call_single_data *csd, *csd_next;<BR>&nbsp;static bool warned;</P>
<P>&nbsp;WARN_ON(!irqs_disabled());</P>
<P>&nbsp;head = this_cpu_ptr(&amp;call_single_queue);<BR>&nbsp;entry = llist_del_all(head);<BR>&nbsp;entry = llist_reverse_order(entry);</P>
<P>&nbsp;/* There shouldn't be any pending callbacks on an offline CPU. */<BR>&nbsp;if (unlikely(warn_cpu_offline &amp;&amp; !cpu_online(smp_processor_id()) &amp;&amp;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; !warned &amp;&amp; !llist_empty(head))) {<BR>&nbsp;&nbsp;warned = true;<BR>&nbsp;&nbsp;WARN(1, "IPI on offline CPU %d\n", smp_processor_id());</P>
<P>&nbsp;&nbsp;/*<BR>&nbsp;&nbsp; * We don't have to use the _safe() variant here<BR>&nbsp;&nbsp; * because we are not invoking the IPI handlers yet.<BR>&nbsp;&nbsp; */<BR>&nbsp;&nbsp;llist_for_each_entry(csd, entry, llist)<BR>&nbsp;&nbsp;&nbsp;pr_warn("IPI callback %pS sent to offline CPU\n",<BR>&nbsp;&nbsp;&nbsp;&nbsp;csd-&gt;func);<BR>&nbsp;}</P>
<P>&nbsp;llist_for_each_entry_safe(csd, csd_next, entry, llist) {<BR>&nbsp;&nbsp;smp_call_func_t func = csd-&gt;func;<BR>&nbsp;&nbsp;void *info = csd-&gt;info;</P>
<P>&nbsp;&nbsp;/* Do we wait until *after* callback? */<BR>&nbsp;&nbsp;if (csd-&gt;flags &amp; CSD_FLAG_SYNCHRONOUS) {<BR>&nbsp;&nbsp;&nbsp;func(info);<BR>&nbsp;&nbsp;&nbsp;csd_unlock(csd);<BR>&nbsp;&nbsp;} else {<BR>&nbsp;&nbsp;&nbsp;csd_unlock(csd);<BR>&nbsp;&nbsp;&nbsp;func(info);<BR>&nbsp;&nbsp;}<BR>&nbsp;}</P>
<P>&nbsp;/*<BR>&nbsp; * Handle irq works queued remotely by irq_work_queue_on().<BR>&nbsp; * Smp functions above are typically synchronous so they<BR>&nbsp; * better run first since some other CPUs may be busy waiting<BR>&nbsp; * for them.<BR>&nbsp; */<BR>&nbsp;irq_work_run();<BR>}</P>
<P>/*<BR>&nbsp;* smp_call_function_single - Run a function on a specific CPU<BR>&nbsp;* @func: The function to run. This must be fast and non-blocking.<BR>&nbsp;* @info: An arbitrary pointer to pass to the function.<BR>&nbsp;* @wait: If true, wait until function has completed on other CPUs.<BR>&nbsp;*<BR>&nbsp;* Returns 0 on success, else a negative status code.<BR>&nbsp;*/<BR>int smp_call_function_single(int cpu, smp_call_func_t func, void *info,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; int wait)<BR>{<BR>&nbsp;struct call_single_data *csd;<BR>&nbsp;struct call_single_data csd_stack = { .flags = CSD_FLAG_LOCK | CSD_FLAG_SYNCHRONOUS };<BR>&nbsp;int this_cpu;<BR>&nbsp;int err;</P>
<P>&nbsp;/*<BR>&nbsp; * prevent preemption and reschedule on another processor,<BR>&nbsp; * as well as CPU removal<BR>&nbsp; */<BR>&nbsp;this_cpu = get_cpu();</P>
<P>&nbsp;/*<BR>&nbsp; * Can deadlock when called with interrupts disabled.<BR>&nbsp; * We allow cpu's that are not yet online though, as no one else can<BR>&nbsp; * send smp call function interrupt to this cpu and as such deadlocks<BR>&nbsp; * can't happen.<BR>&nbsp; */<BR>&nbsp;WARN_ON_ONCE(cpu_online(this_cpu) &amp;&amp; irqs_disabled()<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;&amp; !oops_in_progress);</P>
<P>&nbsp;csd = &amp;csd_stack;<BR>&nbsp;if (!wait) {<BR>&nbsp;&nbsp;csd = this_cpu_ptr(&amp;csd_data);<BR>&nbsp;&nbsp;csd_lock(csd);<BR>&nbsp;}</P>
<P>&nbsp;err = generic_exec_single(cpu, csd, func, info);</P>
<P>&nbsp;if (wait)<BR>&nbsp;&nbsp;csd_lock_wait(csd);</P>
<P>&nbsp;put_cpu();</P>
<P>&nbsp;return err;<BR>}<BR>EXPORT_SYMBOL(smp_call_function_single);</P>
<P>/**<BR>&nbsp;* smp_call_function_single_async(): Run an asynchronous function on a<BR>&nbsp;* &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; specific CPU.<BR>&nbsp;* @cpu: The CPU to run on.<BR>&nbsp;* @csd: Pre-allocated and setup data structure<BR>&nbsp;*<BR>&nbsp;* Like smp_call_function_single(), but the call is asynchonous and<BR>&nbsp;* can thus be done from contexts with disabled interrupts.<BR>&nbsp;*<BR>&nbsp;* The caller passes his own pre-allocated data structure<BR>&nbsp;* (ie: embedded in an object) and is responsible for synchronizing it<BR>&nbsp;* such that the IPIs performed on the @csd are strictly serialized.<BR>&nbsp;*<BR>&nbsp;* NOTE: Be careful, there is unfortunately no current debugging facility to<BR>&nbsp;* validate the correctness of this serialization.<BR>&nbsp;*/<BR>int smp_call_function_single_async(int cpu, struct call_single_data *csd)<BR>{<BR>&nbsp;int err = 0;</P>
<P>&nbsp;preempt_disable();</P>
<P>&nbsp;/* We could deadlock if we have to wait here with interrupts disabled! */<BR>&nbsp;if (WARN_ON_ONCE(csd-&gt;flags &amp; CSD_FLAG_LOCK))<BR>&nbsp;&nbsp;csd_lock_wait(csd);</P>
<P>&nbsp;csd-&gt;flags = CSD_FLAG_LOCK;<BR>&nbsp;smp_wmb();</P>
<P>&nbsp;err = generic_exec_single(cpu, csd, csd-&gt;func, csd-&gt;info);<BR>&nbsp;preempt_enable();</P>
<P>&nbsp;return err;<BR>}<BR>EXPORT_SYMBOL_GPL(smp_call_function_single_async);</P>
<P>/*<BR>&nbsp;* smp_call_function_any - Run a function on any of the given cpus<BR>&nbsp;* @mask: The mask of cpus it can run on.<BR>&nbsp;* @func: The function to run. This must be fast and non-blocking.<BR>&nbsp;* @info: An arbitrary pointer to pass to the function.<BR>&nbsp;* @wait: If true, wait until function has completed.<BR>&nbsp;*<BR>&nbsp;* Returns 0 on success, else a negative status code (if no cpus were online).<BR>&nbsp;*<BR>&nbsp;* Selection preference:<BR>&nbsp;*&nbsp;1) current cpu if in @mask<BR>&nbsp;*&nbsp;2) any cpu of current node if in @mask<BR>&nbsp;*&nbsp;3) any other online cpu in @mask<BR>&nbsp;*/<BR>int smp_call_function_any(const struct cpumask *mask,<BR>&nbsp;&nbsp;&nbsp;&nbsp; smp_call_func_t func, void *info, int wait)<BR>{<BR>&nbsp;unsigned int cpu;<BR>&nbsp;const struct cpumask *nodemask;<BR>&nbsp;int ret;</P>
<P>&nbsp;/* Try for same CPU (cheapest) */<BR>&nbsp;cpu = get_cpu();<BR>&nbsp;if (cpumask_test_cpu(cpu, mask))<BR>&nbsp;&nbsp;goto call;</P>
<P>&nbsp;/* Try for same node. */<BR>&nbsp;nodemask = cpumask_of_node(cpu_to_node(cpu));<BR>&nbsp;for (cpu = cpumask_first_and(nodemask, mask); cpu &lt; nr_cpu_ids;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; cpu = cpumask_next_and(cpu, nodemask, mask)) {<BR>&nbsp;&nbsp;if (cpu_online(cpu))<BR>&nbsp;&nbsp;&nbsp;goto call;<BR>&nbsp;}</P>
<P>&nbsp;/* Any online will do: smp_call_function_single handles nr_cpu_ids. */<BR>&nbsp;cpu = cpumask_any_and(mask, cpu_online_mask);<BR>call:<BR>&nbsp;ret = smp_call_function_single(cpu, func, info, wait);<BR>&nbsp;put_cpu();<BR>&nbsp;return ret;<BR>}<BR>EXPORT_SYMBOL_GPL(smp_call_function_any);</P>
<P>/**<BR>&nbsp;* smp_call_function_many(): Run a function on a set of other CPUs.<BR>&nbsp;* @mask: The set of cpus to run on (only runs on online subset).<BR>&nbsp;* @func: The function to run. This must be fast and non-blocking.<BR>&nbsp;* @info: An arbitrary pointer to pass to the function.<BR>&nbsp;* @wait: If true, wait (atomically) until function has completed<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; on other CPUs.<BR>&nbsp;*<BR>&nbsp;* If @wait is true, then returns once @func has returned.<BR>&nbsp;*<BR>&nbsp;* You must not call this function with disabled interrupts or from a<BR>&nbsp;* hardware interrupt handler or from a bottom half handler. Preemption<BR>&nbsp;* must be disabled when calling this function.<BR>&nbsp;*/<BR>void smp_call_function_many(const struct cpumask *mask,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; smp_call_func_t func, void *info, bool wait)<BR>{<BR>&nbsp;struct call_function_data *cfd;<BR>&nbsp;int cpu, next_cpu, this_cpu = smp_processor_id();</P>
<P>&nbsp;/*<BR>&nbsp; * Can deadlock when called with interrupts disabled.<BR>&nbsp; * We allow cpu's that are not yet online though, as no one else can<BR>&nbsp; * send smp call function interrupt to this cpu and as such deadlocks<BR>&nbsp; * can't happen.<BR>&nbsp; */<BR>&nbsp;WARN_ON_ONCE(cpu_online(this_cpu) &amp;&amp; irqs_disabled()<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;&amp; !oops_in_progress &amp;&amp; !early_boot_irqs_disabled);</P>
<P>&nbsp;/* Try to fastpath.&nbsp; So, what's a CPU they want? Ignoring this one. */<BR>&nbsp;cpu = cpumask_first_and(mask, cpu_online_mask);<BR>&nbsp;if (cpu == this_cpu)<BR>&nbsp;&nbsp;cpu = cpumask_next_and(cpu, mask, cpu_online_mask);</P>
<P>&nbsp;/* No online cpus?&nbsp; We're done. */<BR>&nbsp;if (cpu &gt;= nr_cpu_ids)<BR>&nbsp;&nbsp;return;</P>
<P>&nbsp;/* Do we have another CPU which isn't us? */<BR>&nbsp;next_cpu = cpumask_next_and(cpu, mask, cpu_online_mask);<BR>&nbsp;if (next_cpu == this_cpu)<BR>&nbsp;&nbsp;next_cpu = cpumask_next_and(next_cpu, mask, cpu_online_mask);</P>
<P>&nbsp;/* Fastpath: do that cpu by itself. */<BR>&nbsp;if (next_cpu &gt;= nr_cpu_ids) {<BR>&nbsp;&nbsp;smp_call_function_single(cpu, func, info, wait);<BR>&nbsp;&nbsp;return;<BR>&nbsp;}</P>
<P>&nbsp;cfd = this_cpu_ptr(&amp;cfd_data);</P>
<P>&nbsp;cpumask_and(cfd-&gt;cpumask, mask, cpu_online_mask);<BR>&nbsp;cpumask_clear_cpu(this_cpu, cfd-&gt;cpumask);</P>
<P>&nbsp;/* Some callers race with other cpus changing the passed mask */<BR>&nbsp;if (unlikely(!cpumask_weight(cfd-&gt;cpumask)))<BR>&nbsp;&nbsp;return;</P>
<P>&nbsp;for_each_cpu(cpu, cfd-&gt;cpumask) {<BR>&nbsp;&nbsp;struct call_single_data *csd = per_cpu_ptr(cfd-&gt;csd, cpu);</P>
<P>&nbsp;&nbsp;csd_lock(csd);<BR>&nbsp;&nbsp;if (wait)<BR>&nbsp;&nbsp;&nbsp;csd-&gt;flags |= CSD_FLAG_SYNCHRONOUS;<BR>&nbsp;&nbsp;csd-&gt;func = func;<BR>&nbsp;&nbsp;csd-&gt;info = info;<BR>&nbsp;&nbsp;llist_add(&amp;csd-&gt;llist, &amp;per_cpu(call_single_queue, cpu));<BR>&nbsp;}</P>
<P>&nbsp;/* Send a message to all CPUs in the map */<BR>&nbsp;arch_send_call_function_ipi_mask(cfd-&gt;cpumask);</P>
<P>&nbsp;if (wait) {<BR>&nbsp;&nbsp;for_each_cpu(cpu, cfd-&gt;cpumask) {<BR>&nbsp;&nbsp;&nbsp;struct call_single_data *csd;</P>
<P>&nbsp;&nbsp;&nbsp;csd = per_cpu_ptr(cfd-&gt;csd, cpu);<BR>&nbsp;&nbsp;&nbsp;csd_lock_wait(csd);<BR>&nbsp;&nbsp;}<BR>&nbsp;}<BR>}<BR>EXPORT_SYMBOL(smp_call_function_many);</P>
<P>/**<BR>&nbsp;* smp_call_function(): Run a function on all other CPUs.<BR>&nbsp;* @func: The function to run. This must be fast and non-blocking.<BR>&nbsp;* @info: An arbitrary pointer to pass to the function.<BR>&nbsp;* @wait: If true, wait (atomically) until function has completed<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; on other CPUs.<BR>&nbsp;*<BR>&nbsp;* Returns 0.<BR>&nbsp;*<BR>&nbsp;* If @wait is true, then returns once @func has returned; otherwise<BR>&nbsp;* it returns just before the target cpu calls @func.<BR>&nbsp;*<BR>&nbsp;* You must not call this function with disabled interrupts or from a<BR>&nbsp;* hardware interrupt handler or from a bottom half handler.<BR>&nbsp;*/<BR>int smp_call_function(smp_call_func_t func, void *info, int wait)<BR>{<BR>&nbsp;preempt_disable();<BR>&nbsp;smp_call_function_many(cpu_online_mask, func, info, wait);<BR>&nbsp;preempt_enable();</P>
<P>&nbsp;return 0;<BR>}<BR>EXPORT_SYMBOL(smp_call_function);</P>
<P>/* Setup configured maximum number of CPUs to activate */<BR>unsigned int setup_max_cpus = NR_CPUS;<BR>EXPORT_SYMBOL(setup_max_cpus);</P>
<P><BR>/*<BR>&nbsp;* Setup routine for controlling SMP activation<BR>&nbsp;*<BR>&nbsp;* Command-line option of "nosmp" or "maxcpus=0" will disable SMP<BR>&nbsp;* activation entirely (the MPS table probe still happens, though).<BR>&nbsp;*<BR>&nbsp;* Command-line option of "maxcpus=&lt;NUM&gt;", where &lt;NUM&gt; is an integer<BR>&nbsp;* greater than 0, limits the maximum number of CPUs activated in<BR>&nbsp;* SMP mode to &lt;NUM&gt;.<BR>&nbsp;*/</P>
<P>void __weak arch_disable_smp_support(void) { }</P>
<P>static int __init nosmp(char *str)<BR>{<BR>&nbsp;setup_max_cpus = 0;<BR>&nbsp;arch_disable_smp_support();</P>
<P>&nbsp;return 0;<BR>}</P>
<P>early_param("nosmp", nosmp);</P>
<P>/* this is hard limit */<BR>static int __init nrcpus(char *str)<BR>{<BR>&nbsp;int nr_cpus;</P>
<P>&nbsp;get_option(&amp;str, &amp;nr_cpus);<BR>&nbsp;if (nr_cpus &gt; 0 &amp;&amp; nr_cpus &lt; nr_cpu_ids)<BR>&nbsp;&nbsp;nr_cpu_ids = nr_cpus;</P>
<P>&nbsp;return 0;<BR>}</P>
<P>early_param("nr_cpus", nrcpus);</P>
<P>static int __init maxcpus(char *str)<BR>{<BR>&nbsp;get_option(&amp;str, &amp;setup_max_cpus);<BR>&nbsp;if (setup_max_cpus == 0)<BR>&nbsp;&nbsp;arch_disable_smp_support();</P>
<P>&nbsp;return 0;<BR>}</P>
<P>early_param("maxcpus", maxcpus);</P>
<P>/* Setup number of possible processor ids */<BR>int nr_cpu_ids __read_mostly = NR_CPUS;<BR>EXPORT_SYMBOL(nr_cpu_ids);</P>
<P>/* An arch may set nr_cpu_ids earlier if needed, so this would be redundant */<BR>void __init setup_nr_cpu_ids(void)<BR>{<BR>&nbsp;nr_cpu_ids = find_last_bit(cpumask_bits(cpu_possible_mask),NR_CPUS) + 1;<BR>}</P>
<P>void __weak smp_announce(void)<BR>{<BR>&nbsp;printk(KERN_INFO "Brought up %d CPUs\n", num_online_cpus());<BR>}</P>
<P>/* Called by boot processor to activate the rest. */<BR>void __init smp_init(void)<BR>{<BR>&nbsp;unsigned int cpu;</P>
<P>&nbsp;idle_threads_init();<BR>&nbsp;cpuhp_threads_init();</P>
<P>&nbsp;/* FIXME: This should be done in userspace --RR */<BR>&nbsp;for_each_present_cpu(cpu) {<BR>&nbsp;&nbsp;if (num_online_cpus() &gt;= setup_max_cpus)<BR>&nbsp;&nbsp;&nbsp;break;<BR>&nbsp;&nbsp;if (!cpu_online(cpu))<BR>&nbsp;&nbsp;&nbsp;cpu_up(cpu);<BR>&nbsp;}</P>
<P>&nbsp;/* Any cleanup work */<BR>&nbsp;smp_announce();<BR>&nbsp;smp_cpus_done(setup_max_cpus);<BR>}</P>
<P>/*<BR>&nbsp;* Call a function on all processors.&nbsp; May be used during early boot while<BR>&nbsp;* early_boot_irqs_disabled is set.&nbsp; Use local_irq_save/restore() instead<BR>&nbsp;* of local_irq_disable/enable().<BR>&nbsp;*/<BR>int on_each_cpu(void (*func) (void *info), void *info, int wait)<BR>{<BR>&nbsp;unsigned long flags;<BR>&nbsp;int ret = 0;</P>
<P>&nbsp;preempt_disable();<BR>&nbsp;ret = smp_call_function(func, info, wait);<BR>&nbsp;local_irq_save(flags);<BR>&nbsp;func(info);<BR>&nbsp;local_irq_restore(flags);<BR>&nbsp;preempt_enable();<BR>&nbsp;return ret;<BR>}<BR>EXPORT_SYMBOL(on_each_cpu);</P>
<P>/**<BR>&nbsp;* on_each_cpu_mask(): Run a function on processors specified by<BR>&nbsp;* cpumask, which may include the local processor.<BR>&nbsp;* @mask: The set of cpus to run on (only runs on online subset).<BR>&nbsp;* @func: The function to run. This must be fast and non-blocking.<BR>&nbsp;* @info: An arbitrary pointer to pass to the function.<BR>&nbsp;* @wait: If true, wait (atomically) until function has completed<BR>&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; on other CPUs.<BR>&nbsp;*<BR>&nbsp;* If @wait is true, then returns once @func has returned.<BR>&nbsp;*<BR>&nbsp;* You must not call this function with disabled interrupts or from a<BR>&nbsp;* hardware interrupt handler or from a bottom half handler.&nbsp; The<BR>&nbsp;* exception is that it may be used during early boot while<BR>&nbsp;* early_boot_irqs_disabled is set.<BR>&nbsp;*/<BR>void on_each_cpu_mask(const struct cpumask *mask, smp_call_func_t func,<BR>&nbsp;&nbsp;&nbsp;void *info, bool wait)<BR>{<BR>&nbsp;int cpu = get_cpu();</P>
<P>&nbsp;smp_call_function_many(mask, func, info, wait);<BR>&nbsp;if (cpumask_test_cpu(cpu, mask)) {<BR>&nbsp;&nbsp;unsigned long flags;<BR>&nbsp;&nbsp;local_irq_save(flags);<BR>&nbsp;&nbsp;func(info);<BR>&nbsp;&nbsp;local_irq_restore(flags);<BR>&nbsp;}<BR>&nbsp;put_cpu();<BR>}<BR>EXPORT_SYMBOL(on_each_cpu_mask);</P>
<P>/*<BR>&nbsp;* on_each_cpu_cond(): Call a function on each processor for which<BR>&nbsp;* the supplied function cond_func returns true, optionally waiting<BR>&nbsp;* for all the required CPUs to finish. This may include the local<BR>&nbsp;* processor.<BR>&nbsp;* @cond_func:&nbsp;A callback function that is passed a cpu id and<BR>&nbsp;*&nbsp;&nbsp;the the info parameter. The function is called<BR>&nbsp;*&nbsp;&nbsp;with preemption disabled. The function should<BR>&nbsp;*&nbsp;&nbsp;return a blooean value indicating whether to IPI<BR>&nbsp;*&nbsp;&nbsp;the specified CPU.<BR>&nbsp;* @func:&nbsp;The function to run on all applicable CPUs.<BR>&nbsp;*&nbsp;&nbsp;This must be fast and non-blocking.<BR>&nbsp;* @info:&nbsp;An arbitrary pointer to pass to both functions.<BR>&nbsp;* @wait:&nbsp;If true, wait (atomically) until function has<BR>&nbsp;*&nbsp;&nbsp;completed on other CPUs.<BR>&nbsp;* @gfp_flags:&nbsp;GFP flags to use when allocating the cpumask<BR>&nbsp;*&nbsp;&nbsp;used internally by the function.<BR>&nbsp;*<BR>&nbsp;* The function might sleep if the GFP flags indicates a non<BR>&nbsp;* atomic allocation is allowed.<BR>&nbsp;*<BR>&nbsp;* Preemption is disabled to protect against CPUs going offline but not online.<BR>&nbsp;* CPUs going online during the call will not be seen or sent an IPI.<BR>&nbsp;*<BR>&nbsp;* You must not call this function with disabled interrupts or<BR>&nbsp;* from a hardware interrupt handler or from a bottom half handler.<BR>&nbsp;*/<BR>void on_each_cpu_cond(bool (*cond_func)(int cpu, void *info),<BR>&nbsp;&nbsp;&nbsp;smp_call_func_t func, void *info, bool wait,<BR>&nbsp;&nbsp;&nbsp;gfp_t gfp_flags)<BR>{<BR>&nbsp;cpumask_var_t cpus;<BR>&nbsp;int cpu, ret;</P>
<P>&nbsp;might_sleep_if(gfpflags_allow_blocking(gfp_flags));</P>
<P>&nbsp;if (likely(zalloc_cpumask_var(&amp;cpus, (gfp_flags|__GFP_NOWARN)))) {<BR>&nbsp;&nbsp;preempt_disable();<BR>&nbsp;&nbsp;for_each_online_cpu(cpu)<BR>&nbsp;&nbsp;&nbsp;if (cond_func(cpu, info))<BR>&nbsp;&nbsp;&nbsp;&nbsp;cpumask_set_cpu(cpu, cpus);<BR>&nbsp;&nbsp;on_each_cpu_mask(cpus, func, info, wait);<BR>&nbsp;&nbsp;preempt_enable();<BR>&nbsp;&nbsp;free_cpumask_var(cpus);<BR>&nbsp;} else {<BR>&nbsp;&nbsp;/*<BR>&nbsp;&nbsp; * No free cpumask, bother. No matter, we'll<BR>&nbsp;&nbsp; * just have to IPI them one by one.<BR>&nbsp;&nbsp; */<BR>&nbsp;&nbsp;preempt_disable();<BR>&nbsp;&nbsp;for_each_online_cpu(cpu)<BR>&nbsp;&nbsp;&nbsp;if (cond_func(cpu, info)) {<BR>&nbsp;&nbsp;&nbsp;&nbsp;ret = smp_call_function_single(cpu, func,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;info, wait);<BR>&nbsp;&nbsp;&nbsp;&nbsp;WARN_ON_ONCE(ret);<BR>&nbsp;&nbsp;&nbsp;}<BR>&nbsp;&nbsp;preempt_enable();<BR>&nbsp;}<BR>}<BR>EXPORT_SYMBOL(on_each_cpu_cond);</P>
<P>static void do_nothing(void *unused)<BR>{<BR>}</P>
<P>/**<BR>&nbsp;* kick_all_cpus_sync - Force all cpus out of idle<BR>&nbsp;*<BR>&nbsp;* Used to synchronize the update of pm_idle function pointer. It's<BR>&nbsp;* called after the pointer is updated and returns after the dummy<BR>&nbsp;* callback function has been executed on all cpus. The execution of<BR>&nbsp;* the function can only happen on the remote cpus after they have<BR>&nbsp;* left the idle function which had been called via pm_idle function<BR>&nbsp;* pointer. So it's guaranteed that nothing uses the previous pointer<BR>&nbsp;* anymore.<BR>&nbsp;*/<BR>void kick_all_cpus_sync(void)<BR>{<BR>&nbsp;/* Make sure the change is visible before we kick the cpus */<BR>&nbsp;smp_mb();<BR>&nbsp;smp_call_function(do_nothing, NULL, 1);<BR>}<BR>EXPORT_SYMBOL_GPL(kick_all_cpus_sync);</P>
<P>/**<BR>&nbsp;* wake_up_all_idle_cpus - break all cpus out of idle<BR>&nbsp;* wake_up_all_idle_cpus try to break all cpus which is in idle state even<BR>&nbsp;* including idle polling cpus, for non-idle cpus, we will do nothing<BR>&nbsp;* for them.<BR>&nbsp;*/<BR>void wake_up_all_idle_cpus(void)<BR>{<BR>&nbsp;int cpu;</P>
<P>&nbsp;preempt_disable();<BR>&nbsp;for_each_online_cpu(cpu) {<BR>&nbsp;&nbsp;if (cpu == smp_processor_id())<BR>&nbsp;&nbsp;&nbsp;continue;</P>
<P>&nbsp;&nbsp;wake_up_if_idle(cpu);<BR>&nbsp;}<BR>&nbsp;preempt_enable();<BR>}<BR>EXPORT_SYMBOL_GPL(wake_up_all_idle_cpus);